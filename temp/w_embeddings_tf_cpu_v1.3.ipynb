{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "tested-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labriares\n",
    "import os \n",
    "import json \n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-garlic",
   "metadata": {},
   "source": [
    "### Load & Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "valid-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(descriptions,dir_path):\n",
    "    \n",
    "    counter = 0\n",
    "    counter_issues = 0\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            ##############################\n",
    "            counter += 1\n",
    "            #print(counter,\") reading file\",fname)\n",
    "            ##############################\n",
    "            \n",
    "            #load data in json format\n",
    "            data = json.load(json_file)\n",
    "            for p in data:\n",
    "                \n",
    "                ##############################\n",
    "                issue_name     = p['name']\n",
    "                counter_issues += 1\n",
    "                #print(\"  \",counter_issues,\")\",issue_name)\n",
    "                ##############################\n",
    "                \n",
    "                issue_desc     = p['description']\n",
    "                \n",
    "                # add all non empty issues and non dublicate.\n",
    "                if issue_desc != [] and issue_desc not in descriptions:\n",
    "                    descriptions.append(issue_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "italic-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(clean_descriptions,raw_descriptions):\n",
    "    \n",
    "    # define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    for desc in raw_descriptions:\n",
    "        \n",
    "        #join all lines into one sentence\n",
    "        sentence = ' '.join(desc)\n",
    "        \n",
    "        #translate punctuation\n",
    "        new_sentence = sentence.translate(translator)\n",
    "        \n",
    "        #split the sentense in words\n",
    "        words = new_sentence.split()\n",
    "        words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "        \n",
    "        if words_sw != []:\n",
    "            clean_descriptions.append(words_sw)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "great-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_data(descriptions):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        for counter in range(len(desc)):\n",
    "            if desc[counter].isalpha():\n",
    "                desc[counter] = stemmer.stem(desc[counter])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "logical-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(descriptions,valid_size,test_size,min_size):\n",
    "    \n",
    "    valid_set = []\n",
    "    test_set  = []\n",
    "    \n",
    "    # random select descriptions.\n",
    "    seed(datetime.now())\n",
    "    \n",
    "    for i in range(valid_size):\n",
    "        flag = False\n",
    "        while flag == False:\n",
    "            temp = randint(0,len(descriptions)-1)\n",
    "            if len(descriptions[temp]) >= min_size:\n",
    "                valid_set.append(descriptions.pop(temp))\n",
    "                flag = True\n",
    "    \n",
    "    for i in range(test_size):\n",
    "        flag = False\n",
    "        while flag == False:\n",
    "            temp = randint(0,len(descriptions)-1)\n",
    "            if len(descriptions[temp]) >= min_size:\n",
    "                test_set.append(descriptions.pop(temp))\n",
    "                flag = True\n",
    "    \n",
    "    return valid_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "steady-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first time the below command should run to download stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# define necessary parameters\n",
    "dir_path         = '/home/kostas/Documents/thesis/data_1'\n",
    "raw_descriptions = []\n",
    "min_size         = 10\n",
    "\n",
    "# load all issues descriptions\n",
    "load_data(raw_descriptions,dir_path)\n",
    "\n",
    "# split and clean descriptions\n",
    "clean_descriptions = []\n",
    "clean_data(clean_descriptions,raw_descriptions)\n",
    "\n",
    "# list raw_descriptions now is useless\n",
    "del raw_descriptions\n",
    "\n",
    "# stemming, it's not necessary step.\n",
    "stemming_data(clean_descriptions)\n",
    "\n",
    "# split data set to train,validation and test set\n",
    "# validation and test set would have 20% of total data.\n",
    "total_desc = len(clean_descriptions)\n",
    "valid_size = int(0.4 * total_desc)\n",
    "test_size  = int(0.2 * total_desc)\n",
    "\n",
    "valid_set,test_set = split_dataset(clean_descriptions,valid_size,test_size,min_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "continued-mailman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique descriptions 5973\n",
      "size of train set 2390\n",
      "size of validation set 2389\n",
      "size of test set 1194\n"
     ]
    }
   ],
   "source": [
    "# print messages #\n",
    "print(\"total unique descriptions\",total_desc)\n",
    "print(\"size of train set\",len(clean_descriptions))\n",
    "print(\"size of validation set\",valid_size)\n",
    "print(\"size of test set\",test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-shooting",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "adaptive-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some important variables\n",
    "valid_words   = 80\n",
    "valid_words2  = 70\n",
    "test_words    = 100\n",
    "true_neigh    = 8\n",
    "false_neigh   = 30\n",
    "\n",
    "min_occurance = 5\n",
    "unk_word      = \"UNK\"\n",
    "skip_window   = 2\n",
    "batch_size    = 2048\n",
    "embedding_dim = 32\n",
    "num_sampled   = 8\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "formal-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(word_dict):\n",
    "    \n",
    "    with open(\"vocabulary_test.txt\",\"w\") as file:\n",
    "        for key in word_dict:\n",
    "            file.write(\"%s, %s \\n\"%(key,str(word_dict[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "casual-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(train_set,skip_window):\n",
    "    \n",
    "    #find total words in descriptions\n",
    "    total_words = 0\n",
    "    for desc in train_set:\n",
    "        total_words += len(desc)\n",
    "    \n",
    "    #initialize the corpus which will keep all word pairs\n",
    "    max_size = total_words*2*skip_window\n",
    "    corpus = -1*np.ones((max_size,2), dtype=np.int32)\n",
    "    \n",
    "    # initialize pointers for the iterations\n",
    "    desc_pointer  = 0\n",
    "    word_pointer  = 0\n",
    "    counter       = 0\n",
    "    \n",
    "    #initialize temporary buffer\n",
    "    span   = 2*skip_window+1 \n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    while counter < max_size:\n",
    "        \n",
    "        # avoid tags with -2\n",
    "        while train_set[desc_pointer][word_pointer] < 0:\n",
    "            word_pointer += 1\n",
    "            if word_pointer > len(train_set[desc_pointer])-1:\n",
    "                word_pointer  = 0\n",
    "                desc_pointer +=1\n",
    "                if desc_pointer > len(train_set) -1:\n",
    "                    break\n",
    "                    \n",
    "        #check if all descriptions have been analyzed\n",
    "        if desc_pointer > len(train_set)-1:\n",
    "            break\n",
    "        \n",
    "        find_context_words(train_set[desc_pointer],word_pointer,skip_window,span,buffer)\n",
    "        \n",
    "        for i in range(1,len(buffer)):\n",
    "            corpus[counter][0] = buffer[0]\n",
    "            corpus[counter][1] = buffer[i]\n",
    "            counter += 1\n",
    "        \n",
    "        buffer.clear()\n",
    "        \n",
    "        if word_pointer == len(train_set[desc_pointer]) -1:\n",
    "            word_pointer  = 0\n",
    "            desc_pointer +=1\n",
    "            if desc_pointer > len(train_set) -1:\n",
    "                break\n",
    "        else:\n",
    "            word_pointer += 1\n",
    "    \n",
    "    return corpus[0:counter].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "qualified-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_words(description,word_index,skip_window,span,grams_list):\n",
    "    \n",
    "    # the target word in the first place\n",
    "    grams_list.append(description[word_index])\n",
    "    \n",
    "    # initialize two pointers\n",
    "    counter = 1\n",
    "    data_index = word_index-1\n",
    "    \n",
    "    while counter < span:\n",
    "        # look left from target word\n",
    "        if counter<=skip_window:\n",
    "            # if data_index<0 => out of bound no more words to take into account\n",
    "            if data_index < 0:\n",
    "                data_index = word_index + 1\n",
    "                counter = skip_window + 1\n",
    "            # if the word is not in the dict skip it\n",
    "            elif description[data_index] == -2:\n",
    "                data_index -= 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index -= 1\n",
    "                if counter > skip_window:\n",
    "                    data_index = word_index + 1\n",
    "        # look right from target word\n",
    "        else:\n",
    "            if data_index >= len(description):\n",
    "                counter = span + 1\n",
    "            elif description[data_index] == -2:\n",
    "                data_index += 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index += 1\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "classical-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary based on the frequency of each word.\n",
    "# remove rare words, which occurs less time than min_occurance from voc\n",
    "# word2id:  dictionary which contains the vocabulary and it's int id\n",
    "\n",
    "temp_sentences = [word for desc in clean_descriptions for word in desc]\n",
    "\n",
    "count = []\n",
    "count.extend(collections.Counter(temp_sentences).most_common())\n",
    "\n",
    "# list temp_sentences now is useless\n",
    "del temp_sentences\n",
    "\n",
    "count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "\n",
    "# compute the vocabulary size\n",
    "vocabulary_size = len(count)\n",
    "\n",
    "# assign an id to each word\n",
    "# this dictionary will have voc_size+1 length.\n",
    "word2id = dict()\n",
    "word2id[unk_word] = -2\n",
    "\n",
    "for i,(word,_) in enumerate(count):\n",
    "    word2id[word] = i\n",
    "\n",
    "# list count now is useless\n",
    "del count\n",
    "\n",
    "#express train, valid and test set using id\n",
    "train_set_id = [[word2id.get(word,-2) for word in desc] for desc in clean_descriptions]\n",
    "del clean_descriptions\n",
    "valid_set_id = [[word2id.get(word,-2) for word in desc] for desc in valid_set]\n",
    "del valid_set\n",
    "test_set_id  = [[word2id.get(word,-2) for word in desc] for desc in test_set]\n",
    "del test_set\n",
    "\n",
    "# save vocabulary\n",
    "save_vocabulary(word2id)\n",
    "\n",
    "# create corpus with word pairs\n",
    "corpus         = create_corpus(train_set_id,skip_window)\n",
    "corpus_indexes = [w for w in range(len(corpus))] \n",
    "\n",
    "# save them \n",
    "np.savetxt('corpus_words_test.txt',corpus,fmt=\"%d\")\n",
    "\n",
    "# train_set_id now is useless\n",
    "del train_set_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-control",
   "metadata": {},
   "source": [
    "### Validation and Test Pairs Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "electoral-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_pairs(test_dict):\n",
    "    \n",
    "    with open('testing_pairs_test.pkl','wb') as file:\n",
    "        pickle.dump(test_dict,file,pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "moved-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_dict(test_set,min_occurance,num_words,num_words2,skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # numerate all words in the dataset.\n",
    "    temp_sentences = [word for desc in test_set for word in desc]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(temp_sentences).most_common())\n",
    "    \n",
    "    # list temp_sentences now is useless\n",
    "    del temp_sentences\n",
    "    \n",
    "    # remove rare words\n",
    "    count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "    indexes  = [i for i in range(len(count)) if count[i][0] != -2]\n",
    "    \n",
    "    # split validation set into two sets one small used for cross entropy computation\n",
    "    # and the other at the end to meassure results.\n",
    "    if num_words2>0:\n",
    "        \n",
    "        samples2  = np.random.choice(indexes,num_words2,replace = False)\n",
    "        target_w2 = [count[i][0] for i in samples2]\n",
    "        w_dict2   = create_testing_pairs(test_set,count,target_w2,indexes,skip_window,true_neigh,false_neigh)\n",
    "        \n",
    "        # test on the \"num_words\" most frequent words\n",
    "        tmp_indexes = [i for i in indexes if i not in samples2]\n",
    "        target_w    = [count[tmp_indexes[i]][0] for i in range(num_words)]\n",
    "        w_dict      = create_testing_pairs(test_set,count,target_w,indexes,skip_window,true_neigh,false_neigh)\n",
    "        del tmp_indexes\n",
    "        return w_dict2,w_dict\n",
    "    \n",
    "    else:\n",
    "        # test on the \"num_words\" most frequent words\n",
    "        target_w = [count[indexes[i]][0] for i in range(num_words)]\n",
    "        w_dict   = create_testing_pairs(test_set,count,target_w,indexes,skip_window,true_neigh,false_neigh)\n",
    "        return None,w_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "wrong-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_pairs(test_set,count,target_w,indexes,skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # initialize temporary buffer\n",
    "    span   = skip_window*2+1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    # initialize dictionary\n",
    "    w_dict   = dict([(key, [[],[]]) for key in target_w])\n",
    "    \n",
    "    # find true neighbors for target words\n",
    "    for desc in test_set:\n",
    "        for w in target_w:\n",
    "            temp_idx = [i for i,e in enumerate(desc) if w == e]\n",
    "            for idx in temp_idx:\n",
    "                find_context_words(desc,idx,skip_window,span,buffer)\n",
    "                for i in range(1,len(buffer)):\n",
    "                    if w_dict[w][0] == []:\n",
    "                        w_dict[w][0].append(buffer[i])\n",
    "                    elif buffer[i] not in w_dict[w][0]:\n",
    "                        w_dict[w][0].append(buffer[i])\n",
    "    \n",
    "    # find false neigbors for target words\n",
    "    for key in w_dict:\n",
    "        neig_counter = 0\n",
    "        flag         = True\n",
    "        while flag == True:\n",
    "            random_idx   = np.random.choice(indexes,2*false_neigh,replace = False)\n",
    "            for idx in random_idx:\n",
    "                if count[idx][0] == key:\n",
    "                    continue\n",
    "                elif count[idx][0] in w_dict[key][0]:\n",
    "                    continue\n",
    "                elif count[idx][0] not in w_dict[key][1]:\n",
    "                    w_dict[key][1].append(count[idx][0])\n",
    "                    neig_counter += 1\n",
    "                    if neig_counter >= false_neigh:\n",
    "                        flag = False\n",
    "                        break\n",
    "    \n",
    "    # choose randomly only true_neigh neighbors.\n",
    "    for key in w_dict:\n",
    "        if len(w_dict[key][0])>=true_neigh:\n",
    "            idx_neigh =  np.random.choice([i for i in range(len(w_dict[key][0]))],true_neigh,replace = False)\n",
    "            w_dict[key][0] = [w_dict[key][0][i] for i in idx_neigh]\n",
    "        else:\n",
    "            w_dict.pop(key)\n",
    "    return w_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "julian-lyric",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_,test_dict  = create_testing_dict(test_set_id,min_occurance,test_words,0,2,true_neigh,false_neigh)\n",
    "save_test_pairs(test_dict)\n",
    "del test_dict\n",
    "\n",
    "v_dict2,v_dict = create_testing_dict(valid_set_id,min_occurance,valid_words,valid_words2,2,true_neigh,false_neigh)\n",
    "t_batch  = []\n",
    "t_label  = []\n",
    "for key in v_dict2:\n",
    "    for value in v_dict2[key][0]:\n",
    "        t_batch.append(key)\n",
    "        t_label.append(value)\n",
    "\n",
    "v_batch = np.reshape(t_batch,(len(t_batch),))\n",
    "v_label = np.reshape(t_label,(len(t_label),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-bacon",
   "metadata": {},
   "source": [
    "## Word's Embeddings Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "consolidated-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "timely-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(corpus_data,corpus_indexes,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size),   dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.int32)\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    words_to_use = random.sample(corpus_indexes,batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(words_to_use):\n",
    "        batch[counter]    = corpus_data[value][0]\n",
    "        labels[counter,0] = corpus_data[value][1] \n",
    "    \n",
    "    return batch,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "precise-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def_cpu(corpus_data,corpus_indexes,batch_size,embedding_dim,\n",
    "                  num_sampled,learning_rate,vocabulary_size,v_batch,v_labels):\n",
    "    \n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.int32, shape=[None])\n",
    "    # Input label\n",
    "    Y_train = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    \n",
    "    # ensure that the following ops & var are assigned to CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "    \n",
    "        # create the embedding variable wich contains the weights\n",
    "        embedding = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "        \n",
    "        # create the lookup table for each sample in X_train=>avoiding to use one_hot encoder\n",
    "        X_embed   = tf.nn.embedding_lookup(embedding,X_train)\n",
    "        \n",
    "        # create variables for the loss function\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_dim],stddev=1.0))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    loss_func = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,biases =nce_biases,labels = Y_train,\n",
    "                                              inputs = X_embed,num_sampled = num_sampled,\n",
    "                                              num_classes = vocabulary_size ))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)   \n",
    "    train_opt = optimizer.minimize(loss_func)\n",
    "    \n",
    "    #Define initializer for tensorflow variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #actual initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # patience method's variables \n",
    "        min_loss           = float('inf')\n",
    "        min_emb_matrix     = np.zeros((vocabulary_size,embedding_dim))\n",
    "        patience_remaining = 500\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # train the model using 500 epoch patience\n",
    "        for epoch in range(50000):\n",
    "            # take a batch of data.\n",
    "            batch_x,batch_y = generate_batch(corpus_data,corpus_indexes,batch_size)\n",
    "            \n",
    "            _,train_loss = sess.run([train_opt,loss_func],feed_dict={X_train:batch_x, Y_train:batch_y})\n",
    "            valid_loss   = sess.run(loss_func,feed_dict={X_train:v_batch, Y_train:v_labels})\n",
    "            \n",
    "            patience_remaining -= 1\n",
    "            if valid_loss < min_loss:\n",
    "                min_loss           = train_loss\n",
    "                patience_remaining = 500\n",
    "                min_emb_matrix     = embedding.eval()\n",
    "            if patience_remaining == 0:\n",
    "                break\n",
    "        \n",
    "        #restore min embeddings\n",
    "        embedding = tf.convert_to_tensor(min_emb_matrix)\n",
    "        \n",
    "        #normalize embeddings before using them\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keepdims = True))\n",
    "        normalized_embedding = embedding/norm\n",
    "        \n",
    "        normalized_embedding_matrix = sess.run(normalized_embedding)\n",
    "        \n",
    "        #measure total time\n",
    "        total_time = time.time() - start_time\n",
    "        print(\"training time in seconds %s \"%(str(total_time)))\n",
    "        print(\"total epochs was\",epoch+1)\n",
    "    return normalized_embedding_matrix\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "primary-powder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time in seconds 25.09490418434143 \n",
      "total epochs was 4676\n"
     ]
    }
   ],
   "source": [
    "norm_embedding_matrix = model_def_cpu(corpus,corpus_indexes,batch_size,embedding_dim,num_sampled,\n",
    "                                      learning_rate,vocabulary_size,v_batch,v_label)\n",
    "\n",
    "np.savetxt('word_embeddings_test.txt',norm_embedding_matrix,fmt='%.8f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-carol",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "parliamentary-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model computes tpr, fpr and auc. The classes are class_A = real neighbor\n",
    "# and class_B = false neighbor. The model based on cosine similarity\n",
    "# will try to predict the right label for each word pair given.\n",
    "def model_validation_v2(embedding_matrix,words_dict):\n",
    "    \n",
    "    ylabels = list()\n",
    "    ypreds  = list()\n",
    "    \n",
    "    for key in words_dict:\n",
    "        target_emb = embedding_matrix[key]\n",
    "        for true_neigh in words_dict[key][0]:\n",
    "            neigh_emb = embedding_matrix[true_neigh]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            ylabels.append(1)\n",
    "            ypreds.append(result)\n",
    "            \n",
    "        for false_neigh in words_dict[key][1]:\n",
    "            neigh_emb = embedding_matrix[false_neigh]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            ylabels.append(0)\n",
    "            ypreds.append(result)\n",
    "    \n",
    "    y = np.array(ylabels)\n",
    "    score = np.array(ypreds)\n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y,score)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "centered-establishment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7232376302083333\n"
     ]
    }
   ],
   "source": [
    "auc = model_validation_v2(norm_embedding_matrix,v_dict)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-source",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
