{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ec69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labriares\n",
    "import os \n",
    "import json \n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b97e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(descriptions,dir_path):\n",
    "    \n",
    "    counter        = 0\n",
    "    counter_issues = 0\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            ##############################\n",
    "            print(counter,\") reading file\",fname)\n",
    "            counter += 1\n",
    "            ##############################\n",
    "            \n",
    "            #load data in json format\n",
    "            data = json.load(json_file)\n",
    "            for p in data:\n",
    "                \n",
    "                ##############################\n",
    "                issue_name     = p['name']\n",
    "                counter_issues += 1\n",
    "                #print(\"  \",counter_issues,\")\",issue_name)\n",
    "                ##############################\n",
    "                \n",
    "                issue_desc     = p['description']\n",
    "                \n",
    "                # add all non empty issues and non dublicate.\n",
    "                if issue_desc != [] and issue_desc not in descriptions:\n",
    "                    descriptions.append(issue_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e79ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(clean_descriptions,raw_descriptions):\n",
    "    \n",
    "    # define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    for desc in raw_descriptions:\n",
    "        \n",
    "        #join all lines into one sentence\n",
    "        sentence = ' '.join(desc)\n",
    "        \n",
    "        #translate punctuation\n",
    "        new_sentence = sentence.translate(translator)\n",
    "        \n",
    "        #split the sentense in words\n",
    "        words = new_sentence.split()\n",
    "        words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "        \n",
    "        clean_descriptions.append(words_sw)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352da1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_data(descriptions):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        for counter in range(len(desc)):\n",
    "            if desc[counter].isalpha():\n",
    "                desc[counter] = stemmer.stem(desc[counter])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1abbb5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) reading file data_word_emb94.json\n",
      "1 ) reading file data_word_emb26.json\n",
      "2 ) reading file data_word_emb58.json\n",
      "3 ) reading file data_word_emb130.json\n",
      "4 ) reading file data_word_emb215.json\n",
      "5 ) reading file data_word_emb156.json\n",
      "6 ) reading file data_word_emb128.json\n",
      "7 ) reading file data_word_emb157.json\n",
      "8 ) reading file data_word_emb177.json\n",
      "9 ) reading file data_word_emb22.json\n",
      "10 ) reading file data_word_emb28.json\n",
      "11 ) reading file data_word_emb79.json\n",
      "12 ) reading file data_word_emb152.json\n",
      "13 ) reading file data_word_emb126.json\n",
      "14 ) reading file data_word_emb114.json\n",
      "15 ) reading file data_word_emb70.json\n",
      "16 ) reading file data_word_emb5.json\n",
      "17 ) reading file data_word_emb93.json\n",
      "18 ) reading file data_word_emb117.json\n",
      "19 ) reading file data_word_emb135.json\n",
      "20 ) reading file data_word_emb37.json\n",
      "21 ) reading file data_word_emb96.json\n",
      "22 ) reading file data_word_emb217.json\n",
      "23 ) reading file data_word_emb243.json\n",
      "24 ) reading file data_word_emb155.json\n",
      "25 ) reading file data_word_emb180.json\n",
      "26 ) reading file data_word_emb253.json\n",
      "27 ) reading file data_word_emb202.json\n",
      "28 ) reading file data_word_emb32.json\n",
      "29 ) reading file data_word_emb31.json\n",
      "30 ) reading file data_word_emb210.json\n",
      "31 ) reading file data_word_emb88.json\n",
      "32 ) reading file data_word_emb160.json\n",
      "33 ) reading file data_word_emb87.json\n",
      "34 ) reading file data_word_emb123.json\n",
      "35 ) reading file data_word_emb151.json\n",
      "36 ) reading file data_word_emb174.json\n",
      "37 ) reading file data_word_emb208.json\n",
      "38 ) reading file data_word_emb184.json\n",
      "39 ) reading file data_word_emb7.json\n",
      "40 ) reading file data_word_emb172.json\n",
      "41 ) reading file data_word_emb171.json\n",
      "42 ) reading file data_word_emb187.json\n",
      "43 ) reading file data_word_emb250.json\n",
      "44 ) reading file data_word_emb136.json\n",
      "45 ) reading file data_word_emb51.json\n",
      "46 ) reading file data_word_emb203.json\n",
      "47 ) reading file data_word_emb238.json\n",
      "48 ) reading file data_word_emb226.json\n",
      "49 ) reading file data_word_emb252.json\n",
      "50 ) reading file data_word_emb108.json\n",
      "51 ) reading file data_word_emb247.json\n",
      "52 ) reading file data_word_emb49.json\n",
      "53 ) reading file data_word_emb170.json\n",
      "54 ) reading file data_word_emb148.json\n",
      "55 ) reading file data_word_emb249.json\n",
      "56 ) reading file data_word_emb251.json\n",
      "57 ) reading file data_word_emb35.json\n",
      "58 ) reading file data_word_emb149.json\n",
      "59 ) reading file data_word_emb48.json\n",
      "60 ) reading file data_word_emb38.json\n",
      "61 ) reading file data_word_emb12.json\n",
      "62 ) reading file data_word_emb158.json\n",
      "63 ) reading file data_word_emb233.json\n",
      "64 ) reading file data_word_emb192.json\n",
      "65 ) reading file data_word_emb68.json\n",
      "66 ) reading file data_word_emb127.json\n",
      "67 ) reading file data_word_emb20.json\n",
      "68 ) reading file data_word_emb147.json\n",
      "69 ) reading file data_word_emb212.json\n",
      "70 ) reading file data_word_emb120.json\n",
      "71 ) reading file data_word_emb248.json\n",
      "72 ) reading file data_word_emb213.json\n",
      "73 ) reading file data_word_emb236.json\n",
      "74 ) reading file data_word_emb59.json\n",
      "75 ) reading file data_word_emb227.json\n",
      "76 ) reading file data_word_emb44.json\n",
      "77 ) reading file data_word_emb78.json\n",
      "78 ) reading file data_word_emb206.json\n",
      "79 ) reading file data_word_emb34.json\n",
      "80 ) reading file data_word_emb62.json\n",
      "81 ) reading file data_word_emb186.json\n",
      "82 ) reading file data_word_emb218.json\n",
      "83 ) reading file data_word_emb195.json\n",
      "84 ) reading file data_word_emb89.json\n",
      "85 ) reading file data_word_emb232.json\n",
      "86 ) reading file data_word_emb143.json\n",
      "87 ) reading file data_word_emb74.json\n",
      "88 ) reading file data_word_emb145.json\n",
      "89 ) reading file data_word_emb11.json\n",
      "90 ) reading file data_word_emb100.json\n",
      "91 ) reading file data_word_emb199.json\n",
      "92 ) reading file data_word_emb204.json\n",
      "93 ) reading file data_word_emb19.json\n",
      "94 ) reading file data_word_emb14.json\n",
      "95 ) reading file data_word_emb134.json\n",
      "96 ) reading file data_word_emb36.json\n",
      "97 ) reading file data_word_emb188.json\n",
      "98 ) reading file data_word_emb109.json\n",
      "99 ) reading file data_word_emb115.json\n",
      "100 ) reading file data_word_emb1.json\n",
      "101 ) reading file data_word_emb15.json\n",
      "102 ) reading file data_word_emb245.json\n",
      "103 ) reading file data_word_emb159.json\n",
      "104 ) reading file data_word_emb16.json\n",
      "105 ) reading file data_word_emb80.json\n",
      "106 ) reading file data_word_emb241.json\n",
      "107 ) reading file data_word_emb113.json\n",
      "108 ) reading file data_word_emb61.json\n",
      "109 ) reading file data_word_emb107.json\n",
      "110 ) reading file data_word_emb82.json\n",
      "111 ) reading file data_word_emb66.json\n",
      "112 ) reading file data_word_emb240.json\n",
      "113 ) reading file data_word_emb198.json\n",
      "114 ) reading file data_word_emb161.json\n",
      "115 ) reading file data_word_emb13.json\n",
      "116 ) reading file data_word_emb219.json\n",
      "117 ) reading file data_word_emb97.json\n",
      "118 ) reading file data_word_emb64.json\n",
      "119 ) reading file data_word_emb237.json\n",
      "120 ) reading file data_word_emb23.json\n",
      "121 ) reading file data_word_emb46.json\n",
      "122 ) reading file data_word_emb30.json\n",
      "123 ) reading file data_word_emb86.json\n",
      "124 ) reading file data_word_emb92.json\n",
      "125 ) reading file data_word_emb229.json\n",
      "126 ) reading file data_word_emb173.json\n",
      "127 ) reading file data_word_emb18.json\n",
      "128 ) reading file data_word_emb102.json\n",
      "129 ) reading file data_word_emb90.json\n",
      "130 ) reading file data_word_emb254.json\n",
      "131 ) reading file data_word_emb142.json\n",
      "132 ) reading file data_word_emb29.json\n",
      "133 ) reading file data_word_emb207.json\n",
      "134 ) reading file data_word_emb146.json\n",
      "135 ) reading file data_word_emb228.json\n",
      "136 ) reading file data_word_emb17.json\n",
      "137 ) reading file data_word_emb99.json\n",
      "138 ) reading file data_word_emb231.json\n",
      "139 ) reading file data_word_emb63.json\n",
      "140 ) reading file data_word_emb91.json\n",
      "141 ) reading file data_word_emb69.json\n",
      "142 ) reading file data_word_emb104.json\n",
      "143 ) reading file data_word_emb176.json\n",
      "144 ) reading file data_word_emb71.json\n",
      "145 ) reading file data_word_emb24.json\n",
      "146 ) reading file data_word_emb216.json\n",
      "147 ) reading file data_word_emb214.json\n",
      "148 ) reading file data_word_emb101.json\n",
      "149 ) reading file data_word_emb165.json\n",
      "150 ) reading file data_word_emb10.json\n",
      "151 ) reading file data_word_emb200.json\n",
      "152 ) reading file data_word_emb98.json\n",
      "153 ) reading file data_word_emb43.json\n",
      "154 ) reading file data_word_emb121.json\n",
      "155 ) reading file data_word_emb40.json\n",
      "156 ) reading file data_word_emb65.json\n",
      "157 ) reading file data_word_emb25.json\n",
      "158 ) reading file data_word_emb60.json\n",
      "159 ) reading file data_word_emb77.json\n",
      "160 ) reading file data_word_emb50.json\n",
      "161 ) reading file data_word_emb57.json\n",
      "162 ) reading file data_word_emb182.json\n",
      "163 ) reading file data_word_emb246.json\n",
      "164 ) reading file data_word_emb185.json\n",
      "165 ) reading file data_word_emb27.json\n",
      "166 ) reading file data_word_emb2.json\n",
      "167 ) reading file data_word_emb189.json\n",
      "168 ) reading file data_word_emb140.json\n",
      "169 ) reading file data_word_emb164.json\n",
      "170 ) reading file data_word_emb168.json\n",
      "171 ) reading file data_word_emb72.json\n",
      "172 ) reading file data_word_emb67.json\n",
      "173 ) reading file data_word_emb33.json\n",
      "174 ) reading file data_word_emb4.json\n",
      "175 ) reading file data_word_emb3.json\n",
      "176 ) reading file data_word_emb138.json\n",
      "177 ) reading file data_word_emb111.json\n",
      "178 ) reading file data_word_emb194.json\n",
      "179 ) reading file data_word_emb105.json\n",
      "180 ) reading file data_word_emb116.json\n",
      "181 ) reading file data_word_emb235.json\n",
      "182 ) reading file data_word_emb205.json\n",
      "183 ) reading file data_word_emb76.json\n",
      "184 ) reading file data_word_emb154.json\n",
      "185 ) reading file data_word_emb8.json\n",
      "186 ) reading file data_word_emb209.json\n",
      "187 ) reading file data_word_emb41.json\n",
      "188 ) reading file data_word_emb6.json\n",
      "189 ) reading file data_word_emb95.json\n",
      "190 ) reading file data_word_emb183.json\n",
      "191 ) reading file data_word_emb179.json\n",
      "192 ) reading file data_word_emb39.json\n",
      "193 ) reading file data_word_emb163.json\n",
      "194 ) reading file data_word_emb131.json\n",
      "195 ) reading file data_word_emb211.json\n",
      "196 ) reading file data_word_emb166.json\n",
      "197 ) reading file data_word_emb106.json\n",
      "198 ) reading file data_word_emb201.json\n",
      "199 ) reading file data_word_emb112.json\n",
      "200 ) reading file data_word_emb141.json\n",
      "201 ) reading file data_word_emb132.json\n",
      "202 ) reading file data_word_emb124.json\n",
      "203 ) reading file data_word_emb53.json\n",
      "204 ) reading file data_word_emb230.json\n",
      "205 ) reading file data_word_emb150.json\n",
      "206 ) reading file data_word_emb103.json\n",
      "207 ) reading file data_word_emb85.json\n",
      "208 ) reading file data_word_emb110.json\n",
      "209 ) reading file data_word_emb191.json\n",
      "210 ) reading file data_word_emb84.json\n",
      "211 ) reading file data_word_emb225.json\n",
      "212 ) reading file data_word_emb190.json\n",
      "213 ) reading file data_word_emb221.json\n",
      "214 ) reading file data_word_emb196.json\n",
      "215 ) reading file data_word_emb133.json\n",
      "216 ) reading file data_word_emb244.json\n",
      "217 ) reading file data_word_emb178.json\n",
      "218 ) reading file data_word_emb224.json\n",
      "219 ) reading file data_word_emb222.json\n",
      "220 ) reading file data_word_emb47.json\n",
      "221 ) reading file data_word_emb175.json\n",
      "222 ) reading file data_word_emb193.json\n",
      "223 ) reading file data_word_emb234.json\n",
      "224 ) reading file data_word_emb45.json\n",
      "225 ) reading file data_word_emb81.json\n",
      "226 ) reading file data_word_emb54.json\n",
      "227 ) reading file data_word_emb223.json\n",
      "228 ) reading file data_word_emb75.json\n",
      "229 ) reading file data_word_emb122.json\n",
      "230 ) reading file data_word_emb169.json\n",
      "231 ) reading file data_word_emb139.json\n",
      "232 ) reading file data_word_emb52.json\n",
      "233 ) reading file data_word_emb153.json\n",
      "234 ) reading file data_word_emb21.json\n",
      "235 ) reading file data_word_emb144.json\n",
      "236 ) reading file data_word_emb73.json\n",
      "237 ) reading file data_word_emb239.json\n",
      "238 ) reading file data_word_emb83.json\n",
      "239 ) reading file data_word_emb55.json\n",
      "240 ) reading file data_word_emb129.json\n",
      "241 ) reading file data_word_emb220.json\n",
      "242 ) reading file data_word_emb197.json\n",
      "243 ) reading file data_word_emb42.json\n",
      "244 ) reading file data_word_emb9.json\n",
      "245 ) reading file data_word_emb125.json\n",
      "246 ) reading file data_word_emb181.json\n",
      "247 ) reading file data_word_emb118.json\n",
      "248 ) reading file data_word_emb162.json\n",
      "249 ) reading file data_word_emb119.json\n",
      "250 ) reading file data_word_emb167.json\n",
      "251 ) reading file data_word_emb137.json\n",
      "252 ) reading file data_word_emb56.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 ) reading file data_word_emb242.json\n"
     ]
    }
   ],
   "source": [
    "#the first time the below command should run to download stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# define necessary parameters\n",
    "dir_path         = '../data'\n",
    "raw_descriptions = []\n",
    "min_size         = 10\n",
    "\n",
    "# load all issues descriptions\n",
    "load_data(raw_descriptions,dir_path)\n",
    "\n",
    "# split and clean descriptions\n",
    "clean_descriptions = []\n",
    "clean_data(clean_descriptions,raw_descriptions)\n",
    "\n",
    "# list raw_descriptions now is useless\n",
    "# del raw_descriptions\n",
    "\n",
    "# stemming, it's not necessary step.\n",
    "stemming_data(clean_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667c43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_str(clean_descriptions,search_string):\n",
    "    for counter,desc in enumerate(clean_descriptions):\n",
    "        if search_string in desc:\n",
    "            print(counter)\n",
    "            print(counter//25)\n",
    "            print(counter%25)\n",
    "            break\n",
    "    print(raw_descriptions[counter])\n",
    "    print(\"------------------------\")\n",
    "    print(clean_descriptions[counter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f531f0c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's a bug found on Market\", ' In settings if enter a value for height fix other that integer (like +13)', ' at dalvik.system.NativeStart.main(Native Method)', ' ... 12 more']\n",
      "------------------------\n",
      "['bug', 'found', 'market', 'settings', 'enter', 'value', 'height', 'fix', 'integer', 'like', '13', 'dalvik', 'system', 'nativestart', 'main', 'native', 'method', '12']\n"
     ]
    }
   ],
   "source": [
    "find_str(clean_descriptions,\"releasewifilocklock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f475e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi,', 'Just for discussion topic ... I have one idea ... which can make our life easier. What about little redesign to CacheCache. Now when you modify anything in Geocache object, waypoint ... etc, you have to call cache.saveToDatabase(...), otherwise your changes will not persist. I have one idea ... to have CacheCache as singleton in our app and modifying models in that way, that it automatically track changes and set \"commit interval\" like it is being on filesystems.', 'For example:', \"you query cache by GC code from database => it is not in cache, so it will be loaded from db and sets it's validity to current timestamp and modified flag to false\", 'you modify it by calling some setter on it or on its childs (for example you add waypoint)', 'model itself sets modified flag to true', 'changes will be written back to db, by background thread, which will go through cache and write all changed objects and reset their modified/validity state', 'on destruction of CacheCache (or the whole app), the same procedure applies', \"(It's just and idea ... and to make it useful, it is needed to think more about it).\", ' But the main advantage will be in separating other code and caching handling.', 'It can be good to look simultaneously on content providers and just think about creating content provider backed by CacheCache, which will provide all data to our (but not necessary only our) app, in future it can lead to creation of third party plugins.']\n"
     ]
    }
   ],
   "source": [
    "print(raw_descriptions[6060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "243aa6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'discussion', 'topic', 'one', 'idea', 'make', 'life', 'easier', 'little', 'redesign', 'cachecache', 'modify', 'anything', 'geocache', 'object', 'waypoint', 'etc', 'call', 'cache', 'savetodatabase', 'otherwise', 'changes', 'persist', 'one', 'idea', 'cachecache', 'singleton', 'app', 'modifying', 'models', 'way', 'automatically', 'track', 'changes', 'set', 'commit', 'interval', 'like', 'filesystems', 'example', 'query', 'cache', 'gc', 'code', 'database', 'cache', 'loaded', 'db', 'sets', 'validity', 'current', 'timestamp', 'modified', 'flag', 'false', 'modify', 'calling', 'setter', 'childs', 'example', 'add', 'waypoint', 'model', 'sets', 'modified', 'flag', 'true', 'changes', 'written', 'back', 'db', 'background', 'thread', 'go', 'cache', 'write', 'changed', 'objects', 'reset', 'modified', 'validity', 'state', 'destruction', 'cachecache', 'whole', 'app', 'procedure', 'applies', 'idea', 'make', 'useful', 'needed', 'think', 'main', 'advantage', 'separating', 'code', 'caching', 'handling', 'good', 'look', 'simultaneously', 'content', 'providers', 'think', 'creating', 'content', 'provider', 'backed', 'cachecache', 'provide', 'data', 'necessary', 'app', 'future', 'lead', 'creation', 'third', 'party', 'plugins']\n"
     ]
    }
   ],
   "source": [
    "print(clean_descriptions[6060])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27ff459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6245\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7acd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.loadtxt('../outputs/words_corpus.txt',dtype ='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a562a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22  13]\n",
      " [ 13  22]\n",
      " [ 13  96]\n",
      " ...\n",
      " [413 742]\n",
      " [413 165]\n",
      " [165 413]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb692a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
