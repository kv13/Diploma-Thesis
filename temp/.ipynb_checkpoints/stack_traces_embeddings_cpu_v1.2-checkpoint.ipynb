{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mexican-channels",
   "metadata": {},
   "source": [
    "# Stack Traces Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-begin",
   "metadata": {},
   "source": [
    "The mechanism to compute word embeddings is used here to compute embeddings for stack traces in order to capture the sequence and the correlation of function calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-platinum",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "aggressive-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "from random   import seed\n",
    "from random   import randint\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-channels",
   "metadata": {},
   "source": [
    "### Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "sound-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary and the path to find the stack traces\n",
    "dir_path       = '/home/kostas/Documents/thesis/data_1'\n",
    "st_traces_ls   = list()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "proved-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove text from the stack trace \n",
    "# and keep only the sequence of functions\n",
    "# returns a list with the function calls\n",
    "def clean_stack_trace(stack_trace):\n",
    "    \n",
    "    clean_stack_trace = []\n",
    "    temp_stack        = stack_trace.split(\" at \")[1:]\n",
    "    \n",
    "    to_find = re.compile(\"[|,|<|>]|/|\\|=\")\n",
    "    \n",
    "    #find where each function ends and keep only the path\n",
    "    for f in temp_stack:\n",
    "        temp      = f.find(')')\n",
    "        temp_file = f[0:temp]\n",
    "        \n",
    "        # check the punctuations in order to avoid anything else\n",
    "        match_obj = to_find.search(temp_file)\n",
    "        if match_obj == None:\n",
    "            filename = find_filename(temp_file)\n",
    "            if filename != '':\n",
    "                clean_stack_trace.append(filename)\n",
    "    \n",
    "    return clean_stack_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "informational-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the name of the function and store only the file which contains the function.  \n",
    "# This is done by tracking full stops\n",
    "def find_filename(value):\n",
    "    filename = \"\"\n",
    "    words    = value.split(\"(\")\n",
    "    if len(words)>=2:\n",
    "        parts = words[0].split(\".\")\n",
    "        filename = \".\".join(parts[0:-1])\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "sporting-flexibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file data_word_emb72.json \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb143.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb188.json \n",
      "\n",
      "working on file data_word_emb98.json \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb239.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb108.json \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb46.json \n",
      "\n",
      "working on file data_word_emb31.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on file data_word_emb120.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on file data_word_emb32.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb82.json \n",
      "\n",
      "working on file data_word_emb228.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on file data_word_emb100.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on file data_word_emb114.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on file data_word_emb124.json \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb86.json \n",
      "\n",
      "working on file data_word_emb140.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on file data_word_emb94.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on file data_word_emb9.json \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb180.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on file data_word_emb13.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb147.json \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb144.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb81.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on file data_word_emb220.json \n",
      "\n",
      "working on file data_word_emb24.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb56.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb5.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on file data_word_emb237.json \n",
      "\n",
      "working on file data_word_emb221.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on file data_word_emb236.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb133.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb91.json \n",
      "\n",
      "working on file data_word_emb47.json \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb43.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb49.json \n",
      "\n",
      "working on file data_word_emb182.json \n",
      "\n",
      "working on file data_word_emb160.json \n",
      "\n",
      "working on file data_word_emb37.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on file data_word_emb197.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on file data_word_emb215.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb40.json \n",
      "\n",
      "working on file data_word_emb61.json \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on file data_word_emb119.json \n",
      "\n",
      "working on file data_word_emb85.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on file data_word_emb178.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb36.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on file data_word_emb242.json \n",
      "\n",
      "working on file data_word_emb187.json \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on file data_word_emb205.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb142.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb240.json \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb23.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb174.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb8.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb16.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb175.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb44.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on file data_word_emb139.json \n",
      "\n",
      "working on file data_word_emb7.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb208.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb186.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb202.json \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb97.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb78.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb111.json \n",
      "\n",
      "working on file data_word_emb123.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on file data_word_emb39.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on file data_word_emb62.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on file data_word_emb51.json \n",
      "\n",
      "working on file data_word_emb102.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb224.json \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb17.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb230.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb179.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb223.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on file data_word_emb4.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb69.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on file data_word_emb76.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb67.json \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb115.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb104.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb95.json \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb161.json \n",
      "\n",
      "working on file data_word_emb22.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on file data_word_emb34.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on file data_word_emb211.json \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb227.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb165.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb129.json \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb226.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on file data_word_emb238.json \n",
      "\n",
      "working on file data_word_emb172.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on file data_word_emb99.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on file data_word_emb128.json \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb194.json \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on file data_word_emb216.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb65.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on file data_word_emb96.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb199.json \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb126.json \n",
      "\n",
      "working on file data_word_emb206.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb26.json \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb152.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on file data_word_emb74.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb58.json \n",
      "\n",
      "working on file data_word_emb38.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on file data_word_emb201.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb3.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb2.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on file data_word_emb141.json \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on file data_word_emb18.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb158.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on file data_word_emb11.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb125.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on file data_word_emb213.json \n",
      "\n",
      "working on file data_word_emb59.json \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb218.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb1.json \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on file data_word_emb84.json \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb195.json \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb217.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb192.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb27.json \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb19.json \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb219.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb52.json \n",
      "\n",
      "working on file data_word_emb163.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb225.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb169.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb66.json \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb185.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on file data_word_emb243.json \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on file data_word_emb93.json \n",
      "\n",
      "working on file data_word_emb157.json \n",
      "\n",
      "working on file data_word_emb210.json \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb138.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb71.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb54.json \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb41.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb20.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on file data_word_emb14.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb60.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb134.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb88.json \n",
      "\n",
      "working on file data_word_emb189.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb64.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb35.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb79.json \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb168.json \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb171.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on file data_word_emb105.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb113.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb28.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb234.json \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb177.json \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb117.json \n",
      "\n",
      "working on file data_word_emb149.json \n",
      "\n",
      "working on file data_word_emb116.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb184.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb181.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on file data_word_emb103.json \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb229.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb70.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb73.json \n",
      "\n",
      "working on file data_word_emb146.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on file data_word_emb50.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on file data_word_emb131.json \n",
      "\n",
      "working on file data_word_emb106.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb118.json \n",
      "\n",
      "working on file data_word_emb127.json \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb170.json \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb162.json \n",
      "\n",
      "working on file data_word_emb48.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on file data_word_emb109.json \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb196.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb151.json \n",
      "\n",
      "working on file data_word_emb198.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on file data_word_emb121.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb214.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on file data_word_emb204.json \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb33.json \n",
      "\n",
      "working on file data_word_emb12.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb75.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb83.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb207.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on file data_word_emb233.json \n",
      "\n",
      "working on file data_word_emb200.json \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb80.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb191.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb30.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on file data_word_emb135.json \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb148.json \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on file data_word_emb63.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb25.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb92.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb110.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on file data_word_emb235.json \n",
      "\n",
      "working on file data_word_emb193.json \n",
      "\n",
      "working on file data_word_emb10.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on file data_word_emb222.json \n",
      "\n",
      "working on file data_word_emb145.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb150.json \n",
      "\n",
      "working on file data_word_emb6.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb155.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on file data_word_emb87.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb164.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file data_word_emb112.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb68.json \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on stack trace on issue 5 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb153.json \n",
      "\n",
      "working on file data_word_emb203.json \n",
      "\n",
      "working on file data_word_emb53.json \n",
      "\n",
      "working on file data_word_emb136.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 16 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb166.json \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on file data_word_emb90.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on file data_word_emb232.json \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on file data_word_emb77.json \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb183.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on file data_word_emb15.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb154.json \n",
      "\n",
      "working on stack trace on issue 12 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb159.json \n",
      "\n",
      "working on file data_word_emb45.json \n",
      "\n",
      "working on file data_word_emb209.json \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb167.json \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n",
      "working on file data_word_emb21.json \n",
      "\n",
      "working on file data_word_emb176.json \n",
      "\n",
      "working on stack trace on issue 9 \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on file data_word_emb241.json \n",
      "\n",
      "working on file data_word_emb55.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb190.json \n",
      "\n",
      "working on file data_word_emb29.json \n",
      "\n",
      "working on stack trace on issue 2 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 19 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb137.json \n",
      "\n",
      "working on stack trace on issue 13 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb42.json \n",
      "\n",
      "working on stack trace on issue 3 \n",
      "\n",
      "working on stack trace on issue 4 \n",
      "\n",
      "working on file data_word_emb231.json \n",
      "\n",
      "working on stack trace on issue 1 \n",
      "\n",
      "working on stack trace on issue 8 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 17 \n",
      "\n",
      "working on stack trace on issue 18 \n",
      "\n",
      "working on file data_word_emb57.json \n",
      "\n",
      "working on stack trace on issue 11 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb107.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb212.json \n",
      "\n",
      "working on stack trace on issue 24 \n",
      "\n",
      "working on file data_word_emb132.json \n",
      "\n",
      "working on file data_word_emb130.json \n",
      "\n",
      "working on stack trace on issue 10 \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on file data_word_emb156.json \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on file data_word_emb173.json \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on file data_word_emb89.json \n",
      "\n",
      "working on stack trace on issue 6 \n",
      "\n",
      "working on file data_word_emb101.json \n",
      "\n",
      "working on stack trace on issue 14 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 21 \n",
      "\n",
      "working on file data_word_emb122.json \n",
      "\n",
      "working on stack trace on issue 7 \n",
      "\n",
      "working on stack trace on issue 15 \n",
      "\n",
      "working on stack trace on issue 20 \n",
      "\n",
      "working on stack trace on issue 22 \n",
      "\n",
      "working on stack trace on issue 23 \n",
      "\n",
      "working on stack trace on issue 25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# proceed for every file in the directory\n",
    "for fname in os.listdir(dir_path):\n",
    "    with open(os.path.join(dir_path,fname)) as json_file:\n",
    "        \n",
    "        print(\"working on file\",fname,\"\\n\")\n",
    "        \n",
    "        #load data\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        for counter,issue in enumerate(data):\n",
    "            dirty_stack_trace = issue['stack_trace']\n",
    "            \n",
    "            if dirty_stack_trace != []:\n",
    "                print(\"working on stack trace on issue\",counter + 1,\"\\n\")\n",
    "                if len(dirty_stack_trace) > 1:\n",
    "                    dirty_stack_trace_1 = ''.join(dirty_stack_trace)\n",
    "                    stack_trace = clean_stack_trace(dirty_stack_trace_1)\n",
    "                else:\n",
    "                    stack_trace = clean_stack_trace(dirty_stack_trace[0])\n",
    "                \n",
    "                if stack_trace != []:\n",
    "                    if len(stack_trace)>1:\n",
    "                        st_traces_ls.append(stack_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-senior",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "organic-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function splits the dataset into training validation and testing set\n",
    "# it randomly selects test_size  stack traces for testing\n",
    "# it randomly selects valid_size stack traces for validation\n",
    "\n",
    "def split_dataset(st_traces_ls,valid_size,test_size):\n",
    "    \n",
    "    train_set  = list()\n",
    "    valid_set  = list()\n",
    "    test_set   = list()\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    for i in range(valid_size):\n",
    "        temp = randint(0,len(st_traces_ls)-1)\n",
    "        valid_set.append(st_traces_ls.pop(temp))\n",
    "    \n",
    "    for i in range(test_size):\n",
    "        temp = randint(0,len(st_traces_ls)-1)\n",
    "        test_set.append(st_traces_ls.pop(temp))\n",
    "    \n",
    "    train_set = [i for i in st_traces_ls]\n",
    "    \n",
    "    return train_set,valid_set,test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "compact-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_size = int(0.1*len(st_traces_ls))\n",
    "#test_size  = int(0.1*len(st_traces_ls))\n",
    "valid_size = 0\n",
    "test_size  = 0 \n",
    "train_set,validation_set,test_set = split_dataset(st_traces_ls,valid_size,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "academic-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total stack traces 471\n",
      "train set size 471\n",
      "validation set size 0\n",
      "test set size 0\n"
     ]
    }
   ],
   "source": [
    "# some print messages\n",
    "print(\"total stack traces\",len(st_traces_ls))\n",
    "print(\"train set size\",len(train_set))\n",
    "print(\"validation set size\",len(validation_set))\n",
    "print(\"test set size\",len(test_set))\n",
    "del st_traces_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-authentication",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "alone-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_func        = \"UNK\"\n",
    "vocabulary_size = 0\n",
    "min_occurance   = 2\n",
    "skip_window     = 2\n",
    "learning_rate   = 0.1\n",
    "embedding_dim   = 16\n",
    "num_sampled     = 64\n",
    "batch_size      = 2048\n",
    "false_neigh     = 32\n",
    "true_neigh      = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "transsexual-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(custom_dict):\n",
    "    \n",
    "    with open(\"stack_traces_vocabulary.txt\",\"w\") as file:\n",
    "        for key in custom_dict:\n",
    "            file.write(\"%s, %s \\n\"%(key,str(custom_dict[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "speaking-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_words(description,w_index,skip_window,span,grams_list):\n",
    "    \n",
    "    # the target word in the first place\n",
    "    grams_list.append(description[w_index])\n",
    "    \n",
    "    # initialize two pointers\n",
    "    counter = 1\n",
    "    data_index = w_index-1\n",
    "    \n",
    "    while counter < span:\n",
    "        # look left from target word\n",
    "        if counter<=skip_window:\n",
    "            # if data_index<0 => out of bound no more words to take into account\n",
    "            if data_index < 0:\n",
    "                data_index = w_index  + 1\n",
    "                counter    = skip_window + 1\n",
    "            # if the word is not in the dict skip it\n",
    "            elif description[data_index] == -2:\n",
    "                data_index -= 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index -= 1\n",
    "                if counter > skip_window:\n",
    "                    data_index = w_index + 1\n",
    "        # look right from target word\n",
    "        else:\n",
    "            if data_index >= len(description):\n",
    "                counter = span + 1\n",
    "            elif description[data_index] == -2:\n",
    "                data_index += 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "utility-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(train_set,skip_window):\n",
    "    \n",
    "    #find total instances\n",
    "    total_words = 0\n",
    "    for func in train_set:\n",
    "        total_words += len(func)\n",
    "    \n",
    "    #initialize the corpus which will keep all word pairs\n",
    "    max_size = total_words*2*skip_window\n",
    "    corpus = -1*np.ones((max_size,2), dtype=np.int32)\n",
    "    \n",
    "    # initialize pointers for the iterations\n",
    "    d_pointer  = 0\n",
    "    w_pointer  = 0\n",
    "    counter    = 0\n",
    "    \n",
    "    #initialize temporary buffer\n",
    "    span   = 2*skip_window+1 \n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    while counter< max_size:\n",
    "        \n",
    "        # avoid tags with -2\n",
    "        while train_set[d_pointer][w_pointer] < 0:\n",
    "            w_pointer += 1\n",
    "            if w_pointer > len(train_set[d_pointer])-1:\n",
    "                w_pointer  = 0\n",
    "                d_pointer +=1\n",
    "                if d_pointer > len(train_set) -1:\n",
    "                    break\n",
    "        \n",
    "        # check if all descriptions have been analyzed\n",
    "        if d_pointer > len(train_set)-1:\n",
    "            break\n",
    "        \n",
    "        find_context_words(train_set[d_pointer],w_pointer,skip_window,span,buffer)\n",
    "        \n",
    "        for i in range(1,len(buffer)):\n",
    "            corpus[counter][0] = buffer[0]\n",
    "            corpus[counter][1] = buffer[i]\n",
    "            counter += 1\n",
    "        \n",
    "        buffer.clear()\n",
    "        \n",
    "        if w_pointer == len(train_set[d_pointer]) -1:\n",
    "            w_pointer  = 0\n",
    "            d_pointer +=1\n",
    "            if d_pointer > len(train_set) -1:\n",
    "                break\n",
    "        else:\n",
    "            w_pointer += 1\n",
    "    \n",
    "    return corpus[0:counter].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "three-collapse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary based on the frequency of each word.\n",
    "# remove rare file names, which occurs less time than min_occurance, from vocabulary\n",
    "\n",
    "temp_list = [func for stack_trace in train_set for func in stack_trace]\n",
    "\n",
    "count     = []\n",
    "count.extend(collections.Counter(temp_list).most_common())\n",
    "count[:]  = [e for e in count if e[1]>=min_occurance]\n",
    "\n",
    "# list temp_sentences now is useless\n",
    "del temp_list\n",
    "\n",
    "# create vocabulary\n",
    "vocabulary_size = len(count)\n",
    "\n",
    "#assign an id to each function\n",
    "func2id = dict()\n",
    "func2id[unk_func] = -2\n",
    "\n",
    "for i,(func,_) in enumerate(count):\n",
    "    func2id[func] = i\n",
    "\n",
    "# list count now is useless\n",
    "print(len(count))\n",
    "del count\n",
    "\n",
    "train_set_id = [[func2id.get(func,-2) for func in i] for i in train_set]\n",
    "valid_set_id = [[func2id.get(func,-2) for func in i] for i in validation_set]\n",
    "test_set_id  = [[func2id.get(func,-2) for func in i] for i in test_set ]\n",
    "\n",
    "del train_set\n",
    "del validation_set\n",
    "del test_set\n",
    "\n",
    "# save the vocabulary\n",
    "save_vocabulary(func2id)\n",
    "\n",
    "# create corpus with word pairs\n",
    "corpus         = create_corpus(train_set_id,skip_window)\n",
    "corpus_indexes = [w for w in range(len(corpus))] \n",
    "\n",
    "# save them \n",
    "np.savetxt('stacks_corpus.txt',corpus,fmt=\"%d\")\n",
    "\n",
    "# train_set_id now is useless\n",
    "del train_set_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-mayor",
   "metadata": {},
   "source": [
    "### Validation and Test Pairs Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "reserved-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(arr,low,high):\n",
    "    \n",
    "    i = (low - 1)\n",
    "    pivot = arr[high][1]\n",
    "    \n",
    "    for j in range(low,high):\n",
    "        \n",
    "        if arr[j][1] >= pivot:\n",
    "            \n",
    "            i += 1\n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "    \n",
    "    arr[i+1],arr[high] = arr[high], arr[i+1]\n",
    "    return (i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "extreme-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickSort(arr,low,high):\n",
    "    if len(arr) ==1:\n",
    "        return arr\n",
    "    if low<high:\n",
    "        pi = partition(arr,low,high)\n",
    "        \n",
    "        quickSort(arr,low,pi-1)\n",
    "        quickSort(arr,pi+1,high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "dominican-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_pairs(test_set,min_occurance,num_words,skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # initialize temporary buffer\n",
    "    span   = skip_window*2+1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    # numerate all words in the dataset.\n",
    "    temp_sentences = [word for desc in test_set for word in desc]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(temp_sentences).most_common())\n",
    "    \n",
    "    # list temp_sentences now is useless\n",
    "    del temp_sentences\n",
    "    \n",
    "    # remove rare words\n",
    "    count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "    \n",
    "    # compute weights and select num_testing words\n",
    "    weights      = [e[1] for e in count if e[0] != -2]\n",
    "    total_weight = np.sum(weights)\n",
    "    \n",
    "    #for e in count:\n",
    "    #    if e[0] == -2:\n",
    "    #        continue\n",
    "    #    else:\n",
    "    #        weights.append(e[1])\n",
    "    #        total_weight += e[1]\n",
    "    \n",
    "    # generate random samples \n",
    "    weights[:] = [x/total_weight for x in weights]\n",
    "    indexes    = [i for i in range(len(count)) if count[i][0]!=-2]\n",
    "    samples    = np.random.choice(indexes,num_words,replace = False, p = weights)\n",
    "    \n",
    "    target_w   = [count[i][0] for i in samples]\n",
    "    w_dict     = dict([(key, [[],[]]) for key in target_w])\n",
    "    \n",
    "    for desc in test_set:\n",
    "        for w in target_w:\n",
    "            temp_idx = [i for i,e in enumerate(desc) if e == w]\n",
    "            for idx in temp_idx:\n",
    "                find_context_words(desc,idx,skip_window,span,buffer)\n",
    "                for i in range(1,len(buffer)):\n",
    "                    if w_dict[w][0] == []:\n",
    "                        w_dict[w][0].append([buffer[i],1])\n",
    "                    else:\n",
    "                        flag = False\n",
    "                        for neigh in w_dict[w][0]:\n",
    "                            if neigh[0] == buffer[i]:\n",
    "                                neigh[1] += 1\n",
    "                                flag = True\n",
    "                                break\n",
    "                        if flag == False:\n",
    "                            w_dict[w][0].append([buffer[i],1])\n",
    "                buffer.clear()\n",
    "    \n",
    "    for key in w_dict:\n",
    "        quickSort(w_dict[key][0],0,len(w_dict[key][0])-1)\n",
    "        print(key,w_dict[key][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "reverse-shepherd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58006704\n"
     ]
    }
   ],
   "source": [
    "target_emb = norm_embedding_matrix[24]\n",
    "neigh_emb  = norm_embedding_matrix[7]\n",
    "result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "blind-mathematics",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-199d42b86524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_testing_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_neigh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfalse_neigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-198-e68ea0eac0bd>\u001b[0m in \u001b[0;36mcreate_testing_pairs\u001b[0;34m(test_set, min_occurance, num_words, skip_window, true_neigh, false_neigh)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_weight\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mindexes\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msamples\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtarget_w\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "create_testing_pairs(valid_set_id,5,30,skip_window,true_neigh,false_neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-respect",
   "metadata": {},
   "source": [
    "## Stack's Embeddings Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "weekly-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "corporate-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(corpus_data,corpus_indexes,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size),   dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.int32)\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    words_to_use = random.sample(corpus_indexes,batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(words_to_use):\n",
    "        batch[counter]    = corpus_data[value][0]\n",
    "        labels[counter,0] = corpus_data[value][1] \n",
    "    \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "centered-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def_cpu(corpus_data,corpus_indexes,batch_size,embedding_dim,\n",
    "                  num_sampled,learning_rate,vocabulary_size):\n",
    "    \n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.int32, shape=[None])\n",
    "    # Input label\n",
    "    Y_train = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    \n",
    "    # ensure that the following ops & var are assigned to CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        # create the embedding variable wich contains the weights\n",
    "        embedding = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "        \n",
    "        # create the lookup table for each sample in X_train=>avoiding to use one_hot encoder\n",
    "        X_embed   = tf.nn.embedding_lookup(embedding,X_train)\n",
    "        \n",
    "        # create variables for the loss function\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_dim],stddev=1.0))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "    loss_func = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,biases =nce_biases,labels = Y_train,\n",
    "                                              inputs = X_embed,num_sampled = num_sampled,\n",
    "                                              num_classes = vocabulary_size ))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    train_opt = optimizer.minimize(loss_func)\n",
    "    \n",
    "    # Define initializer for tensorflow variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #actual initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # patience method's variables \n",
    "        min_loss           = float('inf')\n",
    "        min_emb_matrix     = np.zeros((vocabulary_size,embedding_dim))\n",
    "        patience_remaining = 100\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        #train the model using 100 epoch patience\n",
    "        for epoch in range(5000):\n",
    "            \n",
    "            # take a batch of data.\n",
    "            batch_x,batch_y = generate_batch(corpus_data,corpus_indexes,batch_size)\n",
    "            \n",
    "            _,loss = sess.run([train_opt,loss_func],feed_dict={X_train:batch_x, Y_train:batch_y})\n",
    "            patience_remaining -= 1\n",
    "            \n",
    "            if loss < min_loss:\n",
    "                min_loss           = loss\n",
    "                patience_remaining = 200\n",
    "                min_emb_matrix     = embedding.eval()\n",
    "                \n",
    "            if patience_remaining == 0:\n",
    "                break\n",
    "        \n",
    "        # normalize embeddings before using them\n",
    "        # restore min embeddings\n",
    "        embedding = tf.convert_to_tensor(min_emb_matrix)\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keepdims = True))\n",
    "        normalized_embedding = embedding/norm\n",
    "        normalized_embedding_matrix = sess.run(normalized_embedding)\n",
    "        \n",
    "        #measure total time\n",
    "        total_time = time.time() - start_time\n",
    "        print(\"training time in seconds %s \"%(str(total_time)))\n",
    "        print(\"total epochs was\",epoch)\n",
    "        \n",
    "    return normalized_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "informal-oregon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time in seconds 12.520550012588501 \n",
      "total epochs was 2128\n"
     ]
    }
   ],
   "source": [
    "norm_embedding_matrix = model_def_cpu(corpus,corpus_indexes,batch_size,embedding_dim,\n",
    "                                      num_sampled,learning_rate,vocabulary_size)\n",
    "\n",
    "np.savetxt('stack_embeddings_new.txt',norm_embedding_matrix,fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-auditor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
