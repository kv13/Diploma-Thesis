{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sonic-strand",
   "metadata": {},
   "source": [
    "# Stack Traces Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-century",
   "metadata": {},
   "source": [
    "The mechanism to compute word embeddings is used here to compute embeddings for stack traces in order to capture the sequence and the correlation of function calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-agency",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "studied-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from random   import seed\n",
    "from random   import randint\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-earth",
   "metadata": {},
   "source": [
    "### Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "existing-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary and the path to find the stack traces\n",
    "dir_path       = '../data'\n",
    "st_traces_ls   = list()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "indirect-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove text from the stack trace \n",
    "# and keep only the sequence of functions\n",
    "# returns a list with the function calls\n",
    "\n",
    "def clean_stack_trace(stack_trace):\n",
    "    \n",
    "    clean_stack_trace = []\n",
    "    temp_stack        = stack_trace.split(\" at \")[1:]\n",
    "    to_find           = re.compile(\"[|,|<|>]|/|\\|=\")\n",
    "    \n",
    "    # find where each function ends and keep only the path\n",
    "    for f in temp_stack:\n",
    "        temp      = f.find(')')\n",
    "        temp_file = f[0:temp]\n",
    "        \n",
    "        # check the punctuations in order to avoid anything else\n",
    "        match_obj = to_find.search(temp_file)\n",
    "        if match_obj == None:\n",
    "            filename = find_filename(temp_file)\n",
    "            if filename != '':\n",
    "                clean_stack_trace.append(filename)\n",
    "    \n",
    "    return clean_stack_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "constant-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the name of the function and store only the file which contains the function.  \n",
    "# This is done by tracking full stops\n",
    "def find_filename(value):\n",
    "    filename = \"\"\n",
    "    words    = value.split(\"(\")\n",
    "    if len(words)>=2:\n",
    "        parts = words[0].split(\".\")\n",
    "        filename = \".\".join(parts[0:-1])\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "rotary-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceed for every file in the directory\n",
    "total_stack_traces = 0\n",
    "\n",
    "for fname in os.listdir(dir_path):\n",
    "    with open(os.path.join(dir_path,fname)) as json_file:\n",
    "        \n",
    "        ##############################\n",
    "        #print(\"working on file\",fname,\"\\n\")\n",
    "        ##############################\n",
    "        \n",
    "        #load data\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        for counter,issue in enumerate(data):\n",
    "            dirty_stack_trace = issue['stack_trace']\n",
    "            \n",
    "            if dirty_stack_trace != []:\n",
    "                total_stack_traces += 1\n",
    "                \n",
    "                ##############################\n",
    "                #print(\"working on stack trace on issue\",counter + 1,\"\\n\")\n",
    "                ##############################\n",
    "                \n",
    "                if len(dirty_stack_trace) > 1:\n",
    "                    dirty_stack_trace_1 = ' '.join(dirty_stack_trace)\n",
    "                    stack_trace = clean_stack_trace(dirty_stack_trace_1)\n",
    "                else:\n",
    "                    stack_trace = clean_stack_trace(dirty_stack_trace[0])\n",
    "                \n",
    "                if stack_trace != []:\n",
    "                    if len(stack_trace)>1:\n",
    "                        st_traces_ls.append(stack_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-butter",
   "metadata": {},
   "source": [
    "#### Train-Validation-Test Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "tired-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function splits the dataset into training validation and testing set\n",
    "# it randomly selects test_size  stack traces for testing\n",
    "# it randomly selects valid_size stack traces for validation\n",
    "\n",
    "def split_dataset(st_traces_ls,valid_size,test_size):\n",
    "    \n",
    "    train_set  = list()\n",
    "    valid_set  = list()\n",
    "    test_set   = list()\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    for i in range(valid_size):\n",
    "        temp = randint(0,len(st_traces_ls)-1)\n",
    "        valid_set.append(st_traces_ls.pop(temp))\n",
    "        \n",
    "    for i in range(test_size):\n",
    "        temp = randint(0,len(st_traces_ls)-1)\n",
    "        test_set.append(st_traces_ls.pop(temp))\n",
    "        \n",
    "    train_set = [i for i in st_traces_ls]\n",
    "    return train_set,valid_set,test_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "indian-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = int(0.2*len(st_traces_ls))\n",
    "test_size  = int(0.1*len(st_traces_ls))\n",
    "\n",
    "valid_funcs  = 20\n",
    "valid_funcs2 = 40\n",
    "test_funcs   = 30\n",
    "\n",
    "train_set,validation_set,test_set = split_dataset(st_traces_ls,valid_size,test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "freelance-operator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size 339\n",
      "validation set size 96\n",
      "test set size 48\n",
      "total stack traces 531\n"
     ]
    }
   ],
   "source": [
    "# some print messages\n",
    "#print(\"total stack traces\",total_stack_traces)\n",
    "print(\"train set size\",len(train_set))\n",
    "print(\"validation set size\",len(validation_set))\n",
    "print(\"test set size\",len(test_set))\n",
    "print(\"total stack traces\",total_stack_traces)\n",
    "del st_traces_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-floating",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "designed-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_func        = \"UNK\"\n",
    "vocabulary_size = 0\n",
    "min_occurance   = 2\n",
    "skip_window     = 2\n",
    "learning_rate   = 0.1\n",
    "\n",
    "embedding_dim   = 8\n",
    "num_sampled     = 32\n",
    "batch_size      = 2048\n",
    "false_neigh     = 10\n",
    "true_neigh      = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "regulated-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(custom_dict):\n",
    "    \n",
    "    with open(\"../outputs/stacktraces_vocabulary_g.txt\",\"w\") as file:\n",
    "        for key in custom_dict:\n",
    "            file.write(\"%s, %s \\n\"%(key,str(custom_dict[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "infectious-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(train_set,skip_window):\n",
    "    \n",
    "    # find total instances\n",
    "    total_words = 0\n",
    "    for func in train_set:\n",
    "        total_words += len(func)\n",
    "        \n",
    "    # initialize the corpus which will keep all word pairs\n",
    "    max_size = total_words*2*skip_window\n",
    "    corpus   = -1*np.ones((max_size,2), dtype=np.int32)\n",
    "    \n",
    "    # initialize pointers for the iterations\n",
    "    d_pointer  = 0\n",
    "    w_pointer  = 0\n",
    "    counter    = 0\n",
    "    \n",
    "    #initialize temporary buffer\n",
    "    span   = 2*skip_window+1 \n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    while counter< max_size:\n",
    "        \n",
    "        # avoid tags with -2\n",
    "        while train_set[d_pointer][w_pointer] < 0:\n",
    "            w_pointer += 1\n",
    "            if w_pointer > len(train_set[d_pointer])-1:\n",
    "                w_pointer  = 0\n",
    "                d_pointer += 1\n",
    "                if d_pointer > len(train_set) -1:\n",
    "                    break\n",
    "        \n",
    "        # check if all descriptions have been analyzed\n",
    "        if d_pointer > len(train_set)-1:\n",
    "            break\n",
    "        \n",
    "        find_context_words(train_set[d_pointer],w_pointer,skip_window,span,buffer)\n",
    "        \n",
    "        for i in range(1,len(buffer)):\n",
    "            corpus[counter][0] = buffer[0]\n",
    "            corpus[counter][1] = buffer[i]\n",
    "            counter += 1\n",
    "            \n",
    "        buffer.clear()\n",
    "        \n",
    "        if w_pointer == len(train_set[d_pointer]) -1:\n",
    "            w_pointer  = 0\n",
    "            d_pointer += 1\n",
    "            if d_pointer > len(train_set) -1:\n",
    "                break\n",
    "        else:\n",
    "            w_pointer += 1\n",
    "    \n",
    "    return corpus[0:counter].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "equal-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_words(description,w_index,skip_window,span,grams_list):\n",
    "    \n",
    "    # the target word in the first place\n",
    "    grams_list.append(description[w_index])\n",
    "    \n",
    "    # initialize two pointers\n",
    "    counter = 1\n",
    "    data_index = w_index-1\n",
    "    \n",
    "    while counter < span:\n",
    "        # look left from target word\n",
    "        if counter<=skip_window:\n",
    "            # if data_index<0 => out of bound no more words to take into account\n",
    "            if data_index < 0:\n",
    "                data_index = w_index  + 1\n",
    "                counter    = skip_window + 1\n",
    "            # if the word is not in the dict skip it\n",
    "            elif description[data_index] == -2:\n",
    "                data_index -= 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index -= 1\n",
    "                if counter > skip_window:\n",
    "                    data_index = w_index + 1\n",
    "        # look right from target word\n",
    "        else:\n",
    "            if data_index >= len(description):\n",
    "                counter = span + 1\n",
    "            elif description[data_index] == -2:\n",
    "                data_index += 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "nervous-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary based on the frequency of each file name.\n",
    "# remove rare file names, which occurs less time than min_occurance, from vocabulary\n",
    "\n",
    "temp_list = [func for stack_trace in train_set for func in stack_trace]\n",
    "\n",
    "count     = []\n",
    "count.extend(collections.Counter(temp_list).most_common())\n",
    "count[:]  = [e for e in count if e[1]>=min_occurance]\n",
    "\n",
    "# list temp_sentences now is useless\n",
    "del temp_list\n",
    "\n",
    "# create vocabulary\n",
    "vocabulary_size = len(count)\n",
    "\n",
    "# assign an id to each function\n",
    "func2id = dict()\n",
    "func2id[unk_func] = -2\n",
    "\n",
    "for i,(func,_) in enumerate(count):\n",
    "    func2id[func] = i\n",
    "\n",
    "# list count now is useless\n",
    "# print(len(count))\n",
    "del count\n",
    "\n",
    "train_set_id = [[func2id.get(func,-2) for func in i] for i in train_set]\n",
    "valid_set_id = [[func2id.get(func,-2) for func in i] for i in validation_set]\n",
    "test_set_id  = [[func2id.get(func,-2) for func in i] for i in test_set ]\n",
    "\n",
    "del train_set\n",
    "del validation_set\n",
    "del test_set\n",
    "\n",
    "# save the vocabulary\n",
    "save_vocabulary(func2id)\n",
    "\n",
    "# create corpus with word pairs\n",
    "corpus         = create_corpus(train_set_id,skip_window)\n",
    "corpus_indexes = [w for w in range(len(corpus))] \n",
    "\n",
    "# save them \n",
    "np.savetxt('../outputs/stacktraces_corpus_g.txt',corpus,fmt=\"%d\")\n",
    "\n",
    "# train_set_id now is useless\n",
    "del train_set_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-blowing",
   "metadata": {},
   "source": [
    "### Validation and Test Pairs Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "mobile-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_dict(test_set,min_occurance,num_words,num_words2,\n",
    "                        skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # numerate all funcs in the dataset.\n",
    "    temp_funcs = [func for stack_trace in test_set for func in stack_trace]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(temp_funcs).most_common())\n",
    "    \n",
    "    # list temp_funcs now is useless\n",
    "    del temp_funcs\n",
    "    \n",
    "    # remove rare items\n",
    "    count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "    indexes  = [i for i in range(len(count)) if count[i][0] != -2]\n",
    "    \n",
    "    # split validation set into two sets one small used for cross entropy computation\n",
    "    # and the other at the end to meassure results.\n",
    "    if num_words2>0:\n",
    "        \n",
    "        samples2  = np.random.choice(indexes,num_words2,replace = False)\n",
    "        target_w2 = [count[i][0] for i in samples2]\n",
    "        w_dict2   = create_testing_pairs(test_set,count,target_w2,indexes,\n",
    "                                         skip_window,true_neigh,false_neigh)\n",
    "        \n",
    "        # test on the \"num_words\" most frequent items\n",
    "        tmp_indexes = [i for i in indexes if i not in samples2]\n",
    "        target_w    = [count[tmp_indexes[i]][0] for i in range(num_words)]\n",
    "        w_dict      = create_testing_pairs(test_set,count,target_w,indexes,\n",
    "                                           skip_window,true_neigh,false_neigh)\n",
    "        return w_dict2,w_dict\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # test on the \"num_words\" most frequent items\n",
    "        target_w = [count[indexes[i]][0] for i in range(num_words)]\n",
    "        w_dict   = create_testing_pairs(test_set,count,target_w,indexes,skip_window,true_neigh,false_neigh)\n",
    "        return None,w_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "pleased-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_pairs(test_set,count,target_w,indexes,\n",
    "                         skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # initialize temporary buffer\n",
    "    span   = skip_window*2+1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    # initialize dictionary\n",
    "    w_dict   = dict([(key, [[],[]]) for key in target_w])\n",
    "    \n",
    "    # find true neighbors for target words\n",
    "    for desc in test_set:\n",
    "        for w in target_w:\n",
    "            temp_idx = [i for i,e in enumerate(desc) if w == e]\n",
    "            for idx in temp_idx:\n",
    "                find_context_words(desc,idx,skip_window,span,buffer)\n",
    "                for i in range(1,len(buffer)):\n",
    "                    if w_dict[w][0] == []:\n",
    "                        w_dict[w][0].append(buffer[i])\n",
    "                    elif buffer[i] not in w_dict[w][0]:\n",
    "                        w_dict[w][0].append(buffer[i])\n",
    "                        \n",
    "    # find false neigbors\n",
    "    for key in w_dict:\n",
    "        neig_counter = 0\n",
    "        flag         = True\n",
    "        \n",
    "        while flag == True:\n",
    "            random_idx   = np.random.choice(indexes,2*false_neigh,replace = False)\n",
    "            for idx in random_idx:\n",
    "                if count[idx][0] == key:\n",
    "                    continue\n",
    "                elif count[idx][0] in w_dict[key][0]:\n",
    "                    continue\n",
    "                elif count[idx][0] not in w_dict[key][1]:\n",
    "                    w_dict[key][1].append(count[idx][0])\n",
    "                    neig_counter += 1\n",
    "                    if neig_counter >= false_neigh:\n",
    "                        flag = False\n",
    "                        break\n",
    "    \n",
    "    # choose randomly only true_neigh neighbors.\n",
    "    removed_keys = []\n",
    "    for key in w_dict:\n",
    "        if len(w_dict[key][0])>=true_neigh:\n",
    "            idx_neigh =  np.random.choice([i for i in range(len(w_dict[key][0]))],true_neigh,replace = False)\n",
    "            w_dict[key][0] = [w_dict[key][0][i] for i in idx_neigh]\n",
    "        else:\n",
    "            removed_keys.append(key)\n",
    "    \n",
    "    if removed_keys != []:\n",
    "        for key in removed_keys:\n",
    "            w_dict.pop(key)\n",
    "    \n",
    "    return w_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aad2daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_pairs(test_dict):\n",
    "    \n",
    "    with open('../outputs/stack_testing_pairs_g.pkl','wb') as file:\n",
    "        pickle.dump(test_dict,file,pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cellular-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,test_dict = create_testing_dict(test_set_id,1,test_funcs,0,2,true_neigh,false_neigh)\n",
    "save_test_pairs(test_dict)\n",
    "#del test_dict\n",
    "\n",
    "\n",
    "v_dict2,v_dict = create_testing_dict(valid_set_id,1,valid_funcs,\n",
    "                                     valid_funcs2,2,true_neigh,false_neigh)\n",
    "\n",
    "t_batch  = []\n",
    "t_label  = []\n",
    "for key in v_dict2:\n",
    "    for value in v_dict2[key][0]:\n",
    "        t_batch.append(key)\n",
    "        t_label.append(value)\n",
    "v_batch = np.reshape(t_batch,(len(t_batch),))\n",
    "v_label = np.reshape(t_label,(len(t_label),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-editor",
   "metadata": {},
   "source": [
    "## Stack's Embeddings Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "inside-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "functional-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(corpus_data,corpus_indexes,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size),   dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.int32)\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    words_to_use = random.sample(corpus_indexes,batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(words_to_use):\n",
    "        batch[counter]    = corpus_data[value][0]\n",
    "        labels[counter,0] = corpus_data[value][1] \n",
    "    \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "lesser-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def_cpu(corpus_data,corpus_indexes,batch_size,embedding_dim,\n",
    "                  num_sampled,learning_rate,vocabulary_size,v_batch,v_labels):\n",
    "    \n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.int32, shape=[None])\n",
    "    # Input label\n",
    "    Y_train = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    \n",
    "    # ensure that the following ops & var are assigned to CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        # create the embedding variable wich contains the weights\n",
    "        initializer = initializers.GlorotNormal()\n",
    "        embedding   = tf.Variable(initializer(shape=(vocabulary_size,embedding_dim)))\n",
    "        #embedding = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "        \n",
    "        # create the lookup table for each sample in X_train=>avoiding to use one_hot encoder\n",
    "        X_embed   = tf.nn.embedding_lookup(embedding,X_train)\n",
    "        \n",
    "        # create variables for the loss function\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_dim],\n",
    "                                                      stddev=1.0/ math.sqrt(embedding_dim)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "    \n",
    "    loss_func = tf.reduce_sum(tf.nn.nce_loss(weights = nce_weights,biases =nce_biases,labels = Y_train,\n",
    "                                              inputs = X_embed,num_sampled = num_sampled,\n",
    "                                              num_classes = vocabulary_size ))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_opt = optimizer.minimize(loss_func)\n",
    "    \n",
    "    # Define initializer for tensorflow variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # actual initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # patience method's variables\n",
    "        min_loss           = float('inf')\n",
    "        min_emb_matrix     = np.zeros((vocabulary_size,embedding_dim))\n",
    "        step               = skip_window*batch_size/len(corpus_indexes) \n",
    "        print(\"the step is\",step)\n",
    "        patience_remaining = 100\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train the model using 100 epoch patience\n",
    "        for epoch in range(50000):\n",
    "            \n",
    "            # take a batch of data.\n",
    "            batch_x,batch_y = generate_batch(corpus_data,corpus_indexes,batch_size)\n",
    "            \n",
    "            _,loss = sess.run([train_opt,loss_func],feed_dict={X_train:batch_x, Y_train:batch_y})\n",
    "            valid_loss   = sess.run(loss_func,feed_dict={X_train:v_batch, Y_train:v_labels})\n",
    "            \n",
    "            patience_remaining     = patience_remaining - step\n",
    "            if valid_loss < min_loss:\n",
    "                min_loss           = valid_loss\n",
    "                patience_remaining = 100\n",
    "                min_emb_matrix     = embedding.eval()\n",
    "            if patience_remaining <= 0:\n",
    "                break\n",
    "        \n",
    "        # restore min embeddings\n",
    "        embedding = tf.convert_to_tensor(min_emb_matrix)\n",
    "        \n",
    "        # normalize embeddings before using them\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keepdims = True))\n",
    "        normalized_embedding = embedding/norm\n",
    "        normalized_embedding_matrix = sess.run(normalized_embedding)\n",
    "        \n",
    "        #measure total time\n",
    "        total_time = time.time() - start_time\n",
    "        print(\"training time in seconds %s \"%(str(total_time)))\n",
    "        print(\"total epochs was\",epoch+1)\n",
    "        print(\"minimum loss is \",min_loss)\n",
    "    \n",
    "    return normalized_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "israeli-baghdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the step is 0.11314917127071823\n",
      "training time in seconds 16.883629322052002 \n",
      "total epochs was 2692\n",
      "minimum loss is  245.16527\n"
     ]
    }
   ],
   "source": [
    "norm_embedding_matrix = model_def_cpu(corpus,corpus_indexes,batch_size,embedding_dim,\n",
    "                                      num_sampled,learning_rate,vocabulary_size,v_batch,v_label)\n",
    "\n",
    "np.savetxt('../results/stack_embeddings_g.txt',norm_embedding_matrix,fmt='%.8f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-phoenix",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "excess-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model computes tpr, fpr and auc. The classes are class_A = real neighbor\n",
    "# and class_B = false neighbor. The model based on cosine similarity\n",
    "# will try to predict the right label for each word pair given.\n",
    "def model_validation_v2(embedding_matrix,words_dict):\n",
    "    \n",
    "    ylabels = list()\n",
    "    ypreds  = list()\n",
    "    \n",
    "    for key in words_dict:\n",
    "        target_emb = embedding_matrix[key]\n",
    "        for true_neigh in words_dict[key][0]:\n",
    "            neigh_emb = embedding_matrix[true_neigh]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            ylabels.append(1)\n",
    "            ypreds.append(result)\n",
    "            \n",
    "        for false_neigh in words_dict[key][1]:\n",
    "            neigh_emb = embedding_matrix[false_neigh]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            ylabels.append(0)\n",
    "            ypreds.append(result)\n",
    "    \n",
    "    y = np.array(ylabels)\n",
    "    score = np.array(ypreds)\n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y,score)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "silver-shore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.895625\n"
     ]
    }
   ],
   "source": [
    "auc = model_validation_v2(norm_embedding_matrix,v_dict)\n",
    "print(\"Validation AUC:\",auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13f09e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AUC: 0.8558888888888889\n"
     ]
    }
   ],
   "source": [
    "# unpickling test dictionary\n",
    "with open('../outputs/stack_testing_pairs_g.pkl','rb') as infile:\n",
    "    testing_dict = pickle.load(infile)\n",
    "\n",
    "auc = model_validation_v2(norm_embedding_matrix,testing_dict)\n",
    "print(\"Testing AUC:\",auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be701d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
