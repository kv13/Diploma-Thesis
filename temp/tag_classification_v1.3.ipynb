{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc15fa6e",
   "metadata": {},
   "source": [
    "# Tag Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7026519",
   "metadata": {},
   "source": [
    "This notebook contains only implementations based on neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d2d9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34376414",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "262a3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path_to_file):\n",
    "    temp_dict = dict()\n",
    "    with open(path_to_file) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            temp   = str(line)\n",
    "            values = temp.split(',')\n",
    "            temp_dict[values[0]] = int(values[1].replace(\"\\n\",\"\"))\n",
    "    \n",
    "    return temp_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "433f7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_issues(dir_path,tag_labels,descriptions,stack_traces):\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            data = json.load(json_file)\n",
    "            for issue in data:\n",
    "                \n",
    "                tags = issue['tags']\n",
    "                for i in range(len(tags)):\n",
    "                    tags[i] = tags[i].strip()\n",
    "                description = issue['description']\n",
    "                stack_trace = issue['stack_trace']\n",
    "                name        = issue['name']\n",
    "                \n",
    "                if tags != [] and stack_trace !=[] : #(description != [] or stack_trace != []):\n",
    "                    tag_labels.append(tags)\n",
    "                    descriptions.append(description)\n",
    "                    stack_traces.append(stack_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee1bdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def clean_stack_trace(stack_trace):\n",
    "    \n",
    "    clean_stack_trace = []\n",
    "    temp_stack        = stack_trace.split(\" at \")[1:]\n",
    "    \n",
    "    to_find = re.compile(\"[|,|<|>]|/|\\|=\")\n",
    "    \n",
    "    #find where each function ends and keep only the path\n",
    "    for f in temp_stack:\n",
    "        temp      = f.find(')')\n",
    "        temp_file = f[0:temp]\n",
    "        \n",
    "        # check the punctuations in order to avoid anything else\n",
    "        match_obj = to_find.search(temp_file)\n",
    "        if match_obj == None:\n",
    "            filename = find_filename(temp_file)\n",
    "            if filename != '':\n",
    "                clean_stack_trace.append(filename)\n",
    "                \n",
    "    return clean_stack_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02c987fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def find_filename(value):\n",
    "    filename = \"\"\n",
    "    words    = value.split(\"(\")\n",
    "    if len(words)>=2:\n",
    "        parts = words[0].split(\".\")\n",
    "        filename = \".\".join(parts[0:-1])\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e546faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from word embeddings notebook\n",
    "\n",
    "def clean_description(description):\n",
    "    \n",
    "    # define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    #define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    #join all lines into one sentence\n",
    "    sentence     = ' '.join(description)\n",
    "    \n",
    "    #translate punctuation\n",
    "    new_sentence = sentence.translate(translator)\n",
    "    \n",
    "    #split the sentense in words\n",
    "    words = new_sentence.split()\n",
    "    \n",
    "    words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "    \n",
    "    return words_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f7425c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from word embeddings notebook\n",
    "\n",
    "def stemming_data(descriptions):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        for counter in range(len(desc)):\n",
    "            if desc[counter].isalpha():\n",
    "                desc[counter] = stemmer.stem(desc[counter])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ace7ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(descriptions,stack_traces,use_stemming):\n",
    "    \n",
    "    clean_descriptions = list()\n",
    "    clean_stack_traces = list()\n",
    "    \n",
    "    for i in range(len(descriptions)):\n",
    "        \n",
    "        temp_desc   = descriptions[i]\n",
    "        temp_trace  = stack_traces[i]\n",
    "        stack_trace = []\n",
    "        clean_desc  = []\n",
    "        \n",
    "        if temp_trace != []:\n",
    "            if len(temp_trace)>1:\n",
    "                stack_trace = clean_stack_trace(' '.join(temp_trace))\n",
    "            else:\n",
    "                stack_trace = clean_stack_trace(temp_trace[0])\n",
    "            \n",
    "        if temp_desc  != []:\n",
    "            clean_desc = clean_description(temp_desc)\n",
    "            \n",
    "        clean_descriptions.append(clean_desc)\n",
    "        clean_stack_traces.append(stack_trace)\n",
    "            \n",
    "    if use_stemming == True:\n",
    "        stemming_data(clean_descriptions)\n",
    "        \n",
    "    return clean_descriptions,clean_stack_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e75f3",
   "metadata": {},
   "source": [
    "## Compute Arithmetic Representations for Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79aa9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(arithmetic_descriptions,arithmetic_stack_traces,\n",
    "                       word_embedding_matrix,stack_embedding_matrix,use_words,use_stacks):\n",
    "    \n",
    "    total_embeddings_dim = 0\n",
    "    descriptions_dim     = 0\n",
    "    stack_traces_dim     = 0\n",
    "    \n",
    "    if use_words == True:\n",
    "        descriptions_dim     = np.shape(word_embedding_matrix)[1]\n",
    "        total_embeddings_dim = total_embeddings_dim + descriptions_dim\n",
    "        \n",
    "    if use_stacks == True:\n",
    "        stack_traces_dim     = np.shape(stack_embedding_matrix)[1]\n",
    "        total_embeddings_dim = total_embeddings_dim + stack_traces_dim\n",
    "    \n",
    "    # make sure that in any case there are something to compute\n",
    "    if total_embeddings_dim ==0:\n",
    "        return None\n",
    "    \n",
    "    num_issues        = len(arithmetic_descriptions)\n",
    "    issues_embeddings = np.zeros((num_issues,total_embeddings_dim))\n",
    "    \n",
    "    for counter in range(len(arithmetic_descriptions)):\n",
    "        \n",
    "        temp_desc   = arithmetic_descriptions[counter]\n",
    "        temp_stack  = arithmetic_stack_traces[counter]\n",
    "        total_words = 0\n",
    "        total_funcs = 0\n",
    "        \n",
    "        if use_words == True:\n",
    "            for word in temp_desc:\n",
    "                if word != -2:\n",
    "                    total_words += 1\n",
    "                    issues_embeddings[counter][0:descriptions_dim] = issues_embeddings[counter][0:descriptions_dim] + word_embedding_matrix[word]\n",
    "            if total_words != 0 :\n",
    "                issues_embeddings[counter]    /= total_words\n",
    "        \n",
    "        \n",
    "        if use_stacks == True:\n",
    "            for func in temp_stack:\n",
    "                if func != -2:\n",
    "                    issues_embeddings[counter][descriptions_dim:] = issues_embeddings[counter][descriptions_dim:] + stack_embedding_matrix[func]\n",
    "                    total_funcs += 1\n",
    "            if total_funcs != 0:\n",
    "                issues_embeddings[counter][descriptions_dim:] = issues_embeddings[counter][descriptions_dim:] / total_funcs \n",
    "            \n",
    "    return issues_embeddings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9557ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stemming = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "181d9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "word_embedding_matrix = np.loadtxt('../results/word_embeddings_g.txt', dtype=np.float64)\n",
    "\n",
    "# load stack traces embeddings \n",
    "stack_embedding_matrix = np.loadtxt('../results/stack_embeddings_g.txt', dtype=np.float64)\n",
    "\n",
    "# load vocabularies\n",
    "word2id_path = \"../outputs/words_vocabulary_g.txt\"\n",
    "func2id_path = \"../outputs/stacktraces_vocabulary_g.txt\"\n",
    "\n",
    "word2id = load_dict(word2id_path)\n",
    "func2id = load_dict(func2id_path)\n",
    "\n",
    "#load tags and descriptions\n",
    "dir_path     = '../data'\n",
    "tag_labels   = list()\n",
    "descriptions = list()\n",
    "stack_traces = list()\n",
    "\n",
    "# load issues\n",
    "load_issues(dir_path,tag_labels,descriptions,stack_traces)\n",
    "\n",
    "# transform data to arithmetic representation\n",
    "clean_descriptions,clean_stack_traces = clean_data(descriptions,stack_traces,use_stemming)\n",
    "\n",
    "clean_descriptions_2 = list()\n",
    "clean_stack_traces_2 = list()\n",
    "clean_tags_2         = list()\n",
    "\n",
    "# remove empty stack traces\n",
    "for counter,value in enumerate(clean_stack_traces):\n",
    "    if value != []:\n",
    "        clean_stack_traces_2.append(value)\n",
    "        clean_descriptions_2.append(clean_descriptions[counter])\n",
    "        clean_tags_2.append(tag_labels[counter])\n",
    "\n",
    "del clean_descriptions\n",
    "del clean_stack_traces\n",
    "\n",
    "del descriptions\n",
    "del stack_traces\n",
    "\n",
    "#arithmetic_transformations\n",
    "arithmetic_descriptions = [[word2id.get(word,-2) for word in desc]   for desc in clean_descriptions_2]\n",
    "arithmetic_stack_traces = [[func2id.get(func,-2) for func in trace] for trace in clean_stack_traces_2]\n",
    "\n",
    "del clean_descriptions_2\n",
    "del clean_stack_traces_2\n",
    "\n",
    "issues_embeddings  = compute_embeddings(arithmetic_descriptions,arithmetic_stack_traces,\n",
    "                                        word_embedding_matrix,stack_embedding_matrix,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "055d7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_labels = list()\n",
    "# copy by reference in order to avoid to change every where the variable name\n",
    "tag_labels = clean_tags_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbc32954",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['Bug','Google Play or Beta feedback','Prio - High']\n",
    "no_tags = 3\n",
    "np_tags = np.zeros((len(arithmetic_descriptions),no_tags))\n",
    "\n",
    "for counter in range(len(tag_labels)):\n",
    "    for counter_2,value in enumerate(tags):\n",
    "        if value in tag_labels[counter]:\n",
    "            np_tags[counter][counter_2] = 1\n",
    "            \n",
    "df_tags = pd.DataFrame(np_tags, columns = tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675393a8",
   "metadata": {},
   "source": [
    "## Neural Network Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a26ab18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26fb42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset2(issues_embeddings,target_labels,t_size =0.1):\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = t_size, random_state = 0)\n",
    "    \n",
    "    X_train_0 = list()\n",
    "    X_train_1 = list()\n",
    "    \n",
    "    for train_index, test_index in sss.split(issues_embeddings,target_labels):\n",
    "        #X_train,X_test = issues_embeddings[train_index], issues_embeddings[test_index]\n",
    "        #Y_train,Y_test = target_labels[train_index], target_labels[test_index]\n",
    "        \n",
    "        \n",
    "        X_test = issues_embeddings[test_index]\n",
    "        Y_test = target_labels[test_index]\n",
    "        \n",
    "        for index in train_index:\n",
    "            if target_labels.iloc[index] == 0:\n",
    "                X_train_0.append(issues_embeddings[index])\n",
    "            elif target_labels.iloc[index] == 1:\n",
    "                X_train_1.append(issues_embeddings[index])\n",
    "                \n",
    "    return X_train_0,X_train_1,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d5b22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(issues_embeddings,target_labels,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,2), dtype = np.float64)\n",
    "    \n",
    "    issues_to_use = random.sample([i for i in range(np.shape(issues_embeddings)[0])],batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(issues_to_use):\n",
    "        batch[counter][:]  = issues_embeddings[value][:]\n",
    "        # label_0\n",
    "        labels[counter][0] = 1-target_labels.iloc[value]\n",
    "        # label_1\n",
    "        labels[counter][1] =   target_labels.iloc[value]\n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dce3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(issues_embeddings_0, issues_embeddings_1, batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings_0)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,2), dtype = np.float64)\n",
    "    \n",
    "    issues_to_use_0 = random.sample([i for i in range(np.shape(issues_embeddings_0)[0])],batch_size//2)\n",
    "    issues_to_use_1 = random.sample([i for i in range(np.shape(issues_embeddings_1)[0])],batch_size//2)\n",
    "    \n",
    "    # even indexes for issues belong to class 0\n",
    "    # odd  indexes for issues belong to class 1\n",
    "    counter_0 = 0\n",
    "    counter_1 = 0\n",
    "    \n",
    "    for counter in range(batch_size):\n",
    "        \n",
    "        # even indexes\n",
    "        if counter%2 == 0 :\n",
    "            batch[counter][:]  = issues_embeddings_0[issues_to_use_0[counter_0]][:]\n",
    "            labels[counter][0] = 1\n",
    "            labels[counter][1] = 0\n",
    "            counter_0 += 1\n",
    "        else:\n",
    "            batch[counter][:]  = issues_embeddings_1[issues_to_use_1[counter_1]][:]\n",
    "            labels[counter][0] = 0\n",
    "            labels[counter][1] = 1\n",
    "            counter_1 += 1\n",
    "            \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc899dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(y_probs,v_labels):\n",
    "    \n",
    "    y_probs_1 = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "    y_preds_1 = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "    y_true_1  = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64) \n",
    "    \n",
    "    for i in range(np.shape(v_labels)[0]):\n",
    "        y_true_1[i]  = v_labels[i][1]\n",
    "        y_preds_1[i] = 0 if y_probs[i][0]>y_probs[i][1] else 1\n",
    "        y_probs_1[i] = y_probs[i][1]\n",
    "    \n",
    "    matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds_1)\n",
    "    \n",
    "    return y_probs_1, y_preds_1, y_true_1, matrix_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfed1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(y_true,y_probs):\n",
    "    \n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y_true,y_probs)\n",
    "    auc                = metrics.auc(fpr,tpr)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe169dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(total_confusion,aucs):\n",
    "    \n",
    "    acc = (total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion)\n",
    "    \n",
    "    gm  = np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "              (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0])))\n",
    "    \n",
    "    pre = total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0])\n",
    "    \n",
    "    mean_auc = np.sum(aucs)/np.shape(aucs)[0]\n",
    "\n",
    "    print(\"accuracy\" , acc)\n",
    "    print(\"precision\", pre)\n",
    "    print(\"GM\"       , gm)\n",
    "    print(\"mean auc\" , mean_auc)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ca19b",
   "metadata": {},
   "source": [
    "#### First and Simpliest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25a6b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn2(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "     \n",
    "    output_layer   = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2 = tf.nn.softmax(output_layer)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "        \n",
    "        # saving the weights in numpy format\n",
    "        #W1_np = W1.eval()\n",
    "        #b1_np = b1.eval()\n",
    "        #W2_np = W2.eval()\n",
    "        #b2_np = b2.eval()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "    \n",
    "    return compute_predictions(y_probs,v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8665a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91b087b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# first implementation the simpliest neural network\n",
    "# use both word embeddings and stack traces embeddings.\n",
    "\n",
    "target_labels = df_tags[\"Bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bb5833e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7195652173913043\n",
      "precision 0.7073170731707317\n",
      "GM 0.7615773105863908\n",
      "mean auc 0.8131707317073171\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_iterations = 10\n",
    "total_confusion  = np.zeros((2,2))\n",
    "conf_matrix      = np.zeros((2,2))\n",
    "aucs             = np.zeros(total_iterations)\n",
    "\n",
    "for i in range(total_iterations):\n",
    "    y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,16,0.01,\n",
    "                                                            2*batch_size,100,v_batch,v_labels)\n",
    "    total_confusion = total_confusion + conf_matrix\n",
    "    aucs[i]         = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "compute_metrics(total_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82965567",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5868d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"Google Play or Beta feedback\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51c6e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8695652173913043\n",
      "precision 0.8235294117647058\n",
      "GM 0.8592652174945423\n",
      "mean auc 0.8993914807302232\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_iterations = 10\n",
    "total_confusion  = np.zeros((2,2))\n",
    "conf_matrix      = np.zeros((2,2))\n",
    "aucs             = np.zeros(total_iterations)\n",
    "\n",
    "for i in range(total_iterations):\n",
    "    y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,32,0.01,\n",
    "                                                            2*batch_size,100,v_batch,v_labels)\n",
    "    total_confusion = total_confusion + conf_matrix\n",
    "    aucs[i]         = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "compute_metrics(total_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad992bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ae54c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"Prio - High\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99bf8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5478260869565217\n",
      "precision 0.475\n",
      "GM 0.51720402163943\n",
      "mean auc 0.537828947368421\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_iterations = 10\n",
    "total_confusion  = np.zeros((2,2))\n",
    "conf_matrix      = np.zeros((2,2))\n",
    "aucs             = np.zeros(total_iterations)\n",
    "\n",
    "for i in range(total_iterations):\n",
    "    y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                            2*batch_size,50,v_batch,v_labels)\n",
    "    total_confusion = total_confusion + conf_matrix\n",
    "    aucs[i]         = compute_auc(y_true_1,y_probs_1)\n",
    "    \n",
    "compute_metrics(total_confusion,aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5956856",
   "metadata": {},
   "source": [
    "#### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd91e1",
   "metadata": {},
   "source": [
    "Because the first implementation has big variances in scores between sequential trainings in the same training and testing datasets we will implement a ensemble technique in order to make the results more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c886743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions_voting(total_ypreds,total_nn):\n",
    "    \n",
    "    threshold = total_nn//2\n",
    "    ypreds    = np.ndarray(shape = (np.shape(total_ypreds)[0],1),dtype = np.float64)\n",
    "    \n",
    "    for i in range(np.shape(total_ypreds)[0]):\n",
    "        ypreds[i] = 0 if total_ypreds[i]<=threshold else 1\n",
    "    \n",
    "    return ypreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46ab8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "534ca867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# voting technique based on the first and simpliest neural network.\n",
    "# Use odd number of nn so majority wins every time.\n",
    "\n",
    "target_labels = df_tags[\"Bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa30f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7608695652173914\n",
      "precision 0.7560975609756098\n",
      "GM 0.7777390621413379\n",
      "mean auc 0.810840108401084\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 9\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,16,0.01,\n",
    "                                                                    2*batch_size,100,v_batch,v_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1 \n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cede31",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2ed8e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"Google Play or Beta feedback\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f161ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8913043478260869\n",
      "precision 0.8823529411764706\n",
      "GM 0.8894239994007015\n",
      "mean auc 0.9003831417624522\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 9\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,32,0.01,\n",
    "                                                                    2*batch_size,100,v_batch,v_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "    \n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e400abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ee9c46d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"Prio - High\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "680f8ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5\n",
      "precision 0.625\n",
      "GM 0.5441071875825088\n",
      "mean auc 0.5303362573099416\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 9\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                    2*batch_size,100,v_batch,v_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "    \n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cacc6",
   "metadata": {},
   "source": [
    "#### Patience remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613870d",
   "metadata": {},
   "source": [
    "Here the neural network architecture is implemented based on patience remaining technique in order to avoid tuning the hyper parameter epochs and as a consequence avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "16b5b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e38b45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation(train_issues_0,train_issues_1,rate=0.1):\n",
    "    \n",
    "    new_train_issues_0 = list()\n",
    "    new_train_issues_1 = list()\n",
    "    validation_issues  = list()\n",
    "    validation_labels  = list()\n",
    "    \n",
    "    validation_size_0 = int(len(train_issues_0)*rate)\n",
    "    validation_size_1 = int(len(train_issues_1)*rate)\n",
    "    \n",
    "    validation_idxs_0 = random.sample([i for i in range(len(train_issues_0))], validation_size_0)\n",
    "    validation_idxs_1 = random.sample([i for i in range(len(train_issues_1))], validation_size_1)\n",
    "    \n",
    "    for i in range(len(train_issues_0)):\n",
    "        if i in validation_idxs_0:\n",
    "            validation_issues.append(train_issues_0[i])\n",
    "            validation_labels.append(0.0)\n",
    "        else:\n",
    "            new_train_issues_0.append(train_issues_0[i])\n",
    "    \n",
    "    for i in range(len(train_issues_1)):\n",
    "        if i in validation_idxs_1:\n",
    "            validation_issues.append(train_issues_1[i])\n",
    "            validation_labels.append(1.0)\n",
    "        else:\n",
    "            new_train_issues_1.append(train_issues_1[i])\n",
    "    \n",
    "    # create a pandas series for validation labels in order to be compatible with the rest code\n",
    "    val_labels_series = pd.Series(validation_labels, index = [i for i in range(len(validation_labels))])\n",
    "    \n",
    "    # create a np array for validation issues in order to be compatible with the rest code\n",
    "    val_issues = np.array(validation_issues)\n",
    "    return new_train_issues_0,new_train_issues_1,val_issues,val_labels_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3715731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn3(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,v_batch,v_labels,t_batch,t_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                      dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "     \n",
    "    output_layer   = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2 = tf.nn.softmax(output_layer)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    # patience mehtod's variables\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    min_W1             = np.zeros((np.shape(issues_embeddings_0)[1],hidden_layer_dim))\n",
    "    min_b1             = np.zeros(hidden_layrer_dim)\n",
    "    min_W2             = np.zeros((hidden_layer_dim,2))\n",
    "    min_b2             = np.zeros(2)\n",
    "    patience_remaining = 100\n",
    "    step               = batch_size/(np.shape(issues_embeddings_0)[0] + np.shape(issues_embeddings_1)[0])\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(50000):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,train_loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "            valid_loss   = sess.run(cost_func,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "            \n",
    "            patience_remaining -= step\n",
    "            if valid_loss < min_loss:\n",
    "                min_loss           = valid_loss\n",
    "                patience_remaining = 100\n",
    "                min_W1             = W1.eval()\n",
    "                min_b1             = b1.eval()\n",
    "                min_W2             = W2.eval()\n",
    "                min_b2             = b2.eval()\n",
    "            if patience_remaining<=0:\n",
    "                break\n",
    "        \n",
    "        # restore minimum weights\n",
    "        W1 = tf.convert_to_tensor(min_W1)\n",
    "        b1 = tf.convert_to_tensor(min_b1)\n",
    "        W2 = tf.convert_to_tensor(min_W2)\n",
    "        b2 = tf.convert_to_tensor(min_b2)\n",
    "                \n",
    "        # testing\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:t_batch,Y_train:t_labels})\n",
    "    \n",
    "    return compute_predictions(y_probs,v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3207212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "# use patience remaining technique\n",
    "\n",
    "target_labels = df_tags[\"Bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1)\n",
    "\n",
    "\n",
    "batch_size  = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0f621",
   "metadata": {},
   "source": [
    "#### Drop Out layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn4(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "    \n",
    "    dropout_layer  = tf.nn.dropout(hidden_layer,rate = 0.5)\n",
    "    \n",
    "    output_layer   = tf.add(tf.matmul(dropout_layer,W2),b2)\n",
    "    \n",
    "    # for validation and testing dont use dropout\n",
    "    output_layer_all = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2   = tf.nn.softmax(output_layer_all)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "        \n",
    "        # to save the weights in numpy format\n",
    "        #W1_np = W1.eval()\n",
    "        #b1_np = b1.eval()\n",
    "        #W2_np = W2.eval()\n",
    "        #b2_np = b2.eval()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "    \n",
    "    return compute_predictions(y_probs,v_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b7a44",
   "metadata": {},
   "source": [
    "#### DropOut Layer + Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c9903",
   "metadata": {},
   "source": [
    "#### DropOut Layer + Voting + Patience Remaining???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2d8e7",
   "metadata": {},
   "source": [
    "### Multiclassification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
