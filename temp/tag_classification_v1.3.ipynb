{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc15fa6e",
   "metadata": {},
   "source": [
    "# Tag Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7026519",
   "metadata": {},
   "source": [
    "This notebook contains only implementations based on neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2d9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34376414",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262a3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path_to_file):\n",
    "    temp_dict = dict()\n",
    "    with open(path_to_file) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            temp   = str(line)\n",
    "            values = temp.split(',')\n",
    "            temp_dict[values[0]] = int(values[1].replace(\"\\n\",\"\"))\n",
    "    \n",
    "    return temp_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433f7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_issues(dir_path,tag_labels,descriptions,stack_traces):\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            data = json.load(json_file)\n",
    "            for issue in data:\n",
    "                \n",
    "                tags = issue['tags']\n",
    "                for i in range(len(tags)):\n",
    "                    tags[i] = tags[i].strip()\n",
    "                description = issue['description']\n",
    "                stack_trace = issue['stack_trace']\n",
    "                name        = issue['name']\n",
    "                \n",
    "                if tags != [] and stack_trace !=[] and description != []: #(description != [] or stack_trace != []):\n",
    "                    tag_labels.append(tags)\n",
    "                    descriptions.append(description)\n",
    "                    stack_traces.append(stack_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1bdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def clean_stack_trace(stack_trace):\n",
    "    \n",
    "    clean_stack_trace = []\n",
    "    temp_1            = stack_trace.replace(r'\\tat','  at').replace('\\\"at ',' at ')\n",
    "    temp_stack        = temp_1.split(\" at \")[1:]\n",
    "    \n",
    "    if(temp_stack == []):\n",
    "        temp_stack_2 = temp_1.split(' ')\n",
    "        for t in temp_stack_2:\n",
    "            if t.count('.')>2 and t.find('(') != -1 and t.find(')') != -1:\n",
    "                if t.find('.java:') > t.find('(') and t.find('.java:') < t.find(')'):\n",
    "                    if len(t.split())>1:\n",
    "                        temp_stack.append(t.split()[1])\n",
    "                    else:\n",
    "                        temp_stack.append(t)\n",
    "    \n",
    "    to_find = re.compile(\"[|,|<|>]|\\|=\")\n",
    "        \n",
    "    #find where each function ends and keep only the path\n",
    "    for f in temp_stack:\n",
    "        temp      = f.find(')')\n",
    "        temp_file = f[0:temp]\n",
    "        \n",
    "        # check the punctuations in order to avoid anything else\n",
    "        match_obj = to_find.search(temp_file)\n",
    "        if match_obj == None:\n",
    "            filename = find_filename(temp_file)\n",
    "            if filename != '':\n",
    "                clean_stack_trace.append(filename)\n",
    "                \n",
    "    return clean_stack_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c987fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def find_filename(value):\n",
    "    filename = \"\"\n",
    "    words    = value.split(\"(\")\n",
    "    if len(words)>=2:\n",
    "        parts = words[0].split(\".\")\n",
    "        filename = \".\".join(parts[0:-1])\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e546faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from word embeddings notebook\n",
    "\n",
    "def clean_description(description):\n",
    "    \n",
    "    # define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    #define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    #join all lines into one sentence\n",
    "    sentence     = ' '.join(description)\n",
    "    \n",
    "    #translate punctuation\n",
    "    new_sentence = sentence.translate(translator)\n",
    "    \n",
    "    #split the sentense in words\n",
    "    words = new_sentence.split()\n",
    "    \n",
    "    words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "    \n",
    "    return words_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7425c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from word embeddings notebook\n",
    "\n",
    "def stemming_data(descriptions):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        for counter in range(len(desc)):\n",
    "            if desc[counter].isalpha():\n",
    "                desc[counter] = stemmer.stem(desc[counter])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace7ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(descriptions,stack_traces,use_stemming):\n",
    "    \n",
    "    clean_descriptions = list()\n",
    "    clean_stack_traces = list()\n",
    "    \n",
    "    for i in range(len(descriptions)):\n",
    "        \n",
    "        temp_desc   = descriptions[i]\n",
    "        temp_trace  = stack_traces[i]\n",
    "        stack_trace = []\n",
    "        clean_desc  = []\n",
    "        \n",
    "        if temp_trace != []:\n",
    "            if len(temp_trace)>1:\n",
    "                stack_trace = clean_stack_trace(' '.join(temp_trace))\n",
    "            else:\n",
    "                stack_trace = clean_stack_trace(temp_trace[0])\n",
    "            \n",
    "        if temp_desc  != []:\n",
    "            clean_desc = clean_description(temp_desc)\n",
    "            \n",
    "        clean_descriptions.append(clean_desc)\n",
    "        clean_stack_traces.append(stack_trace)\n",
    "            \n",
    "    if use_stemming == True:\n",
    "        stemming_data(clean_descriptions)\n",
    "        \n",
    "    return clean_descriptions,clean_stack_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e75f3",
   "metadata": {},
   "source": [
    "## Compute Arithmetic Representations for Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79aa9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(arithmetic_descriptions,arithmetic_stack_traces,\n",
    "                       word_embedding_matrix,stack_embedding_matrix,use_words,use_stacks):\n",
    "    \n",
    "    total_embeddings_dim = 0\n",
    "    descriptions_dim     = 0\n",
    "    stack_traces_dim     = 0\n",
    "    \n",
    "    if use_words == True:\n",
    "        descriptions_dim     = np.shape(word_embedding_matrix)[1]\n",
    "        total_embeddings_dim = total_embeddings_dim + descriptions_dim\n",
    "        \n",
    "    if use_stacks == True:\n",
    "        stack_traces_dim     = np.shape(stack_embedding_matrix)[1]\n",
    "        total_embeddings_dim = total_embeddings_dim + stack_traces_dim\n",
    "    \n",
    "    # make sure that in any case there are something to compute\n",
    "    if total_embeddings_dim ==0:\n",
    "        return None\n",
    "    \n",
    "    num_issues        = len(arithmetic_descriptions)\n",
    "    issues_embeddings = np.zeros((num_issues,total_embeddings_dim))\n",
    "    \n",
    "    for counter in range(len(arithmetic_descriptions)):\n",
    "        \n",
    "        temp_desc   = arithmetic_descriptions[counter]\n",
    "        temp_stack  = arithmetic_stack_traces[counter]\n",
    "        total_words = 0\n",
    "        total_funcs = 0\n",
    "        \n",
    "        if use_words == True:\n",
    "            for word in temp_desc:\n",
    "                if word != -2:\n",
    "                    total_words += 1\n",
    "                    issues_embeddings[counter][0:descriptions_dim] = issues_embeddings[counter][0:descriptions_dim] + word_embedding_matrix[word]\n",
    "            if total_words != 0 :\n",
    "                issues_embeddings[counter]    /= total_words\n",
    "        \n",
    "        \n",
    "        if use_stacks == True:\n",
    "            for func in temp_stack:\n",
    "                if func != -2:\n",
    "                    issues_embeddings[counter][descriptions_dim:] = issues_embeddings[counter][descriptions_dim:] + stack_embedding_matrix[func]\n",
    "                    total_funcs += 1\n",
    "            if total_funcs != 0:\n",
    "                issues_embeddings[counter][descriptions_dim:] = issues_embeddings[counter][descriptions_dim:] / total_funcs \n",
    "            \n",
    "    return issues_embeddings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9557ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stemming = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "181d9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "word_embedding_matrix = np.loadtxt('../results_project_3/word_embeddings_g1.txt', dtype=np.float64)\n",
    "\n",
    "# load stack traces embeddings \n",
    "stack_embedding_matrix = np.loadtxt('../results_project_3/stack_embeddings_g.txt', dtype=np.float64)\n",
    "\n",
    "# load vocabularies\n",
    "word2id_path = \"../outputs_project_3/words_vocabulary_g1.txt\"\n",
    "func2id_path = \"../outputs_project_3/stacktraces_vocabulary_g.txt\"\n",
    "\n",
    "word2id = load_dict(word2id_path)\n",
    "func2id = load_dict(func2id_path)\n",
    "\n",
    "#load tags and descriptions\n",
    "dir_path     = '../spring'\n",
    "tag_labels   = list()\n",
    "descriptions = list()\n",
    "stack_traces = list()\n",
    "\n",
    "# load issues\n",
    "load_issues(dir_path,tag_labels,descriptions,stack_traces)\n",
    "\n",
    "# transform data to arithmetic representation\n",
    "clean_descriptions,clean_stack_traces = clean_data(descriptions,stack_traces,use_stemming)\n",
    "\n",
    "clean_descriptions_2 = list()\n",
    "clean_stack_traces_2 = list()\n",
    "clean_tags_2         = list()\n",
    "\n",
    "# remove empty stack traces or dublicate issues\n",
    "for counter in range(len(clean_stack_traces)):\n",
    "    \n",
    "    if clean_stack_traces[counter] != []:\n",
    "        \n",
    "        flag   = False\n",
    "        flag_2 = False \n",
    "        \n",
    "        # remove empty stack traces \n",
    "        for i in clean_stack_traces[counter]:\n",
    "            func = func2id.get(i,-2)\n",
    "            if func != -2:\n",
    "                flag_2 = True\n",
    "                break\n",
    "        if flag_2 == False:\n",
    "            continue\n",
    "        \n",
    "        # check for dublicates\n",
    "        for counter_2 in range(len(clean_stack_traces_2)):\n",
    "            if clean_descriptions[counter] == clean_descriptions_2[counter_2] and \\\n",
    "               clean_stack_traces[counter] == clean_stack_traces_2[counter_2]:\n",
    "                    flag = True\n",
    "                    break\n",
    "        \n",
    "        if flag == False:\n",
    "            clean_stack_traces_2.append(clean_stack_traces[counter])\n",
    "            clean_descriptions_2.append(clean_descriptions[counter])\n",
    "            clean_tags_2.append(tag_labels[counter])\n",
    "                    \n",
    "del clean_descriptions\n",
    "del clean_stack_traces\n",
    "\n",
    "del descriptions\n",
    "del stack_traces\n",
    "\n",
    "#arithmetic_transformations\n",
    "arithmetic_descriptions = [[word2id.get(word,-2) for word in desc]   for desc in clean_descriptions_2]\n",
    "arithmetic_stack_traces = [[func2id.get(func,-2) for func in trace] for trace in clean_stack_traces_2]\n",
    "\n",
    "del clean_descriptions_2\n",
    "del clean_stack_traces_2\n",
    "\n",
    "issues_embeddings  = compute_embeddings(arithmetic_descriptions,arithmetic_stack_traces,\n",
    "                                        word_embedding_matrix,stack_embedding_matrix,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055d7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_labels = list()\n",
    "# copy by reference in order to avoid to change every where the variable name\n",
    "tag_labels = clean_tags_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc32954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags = ['Bug','Google Play or Beta feedback','Prio - High']\n",
    "#tags = ['>test-failure','Team:Distributed','>bug',':Distributed/Snapshot/Restore']\n",
    "tags = ['type: bug','for: stackoverflow','status: invalid','for: external-project']\n",
    "no_tags = 4\n",
    "np_tags = np.zeros((len(arithmetic_descriptions),no_tags))\n",
    "\n",
    "for counter in range(len(tag_labels)):\n",
    "    for counter_2,value in enumerate(tags):\n",
    "        if value in tag_labels[counter]:\n",
    "            np_tags[counter][counter_2] = 1\n",
    "            \n",
    "df_tags = pd.DataFrame(np_tags, columns = tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e8f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1387, 8)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(issues_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675393a8",
   "metadata": {},
   "source": [
    "## Neural Network Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a26ab18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26fb42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset2(issues_embeddings,target_labels,t_size =0.1):\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = t_size, random_state = 0)\n",
    "    \n",
    "    X_train_0 = list()\n",
    "    X_train_1 = list()\n",
    "    \n",
    "    for train_index, test_index in sss.split(issues_embeddings,target_labels):\n",
    "        #X_train,X_test = issues_embeddings[train_index], issues_embeddings[test_index]\n",
    "        #Y_train,Y_test = target_labels[train_index], target_labels[test_index]\n",
    "        \n",
    "        \n",
    "        X_test = issues_embeddings[test_index]\n",
    "        Y_test = target_labels[test_index]\n",
    "        \n",
    "        for index in train_index:\n",
    "            if target_labels.iloc[index] == 0:\n",
    "                X_train_0.append(issues_embeddings[index])\n",
    "            elif target_labels.iloc[index] == 1:\n",
    "                X_train_1.append(issues_embeddings[index])\n",
    "                \n",
    "    return X_train_0,X_train_1,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5b22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(issues_embeddings,target_labels,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,2), dtype = np.float64)\n",
    "    \n",
    "    issues_to_use = random.sample([i for i in range(np.shape(issues_embeddings)[0])],batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(issues_to_use):\n",
    "        batch[counter][:]  = issues_embeddings[value][:]\n",
    "        # label_0\n",
    "        labels[counter][0] = 1-target_labels.iloc[value]\n",
    "        # label_1\n",
    "        labels[counter][1] =   target_labels.iloc[value]\n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dce3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(issues_embeddings_0, issues_embeddings_1, batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings_0)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,2), dtype = np.float64)\n",
    "    \n",
    "    issues_to_use_0 = random.sample([i for i in range(np.shape(issues_embeddings_0)[0])],batch_size//2)\n",
    "    issues_to_use_1 = random.sample([i for i in range(np.shape(issues_embeddings_1)[0])],batch_size//2)\n",
    "    \n",
    "    # even indexes for issues belong to class 0\n",
    "    # odd  indexes for issues belong to class 1\n",
    "    counter_0 = 0\n",
    "    counter_1 = 0\n",
    "    \n",
    "    for counter in range(batch_size):\n",
    "        \n",
    "        # even indexes\n",
    "        if counter%2 == 0 :\n",
    "            batch[counter][:]  = issues_embeddings_0[issues_to_use_0[counter_0]][:]\n",
    "            labels[counter][0] = 1\n",
    "            labels[counter][1] = 0\n",
    "            counter_0 += 1\n",
    "        else:\n",
    "            batch[counter][:]  = issues_embeddings_1[issues_to_use_1[counter_1]][:]\n",
    "            labels[counter][0] = 0\n",
    "            labels[counter][1] = 1\n",
    "            counter_1 += 1\n",
    "            \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc899dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(y_probs,v_labels):\n",
    "    \n",
    "    y_probs_1 = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "    y_preds_1 = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "    y_true_1  = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64) \n",
    "    \n",
    "    for i in range(np.shape(v_labels)[0]):\n",
    "        y_true_1[i]  = v_labels[i][1]\n",
    "        y_preds_1[i] = 0 if y_probs[i][0]>y_probs[i][1] else 1\n",
    "        y_probs_1[i] = y_probs[i][1]\n",
    "    \n",
    "    matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds_1)\n",
    "    \n",
    "    return y_probs_1, y_preds_1, y_true_1, matrix_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfed1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(y_true,y_probs):\n",
    "    \n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y_true,y_probs)\n",
    "    auc                = metrics.auc(fpr,tpr)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe169dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(total_confusion,aucs):\n",
    "    \n",
    "    acc = (total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion)\n",
    "    \n",
    "    gm  = np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "              (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0])))\n",
    "    \n",
    "    pre = total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0])\n",
    "    \n",
    "    mean_auc = np.sum(aucs)/np.shape(aucs)[0]\n",
    "\n",
    "    print(\"accuracy\" , acc)\n",
    "    print(\"precision\", pre)\n",
    "    print(\"GM\"       , gm)\n",
    "    print(\"mean auc\" , mean_auc)\n",
    "    print(total_confusion)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ca19b",
   "metadata": {},
   "source": [
    "#### First and Simpliest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a6b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn2(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    \n",
    "    #initializer = initializers.GlorotNormal()\n",
    "    #W1  = tf.Variable(initializer(shape=(np.shape(issues_embeddings_0)[1],hidden_layer_dim),dtype=tf.float64),name='W1')\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    \n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim)\n",
    "                                         ,dtype=tf.float64),name = 'W2')\n",
    "    \n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "     \n",
    "    output_layer   = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2 = tf.nn.softmax(output_layer)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "        \n",
    "        # saving the weights in numpy format\n",
    "        #W1_np = W1.eval()\n",
    "        #b1_np = b1.eval()\n",
    "        #W2_np = W2.eval()\n",
    "        #b2_np = b2.eval()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "    \n",
    "    return compute_predictions(y_probs,v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8665a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b087b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first implementation the simpliest neural network\n",
    "# use both word embeddings and stack traces embeddings.\n",
    "\n",
    "target_labels = df_tags[\">test-failure\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size =0.2)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5833e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_iterations = 10\n",
    "total_confusion  = np.zeros((2,2))\n",
    "conf_matrix      = np.zeros((2,2))\n",
    "aucs             = np.zeros(total_iterations)\n",
    "\n",
    "for i in range(total_iterations):\n",
    "    y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,16,0.01,\n",
    "                                                            2*batch_size,100,v_batch,v_labels)\n",
    "    total_confusion = total_confusion + conf_matrix\n",
    "    aucs[i]         = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "compute_metrics(total_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82965567",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = df_tags[\"Google Play or Beta feedback\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = 10\n",
    "total_confusion  = np.zeros((2,2))\n",
    "conf_matrix      = np.zeros((2,2))\n",
    "aucs             = np.zeros(total_iterations)\n",
    "\n",
    "for i in range(total_iterations):\n",
    "    y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,32,0.01,\n",
    "                                                            2*batch_size,100,v_batch,v_labels)\n",
    "    total_confusion = total_confusion + conf_matrix\n",
    "    aucs[i]         = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "compute_metrics(total_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad992bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae54c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = df_tags[\"Prio - High\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = 10\n",
    "total_confusion  = np.zeros((2,2))\n",
    "conf_matrix      = np.zeros((2,2))\n",
    "aucs             = np.zeros(total_iterations)\n",
    "\n",
    "for i in range(total_iterations):\n",
    "    y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                            2*batch_size,50,v_batch,v_labels)\n",
    "    total_confusion = total_confusion + conf_matrix\n",
    "    aucs[i]         = compute_auc(y_true_1,y_probs_1)\n",
    "    \n",
    "compute_metrics(total_confusion,aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5956856",
   "metadata": {},
   "source": [
    "#### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd91e1",
   "metadata": {},
   "source": [
    "Because the first implementation has big variances in scores between sequential trainings in the same training and testing datasets we will implement a ensemble technique in order to make the results more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c886743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions_voting(total_ypreds,total_nn):\n",
    "    \n",
    "    threshold = total_nn//2\n",
    "    ypreds    = np.ndarray(shape = (np.shape(total_ypreds)[0],1),dtype = np.float64)\n",
    "    \n",
    "    for i in range(np.shape(total_ypreds)[0]):\n",
    "        ypreds[i] = 0 if total_ypreds[i]<=threshold else 1\n",
    "    \n",
    "    return ypreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46ab8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "534ca867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "# voting technique based on the first and simpliest neural network.\n",
    "# Use odd number of nn so majority wins every time.\n",
    "\n",
    "target_labels = df_tags[\"type: bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size =0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa30f9a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5539568345323741\n",
      "precision 0.56\n",
      "GM 0.5563035899673184\n",
      "mean auc 0.6049122807017545\n",
      "[[63 51]\n",
      " [11 14]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 3\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,64,0.01,\n",
    "                                                                    2*batch_size,500,v_batch,v_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1 \n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cede31",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2ed8e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"for: stackoverflow\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size =0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f161ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5107913669064749\n",
      "precision 0.6\n",
      "GM 0.5454163470199977\n",
      "mean auc 0.5004201680672269\n",
      "[[59 60]\n",
      " [ 8 12]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 3\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                    2*batch_size,800,v_batch,v_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "    \n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e400abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee9c46d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"status: invalid\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size =0.15)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "680f8ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time in seconds 5.120529890060425 \n",
      "accuracy 0.5215311004784688\n",
      "precision 0.5945945945945946\n",
      "GM 0.5350572738373774\n",
      "mean auc 0.5601434768101435\n",
      "[[65 70]\n",
      " [30 44]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 3\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                    2*batch_size,300,v_batch,v_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "#measure total time\n",
    "total_time = time.time() - start_time\n",
    "print(\"training time in seconds %s \"%(str(total_time)))\n",
    "\n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac6deb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"for: external-project\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size =0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1367626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time in seconds 4.030569553375244 \n",
      "accuracy 0.6474820143884892\n",
      "precision 0.5\n",
      "GM 0.5797710356524485\n",
      "mean auc 0.5974789915966386\n",
      "[[80 39]\n",
      " [10 10]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 3\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn2(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                    2*batch_size,150,v_batch,v_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "#measure total time\n",
    "total_time = time.time() - start_time\n",
    "print(\"training time in seconds %s \"%(str(total_time)))\n",
    "\n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cacc6",
   "metadata": {},
   "source": [
    "#### Patience remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613870d",
   "metadata": {},
   "source": [
    "Here the neural network architecture is implemented based on patience remaining technique in order to avoid tuning the hyper parameter epochs and as a consequence avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16b5b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e38b45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation(train_issues_0,train_issues_1,rate=0.2):\n",
    "    \n",
    "    new_train_issues_0 = list()\n",
    "    new_train_issues_1 = list()\n",
    "    validation_issues  = list()\n",
    "    validation_labels  = list()\n",
    "    \n",
    "    min_size = len(train_issues_0) if len(train_issues_0)<len(train_issues_1) else len(train_issues_1)\n",
    "    print(min_size)\n",
    "    validation_size_0 = int(min_size*rate)\n",
    "    validation_size_1 = int(min_size*rate)\n",
    "    \n",
    "    validation_idxs_0 = random.sample([i for i in range(len(train_issues_0))], validation_size_0)\n",
    "    validation_idxs_1 = random.sample([i for i in range(len(train_issues_1))], validation_size_1)\n",
    "    \n",
    "    for i in range(len(train_issues_0)):\n",
    "        if i in validation_idxs_0:\n",
    "            validation_issues.append(train_issues_0[i])\n",
    "            validation_labels.append(0.0)\n",
    "        else:\n",
    "            new_train_issues_0.append(train_issues_0[i])\n",
    "    \n",
    "    for i in range(len(train_issues_1)):\n",
    "        if i in validation_idxs_1:\n",
    "            validation_issues.append(train_issues_1[i])\n",
    "            validation_labels.append(1.0)\n",
    "        else:\n",
    "            new_train_issues_1.append(train_issues_1[i])\n",
    "    \n",
    "    # create a pandas series for validation labels in order to be compatible with the rest code\n",
    "    val_labels_series = pd.Series(validation_labels, index = [i for i in range(len(validation_labels))])\n",
    "    \n",
    "    # create a np array for validation issues in order to be compatible with the rest code\n",
    "    val_issues = np.array(validation_issues)\n",
    "    \n",
    "    return new_train_issues_0,new_train_issues_1,val_issues,val_labels_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3715731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn3(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,v_batch,v_labels,t_batch,t_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                      dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "     \n",
    "    output_layer   = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2 = tf.nn.softmax(output_layer)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    # patience mehtod's variables\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    min_W1             = np.zeros((np.shape(issues_embeddings_0)[1],hidden_layer_dim))\n",
    "    min_b1             = np.zeros(hidden_layer_dim)\n",
    "    min_W2             = np.zeros((hidden_layer_dim,2))\n",
    "    min_b2             = np.zeros(2)\n",
    "    patience_remaining = 200\n",
    "    step               = batch_size/(np.shape(issues_embeddings_0)[0] + np.shape(issues_embeddings_1)[0])\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(50000):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,train_loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "            # maybe valid loss should not be cross entropy but better the predictions.\n",
    "            valid_loss   = sess.run(cost_func,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "            \n",
    "            patience_remaining -= step\n",
    "            if valid_loss < min_loss:\n",
    "                min_loss           = valid_loss\n",
    "                patience_remaining = 200\n",
    "                min_W1             = W1.eval()\n",
    "                min_b1             = b1.eval()\n",
    "                min_W2             = W2.eval()\n",
    "                min_b2             = b2.eval()\n",
    "            if patience_remaining<=0:\n",
    "                print(\"total epochs\",epoch+1)\n",
    "                break\n",
    "        \n",
    "        # restore minimum weights\n",
    "        W1 = tf.convert_to_tensor(min_W1)\n",
    "        b1 = tf.convert_to_tensor(min_b1)\n",
    "        W2 = tf.convert_to_tensor(min_W2)\n",
    "        b2 = tf.convert_to_tensor(min_b2)\n",
    "                \n",
    "        # testing\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:t_batch,Y_train:t_labels})\n",
    "        \n",
    "    return compute_predictions(y_probs,t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa7e757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3207212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "# use patience remaining technique\n",
    "\n",
    "target_labels = df_tags[\"type: bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1)\n",
    "\n",
    "\n",
    "batch_size  = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c45d609e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 1031\n",
      "accuracy 0.539568345323741\n",
      "precision 0.56\n",
      "GM 0.5474021582045667\n",
      "mean auc 0.5849122807017544\n",
      "[[61 53]\n",
      " [11 14]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "deb3d659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 362\n",
      "total epochs 420\n",
      "total epochs 405\n",
      "accuracy 0.539568345323741\n",
      "precision 0.6122448979591837\n",
      "GM 0.5532833351724882\n",
      "mean auc 0.5689342403628118\n",
      "[[45 45]\n",
      " [19 30]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 3\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(t_labels)[0],1), dtype = np.float64)\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                    2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1 \n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "910a7155",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78e76071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# use patience remaining technique\n",
    "\n",
    "target_labels = df_tags[\"for: stackoverflow\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1,rate=0.2)\n",
    "\n",
    "\n",
    "batch_size  = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c438b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 3632\n",
      "accuracy 0.539568345323741\n",
      "precision 0.55\n",
      "GM 0.543873440542679\n",
      "mean auc 0.5231092436974789\n",
      "[[64 55]\n",
      " [ 9 11]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f436cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 1218\n",
      "total epochs 924\n",
      "total epochs 940\n",
      "total epochs 914\n",
      "total epochs 948\n",
      "accuracy 0.5683453237410072\n",
      "precision 0.7\n",
      "GM 0.6183469424008422\n",
      "mean auc 0.5341176470588235\n",
      "[[65 54]\n",
      " [ 6 14]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 5\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(t_labels)[0],1), dtype = np.float64)\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                    2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "    \n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "546a9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f11585b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "# use patience remaining technique\n",
    "\n",
    "target_labels = df_tags[\"status: invalid\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1,rate=0.25)\n",
    "\n",
    "\n",
    "batch_size  = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d5bfa031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 338\n",
      "accuracy 0.5827338129496403\n",
      "precision 0.6530612244897959\n",
      "GM 0.5962847939999438\n",
      "mean auc 0.5954648526077098\n",
      "[[49 41]\n",
      " [17 32]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d25b5051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 373\n",
      "total epochs 340\n",
      "total epochs 366\n",
      "training time in seconds 13.134699583053589 \n",
      "accuracy 0.539568345323741\n",
      "precision 0.5714285714285714\n",
      "GM 0.5462716342742852\n",
      "mean auc 0.5882086167800454\n",
      "[[47 43]\n",
      " [21 28]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 3\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(t_labels)[0],1), dtype = np.float64)\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                    2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "#measure total time\n",
    "total_time = time.time() - start_time\n",
    "print(\"training time in seconds %s \"%(str(total_time)))\n",
    "\n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "961dd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "# use patience remaining technique\n",
    "\n",
    "target_labels = df_tags[\"for: external-project\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1,rate=0.1)\n",
    "\n",
    "\n",
    "batch_size  = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c657d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 1735\n",
      "accuracy 0.6258992805755396\n",
      "precision 0.65\n",
      "GM 0.6357685747756561\n",
      "mean auc 0.661764705882353\n",
      "[[74 45]\n",
      " [ 7 13]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab5c4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 924\n",
      "total epochs 1307\n",
      "total epochs 1028\n",
      "total epochs 872\n",
      "total epochs 1005\n",
      "training time in seconds 27.559181213378906 \n",
      "accuracy 0.6834532374100719\n",
      "precision 0.45\n",
      "GM 0.5702719386692751\n",
      "mean auc 0.6352941176470589\n",
      "[[86 33]\n",
      " [11  9]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_nn       = 5\n",
    "aucs           = np.zeros(total_nn)\n",
    "total_ypreds_1 = np.zeros(shape = (np.shape(t_labels)[0],1), dtype = np.float64)\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_nn):\n",
    "    y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn3(train_issues_0,train_issues_1,8,0.1,\n",
    "                                                                    2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "    total_ypreds_1    = total_ypreds_1 + y_preds_1\n",
    "    aucs[i]           = compute_auc(y_true_1,y_probs_1)\n",
    "\n",
    "#measure total time\n",
    "total_time = time.time() - start_time\n",
    "print(\"training time in seconds %s \"%(str(total_time)))\n",
    "\n",
    "y_preds1         = compute_predictions_voting(total_ypreds_1,total_nn)\n",
    "matrix_confusion = metrics.confusion_matrix(y_true=y_true_1,y_pred=y_preds1)\n",
    "compute_metrics(matrix_confusion,aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0f621",
   "metadata": {},
   "source": [
    "#### Drop Out layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a55f791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn4(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "    \n",
    "    dropout_layer  = tf.nn.dropout(hidden_layer,rate = 0.5)\n",
    "    \n",
    "    output_layer   = tf.add(tf.matmul(dropout_layer,W2),b2)\n",
    "    \n",
    "    # for validation and testing dont use dropout\n",
    "    output_layer_all = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2   = tf.nn.softmax(output_layer_all)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "        \n",
    "        # validation\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "    \n",
    "    return compute_predictions(y_probs,v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f399760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "# first implementation the simpliest neural network\n",
    "# use both word embeddings and stack traces embeddings.\n",
    "\n",
    "target_labels = df_tags[\"type: bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49a9087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.60431654676259\n",
      "precision 0.44\n",
      "GM 0.5308054125241304\n",
      "mean auc 0.5957894736842105\n",
      "[[73 41]\n",
      " [14 11]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "\n",
    "# for drop out neral network => hidden_layer_dim = hidden_layer_dim/rate, epochs are more\n",
    "y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn4(train_issues_0,train_issues_1,4,0.1,\n",
    "                                                        2*batch_size,300,v_batch,v_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1))\n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8fabb382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "# first implementation the simpliest neural network\n",
    "# use both word embeddings and stack traces embeddings.\n",
    "\n",
    "target_labels = df_tags[\"for: stackoverflow\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a6580ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.460431654676259\n",
      "precision 0.45\n",
      "GM 0.45605174407879523\n",
      "mean auc 0.4676470588235294\n",
      "[[55 64]\n",
      " [11  9]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "\n",
    "# for drop out neral network => hidden_layer_dim = hidden_layer_dim/rate, epochs are more\n",
    "y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn4(train_issues_0,train_issues_1,16,0.1,\n",
    "                                                        2*batch_size,200,v_batch,v_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1))\n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f095b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439\n"
     ]
    }
   ],
   "source": [
    "# first implementation the simpliest neural network\n",
    "# use both word embeddings and stack traces embeddings.\n",
    "\n",
    "target_labels = df_tags[\"status: invalid\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ef052bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5611510791366906\n",
      "precision 0.5714285714285714\n",
      "GM 0.563436169819011\n",
      "mean auc 0.5829931972789115\n",
      "[[50 40]\n",
      " [21 28]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "\n",
    "# for drop out neral network => hidden_layer_dim = hidden_layer_dim/rate, epochs are more\n",
    "y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn4(train_issues_0,train_issues_1,4,0.1,\n",
    "                                                        2*batch_size,200,v_batch,v_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1))\n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8d81bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "# first implementation the simpliest neural network\n",
    "# use both word embeddings and stack traces embeddings.\n",
    "\n",
    "target_labels = df_tags[\"for: external-project\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a7bb9b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6618705035971223\n",
      "precision 0.45\n",
      "GM 0.5602370446681363\n",
      "mean auc 0.5873949579831933\n",
      "[[83 36]\n",
      " [11  9]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "\n",
    "# for drop out neral network => hidden_layer_dim = hidden_layer_dim/rate, epochs are more\n",
    "y_probs_1, _, y_true_1, conf_matrix = my_classifier_nn4(train_issues_0,train_issues_1,16,0.01,\n",
    "                                                        2*batch_size,200,v_batch,v_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1))\n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b7a44",
   "metadata": {},
   "source": [
    "#### DropOut Layer + Patience Remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "007ebd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn5(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,v_batch,v_labels,t_batch,t_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "    \n",
    "    dropout_layer  = tf.nn.dropout(hidden_layer,rate = 0.5)\n",
    "    \n",
    "    output_layer   = tf.add(tf.matmul(dropout_layer,W2),b2)\n",
    "    \n",
    "    # for validation and testing dont use dropout\n",
    "    output_layer_all = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2   = tf.nn.softmax(output_layer_all)\n",
    "    \n",
    "    cost_func  = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    valid_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer_all))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    # patience mehtod's variables\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    min_W1             = np.zeros((np.shape(issues_embeddings_0)[1],hidden_layer_dim))\n",
    "    min_b1             = np.zeros(hidden_layer_dim)\n",
    "    min_W2             = np.zeros((hidden_layer_dim,2))\n",
    "    min_b2             = np.zeros(2)\n",
    "    \n",
    "    patience_remaining = 100\n",
    "    step               = batch_size/(np.shape(issues_embeddings_0)[0] + np.shape(issues_embeddings_1)[0])\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(50000):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss     = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "            valid_loss = sess.run(valid_func,feed_dict={X_train:v_batch,Y_train:v_labels}) \n",
    "            \n",
    "            patience_remaining -= step\n",
    "            if valid_loss < min_loss:\n",
    "                min_loss           = valid_loss\n",
    "                patience_remaining = 100\n",
    "                min_W1             = W1.eval()\n",
    "                min_b1             = b1.eval()\n",
    "                min_W2             = W2.eval()\n",
    "                min_b2             = b2.eval()\n",
    "            if patience_remaining<=0:\n",
    "                print(\"total epochs\",epoch+1)\n",
    "                break\n",
    "        \n",
    "        # restore minimum weights\n",
    "        W1 = tf.convert_to_tensor(min_W1)\n",
    "        b1 = tf.convert_to_tensor(min_b1)\n",
    "        W2 = tf.convert_to_tensor(min_W2)\n",
    "        b2 = tf.convert_to_tensor(min_b2)\n",
    "        \n",
    "        # testing\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:t_batch,Y_train:t_labels})\n",
    "    \n",
    "    return compute_predictions(y_probs,t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e8c9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "06f723e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "# use patience remaining technique\n",
    "\n",
    "target_labels = df_tags[\"type: bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1,rate=0.2)\n",
    "\n",
    "\n",
    "batch_size  = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "363473f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 688\n",
      "accuracy 0.6115107913669064\n",
      "precision 0.56\n",
      "GM 0.5905691575290412\n",
      "mean auc 0.6256140350877193\n",
      "[[71 43]\n",
      " [11 14]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn5(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "823a4ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# use patience remaining technique\n",
    "\n",
    "target_labels = df_tags[\"for: stackoverflow\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.1)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1,rate=0.2)\n",
    "\n",
    "\n",
    "batch_size  = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f4c1b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 444\n",
      "accuracy 0.460431654676259\n",
      "precision 0.5\n",
      "GM 0.4763305116224668\n",
      "mean auc 0.47100840336134453\n",
      "[[54 65]\n",
      " [10 10]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn5(train_issues_0,train_issues_1,32,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "15b34a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"status: invalid\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.10)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1,rate=0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36e18805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 311\n",
      "accuracy 0.5611510791366906\n",
      "precision 0.46938775510204084\n",
      "GM 0.535581994247714\n",
      "mean auc 0.5877551020408164\n",
      "[[55 35]\n",
      " [26 23]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn5(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b041a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "target_labels = df_tags[\"for: external-project\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,target_labels,t_size=0.10)\n",
    "\n",
    "# create validation set\n",
    "train_issues_0,train_issues_1,valid_issues,valid_labels = create_validation(train_issues_0,train_issues_1,rate=0.1)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "\n",
    "t_batch,t_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "v_batch,v_labels = generate_batch(valid_issues,valid_labels,np.shape(valid_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "01e4105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total epochs 694\n",
      "accuracy 0.6115107913669064\n",
      "precision 0.45\n",
      "GM 0.5360923036037668\n",
      "mean auc 0.5626050420168067\n",
      "[[76 43]\n",
      " [11  9]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aucs = list()\n",
    "y_probs_1, y_preds_1, y_true_1, conf_matrix = my_classifier_nn5(train_issues_0,train_issues_1,4,0.01,\n",
    "                                                                2*batch_size,v_batch,v_labels,t_batch,t_labels)\n",
    "\n",
    "aucs.append(compute_auc(y_true_1,y_probs_1)) \n",
    "compute_metrics(conf_matrix,aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2d8e7",
   "metadata": {},
   "source": [
    "### Multiclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4bd68",
   "metadata": {},
   "source": [
    "Create a neural network for the moulticlassification problem.<br>\n",
    "The model will try to predict 3 classes Bug, Google play or Beta feedback, Prio-High.<br>\n",
    "To begin with every issue belongs only in one category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_classifier_nn(issues_embeddings,issues_labels,hidden_layer_dim,\n",
    "                        learning_rate,batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings)[1]])\n",
    "    \n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,3])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    \n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                      dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,3],\n",
    "                                         stddev = 1.0/ math.sqrt(3),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    \n",
    "    b2 = tf.Variable(tf.random_normal([3],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "     \n",
    "    output_layer   = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2 = tf.nn.softmax(output_layer)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = generate_batch_multi(issues_embeddings,issues_labels,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:valid_embeddings,Y_train:valid_labels})\n",
    "        \n",
    "        compute_metrics_multi(y_probs,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67705d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_multi(y_probs,valid_labels):\n",
    "    \n",
    "    y_preds = np.zeros(shape = (np.shape(valid_labels)[0],np.shape(valid_labels)[1]),dtype = np.float64)\n",
    "    \n",
    "    for i in range(np.shape(valid_labels)[0]):\n",
    "        max_prob    = y_probs[i][0]\n",
    "        max_pointer = 0\n",
    "        \n",
    "        for j in range(1,np.shape(valid_labels)[1]):\n",
    "            if y_probs[i][j] > max_prob:\n",
    "                max_prob    = y_probs[i][j]\n",
    "                max_pointer = j\n",
    "        \n",
    "        y_preds[i][max_pointer] = 1\n",
    "    \n",
    "    print(y_preds.argmax(axis=1)) \n",
    "    print(valid_labels.argmax(axis=1))\n",
    "    \n",
    "    for counter,value in enumerate(y_preds):\n",
    "        print(value,y_probs[counter],valid_labels[counter])\n",
    "    \n",
    "    conf_matrix = metrics.confusion_matrix(y_true=valid_labels.argmax(axis=1),y_pred=y_preds.argmax(axis=1))\n",
    "    \n",
    "    accuracy    = np.sum(np.diagonal(conf_matrix))/np.sum(conf_matrix)\n",
    "\n",
    "    # precisions\n",
    "    rows_sums  = np.sum(conf_matrix,axis=1).tolist()\n",
    "    precisions = list()\n",
    "    gm         = 1\n",
    "    \n",
    "    for i in range(np.shape(conf_matrix)[0]):\n",
    "        precisions.append(conf_matrix[i][i]/rows_sums[i])\n",
    "        gm    = gm * np.sqrt(precisions[i])\n",
    "    \n",
    "    print(\"confusion matrix \\n\", conf_matrix)\n",
    "    print(\"accuracy:\"          , accuracy)\n",
    "    print(\"gm:\"                , gm)\n",
    "    \n",
    "    for counter,value in enumerate(precisions):\n",
    "        print(\"precision \", counter, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_multi(issues_embeddings,issues_labels,batch_size):\n",
    "    \n",
    "    batch_embeddings = np.ndarray(shape = (batch_size,np.shape(issues_embeddings)[1]),dtype = np.float64)\n",
    "    batch_labels     = np.ndarray(shape = (batch_size,3),dtype = np.float64)\n",
    "    \n",
    "    issues_to_use = random.sample([i for i in range(np.shape(issues_embeddings)[0])],batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(issues_to_use):\n",
    "        batch_embeddings[counter][:]  = issues_embeddings[value][:]\n",
    "        batch_labels[counter][:]      = issues_labels[value][:]\n",
    "        \n",
    "    return batch_embeddings,batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff595629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(issues_embeddings,tags_order,df_tags,min_train_size,min_valid_size):\n",
    "    \n",
    "    # train issues\n",
    "    train_embeddings = np.ndarray(shape = (len(tags_order)*min_train_size,np.shape(issues_embeddings)[1]),\n",
    "                                  dtype = np.float64)\n",
    "    train_labels     = np.zeros(shape = (len(tags_order)*min_train_size,3),dtype = np.float64)\n",
    "    \n",
    "    # valid issues\n",
    "    valid_embeddings = np.ndarray(shape = (len(tags_order)*min_valid_size,np.shape(issues_embeddings)[1]),\n",
    "                                  dtype = np.float64)\n",
    "    valid_labels     = np.zeros(shape = (len(tags_order)*min_valid_size,3),dtype = np.float64)\n",
    "    \n",
    "    idxs             = list()\n",
    "    \n",
    "    for tag in tags_order:\n",
    "        temp = df_tags[tag]\n",
    "        idxs.append([i for i in range(len(temp)) if temp.loc[i] == 1])\n",
    "        \n",
    "    # make sure every index(=>issue) belongs only in one category.\n",
    "    for counter,value in enumerate(idxs):\n",
    "        if counter == len(idxs)-1:\n",
    "            break\n",
    "        for idx in value:\n",
    "            for counter_2 in range(counter+1,len(idxs)):\n",
    "                if idx in idxs[counter_2]:\n",
    "                    idxs[counter_2].remove(idx)\n",
    "    \n",
    "    # choose random indexes.\n",
    "    for counter,value in enumerate(idxs):\n",
    "        \n",
    "        random_idx_train = random.sample(value,min_train_size)\n",
    "        random_idx_valid = random.sample([i for i in value if i not in random_idx_train],min_valid_size)\n",
    "        \n",
    "        # training split\n",
    "        temp_2 = 0\n",
    "        for temp in range(counter*min_train_size,(counter+1)*min_train_size):\n",
    "            \n",
    "            train_embeddings[temp]          = issues_embeddings[random_idx_train[temp_2]]\n",
    "            train_labels[temp][counter]     = 1.0\n",
    "            temp_2                         += 1\n",
    "        \n",
    "        # validation split\n",
    "        temp_2 = 0\n",
    "        for temp in range(counter*min_valid_size,(counter+1)*min_valid_size):\n",
    "            valid_embeddings[temp]      = issues_embeddings[random_idx_valid[temp_2]]\n",
    "            valid_labels[temp][counter] = 1.0\n",
    "            temp_2                     += 1\n",
    "            \n",
    "    return train_embeddings,train_labels,valid_embeddings,valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags must be in asceding order.\n",
    "tags_order     = ['Prio - High','Google Play or Beta feedback','Bug']\n",
    "min_train_size = 50\n",
    "min_valid_size = 10\n",
    "\n",
    "train_embeddings,train_labels,valid_embeddings,valid_labels = create_dataset(issues_embeddings,tags_order,df_tags,\n",
    "                                                                            min_train_size,min_valid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_classifier_nn(train_embeddings,train_labels,32,0.01,50,500,valid_embeddings,valid_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
