{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "backed-wisconsin",
   "metadata": {},
   "source": [
    "# TAG CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-shanghai",
   "metadata": {},
   "source": [
    "In the second version of tag classifications we will try to predict the tag labels for issues based on descriptions and stack traces. More precise, we will use our word embeddings and stack traces embeddings which have already been created to compute the arithmetic representation of the issue and then based on that we will try to predict the type of issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-bargain",
   "metadata": {},
   "source": [
    "Still, we use logistic regression. Logistic regression used for binary classification but using the method one vs rest we can train one logistic regression model for each label.  Maybe one better version will be using the multinomial logistic regression\n",
    "\n",
    "Moreover, for the arithmetic representation of issues first we will use the average of the word embeddings concatenated with the average of the stack traces embeddings. For those issues missing stack traces we will just zero padding in order to have fixed size. \n",
    "\n",
    "Maybe in later stage we will try to improve the formula using a weighted average based on TF-IDF method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-article",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-sport",
   "metadata": {},
   "source": [
    "First, load the word embeddings and stack traces embedding matrices, the word's and trace's vocabulary and for every issue the corresponding tags and description and stack trace if exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "married-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-opportunity",
   "metadata": {},
   "source": [
    "### Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "happy-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path_to_file):\n",
    "    temp_dict = dict()\n",
    "    with open(path_to_file) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            temp   = str(line)\n",
    "            values = temp.split(',')\n",
    "            temp_dict[values[0]] = int(values[1].replace(\"\\n\",\"\"))\n",
    "    \n",
    "    return temp_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "original-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_issues(dir_path,tag_labels,descriptions,stack_traces):\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            data = json.load(json_file)\n",
    "            for issue in data:\n",
    "                \n",
    "                tags = issue['tags']\n",
    "                for i in range(len(tags)):\n",
    "                    tags[i] = tags[i].strip()\n",
    "                description = issue['description']\n",
    "                stack_trace = issue['stack_trace']\n",
    "                name        = issue['name']\n",
    "                \n",
    "                if tags != [] and stack_trace !=[] : #(description != [] or stack_trace != []):\n",
    "                    tag_labels.append(tags)\n",
    "                    descriptions.append(description)\n",
    "                    stack_traces.append(stack_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "subjective-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def clean_stack_trace(stack_trace):\n",
    "    \n",
    "    clean_stack_trace = []\n",
    "    temp_stack        = stack_trace.split(\" at \")[1:]\n",
    "    \n",
    "    to_find = re.compile(\"[|,|<|>]|/|\\|=\")\n",
    "    \n",
    "    #find where each function ends and keep only the path\n",
    "    for f in temp_stack:\n",
    "        temp      = f.find(')')\n",
    "        temp_file = f[0:temp]\n",
    "        \n",
    "        # check the punctuations in order to avoid anything else\n",
    "        match_obj = to_find.search(temp_file)\n",
    "        if match_obj == None:\n",
    "            filename = find_filename(temp_file)\n",
    "            if filename != '':\n",
    "                clean_stack_trace.append(filename)\n",
    "                \n",
    "    return clean_stack_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "promising-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def find_filename(value):\n",
    "    filename = \"\"\n",
    "    words    = value.split(\"(\")\n",
    "    if len(words)>=2:\n",
    "        parts = words[0].split(\".\")\n",
    "        filename = \".\".join(parts[0:-1])\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "affected-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from word embeddings notebook\n",
    "\n",
    "def clean_description(description):\n",
    "    \n",
    "    # define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    #define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    #join all lines into one sentence\n",
    "    sentence     = ' '.join(description)\n",
    "    \n",
    "    #translate punctuation\n",
    "    new_sentence = sentence.translate(translator)\n",
    "    \n",
    "    #split the sentense in words\n",
    "    words = new_sentence.split()\n",
    "    \n",
    "    words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "    \n",
    "    return words_sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "virgin-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from word embeddings notebook\n",
    "\n",
    "def stemming_data(descriptions):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        for counter in range(len(desc)):\n",
    "            if desc[counter].isalpha():\n",
    "                desc[counter] = stemmer.stem(desc[counter])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "amateur-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(descriptions,stack_traces,use_stemming):\n",
    "    \n",
    "    clean_descriptions = list()\n",
    "    clean_stack_traces = list()\n",
    "    \n",
    "    for i in range(len(descriptions)):\n",
    "        \n",
    "        temp_desc   = descriptions[i]\n",
    "        temp_trace  = stack_traces[i]\n",
    "        stack_trace = []\n",
    "        \n",
    "        \n",
    "        if temp_trace != []:\n",
    "            if len(temp_trace)>1:\n",
    "                stack_trace = clean_stack_trace(''.join(temp_trace))\n",
    "            else:\n",
    "                stack_trace = clean_stack_trace(temp_trace[0])\n",
    "            \n",
    "        if temp_desc  != []:\n",
    "            clean_desc = clean_description(temp_desc)\n",
    "            \n",
    "        clean_descriptions.append(clean_desc)\n",
    "        clean_stack_traces.append(stack_trace)\n",
    "            \n",
    "    if use_stemming == True:\n",
    "        stemming_data(clean_descriptions)\n",
    "        \n",
    "    return clean_descriptions,clean_stack_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-wilderness",
   "metadata": {},
   "source": [
    "### Compute Arithmetic Representations for Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "possible-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(arithmetic_descriptions,arithmetic_stack_traces,\n",
    "                       word_embedding_matrix,stack_embedding_matrix,use_stacks):\n",
    "    \n",
    "    descriptions_dim  = np.shape(word_embedding_matrix)[1]\n",
    "    if use_stacks == True:\n",
    "        stack_traces_dim  = np.shape(stack_embedding_matrix)[1]\n",
    "    else:\n",
    "        stack_traces_dim  = 0\n",
    "    \n",
    "    num_issues        = len(arithmetic_descriptions)\n",
    "    issues_embeddings = np.zeros((num_issues,descriptions_dim+stack_traces_dim))\n",
    "    \n",
    "    for counter in range(len(arithmetic_descriptions)):\n",
    "        \n",
    "        temp_desc   = arithmetic_descriptions[counter]\n",
    "        temp_stack  = arithmetic_stack_traces[counter]\n",
    "        total_words = 0\n",
    "        total_funcs = 0\n",
    "        \n",
    "        for word in temp_desc:\n",
    "            if word != -2:\n",
    "                total_words += 1\n",
    "                issues_embeddings[counter][0:descriptions_dim] = issues_embeddings[counter][0:descriptions_dim] + word_embedding_matrix[word]\n",
    "        \n",
    "        if total_words != 0 :\n",
    "            issues_embeddings[counter]    /= total_words\n",
    "        \n",
    "        if use_stacks == True:\n",
    "            for func in temp_stack:\n",
    "                if func != -2:\n",
    "                    issues_embeddings[counter][descriptions_dim:] = issues_embeddings[counter][descriptions_dim:] + stack_embedding_matrix[func]\n",
    "                    total_funcs += 1\n",
    "                \n",
    "            if total_funcs != 0:\n",
    "                issues_embeddings[counter][descriptions_dim:] = issues_embeddings[counter][descriptions_dim:] / total_funcs \n",
    "            \n",
    "    return issues_embeddings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "improving-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stemming = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "pending-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "word_embedding_matrix = np.loadtxt('word_embeddings_good.txt', dtype=np.float64)\n",
    "\n",
    "# load stack traces embeddings \n",
    "stack_embedding_matrix = np.loadtxt('stack_embeddings_new.txt', dtype=np.float64)\n",
    "\n",
    "# load vocabularies\n",
    "word2id_path = \"vocabulary_good.txt\"\n",
    "func2id_path = \"stack_traces_vocabulary.txt\"\n",
    "\n",
    "word2id = load_dict(word2id_path)\n",
    "func2id = load_dict(func2id_path)\n",
    "\n",
    "#load tags and descriptions\n",
    "dir_path     = '/home/kostas/Documents/thesis/data_1'\n",
    "tag_labels   = list()\n",
    "descriptions = list()\n",
    "stack_traces = list()\n",
    "\n",
    "# load issues\n",
    "load_issues(dir_path,tag_labels,descriptions,stack_traces)\n",
    "\n",
    "# transform data to arithmetic representation\n",
    "clean_descriptions,clean_stack_traces = clean_data(descriptions,stack_traces,use_stemming)\n",
    "\n",
    "del descriptions\n",
    "del stack_traces\n",
    "\n",
    "#arithmetic_transformations\n",
    "arithmetic_descriptions = [[word2id.get(word,-2) for word in desc]   for desc in clean_descriptions]\n",
    "arithmetic_stack_traces = [[func2id.get(func,-2) for func in trace] for trace in clean_stack_traces]\n",
    "\n",
    "del clean_descriptions\n",
    "del clean_stack_traces\n",
    "\n",
    "issues_embeddings  = compute_embeddings(arithmetic_descriptions,arithmetic_stack_traces,\n",
    "                                        word_embedding_matrix,stack_embedding_matrix,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-burlington",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "moved-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['Bug','Google Play or Beta feedback','Feedback required','Feature Request','Prio - High','Frontend Design']\n",
    "no_tags = 6\n",
    "np_tags = np.zeros((len(arithmetic_descriptions),no_tags))\n",
    "\n",
    "for counter in range(len(tag_labels)):\n",
    "    for counter_2,value in enumerate(tags):\n",
    "        if value in tag_labels[counter]:\n",
    "            np_tags[counter][counter_2] = 1\n",
    "            \n",
    "df_tags = pd.DataFrame(np_tags, columns = tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-fight",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "attractive-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "harmful-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dummy_classifier(tags,df_tags,issues_embeddings,cl_label,n_splits):\n",
    "    \n",
    "    target_label    = df_tags[cl_label]\n",
    "    dummy_clf       = DummyClassifier(strategy = \"uniform\",random_state=0)\n",
    "    total_confusion = np.zeros((2,2))\n",
    "    \n",
    "    #fit model \n",
    "    dummy_clf.fit(issues_embeddings,target_label)\n",
    "    predictions = dummy_clf.predict(issues_embeddings)\n",
    "    total_confusion = confusion_matrix(target_label,predictions)\n",
    "\n",
    "    print(total_confusion)\n",
    "    print(\"accuracy = TP+TN/(TP+TN+FP+FN)\",(total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion))\n",
    "    print(\"custom metric\",np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "                                  (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-volunteer",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "organic-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier(tags,df_tags,issues_embeddings,cl_label,n_splits):\n",
    "    \n",
    "    target_label    = df_tags[cl_label]\n",
    "    counter_1       = np.sum(target_label)\n",
    "    #print(counter_1)\n",
    "    #weight_0        = counter_1/target_label.shape[0]\n",
    "    #weight_1        = (1 - weight_0)\n",
    "    weight_0        = 1/(target_label.shape[0]-counter_1)\n",
    "    weight_1        = 1/counter_1\n",
    "    w               = {0:weight_0,1:weight_1}\n",
    "    skf             = StratifiedKFold(n_splits)\n",
    "    model           = LogisticRegression(solver='lbfgs',class_weight = w)\n",
    "    total_confusion = np.zeros((2,2))\n",
    "    for train_index, test_index in skf.split(issues_embeddings,target_label):\n",
    "        \n",
    "        X_train,X_test = issues_embeddings[train_index], issues_embeddings[test_index]\n",
    "        y_train,y_test = target_label[train_index], target_label[test_index]\n",
    "        \n",
    "        #fit model \n",
    "        model.fit(X_train,y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        #print(confusion_matrix(y_test,predictions))\n",
    "        total_confusion = total_confusion+confusion_matrix(y_test,predictions)\n",
    "        \n",
    "    print(total_confusion)\n",
    "    print(\"accuracy = TP+TN/(TP+TN+FP+FN)\",(total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion))\n",
    "    print(\"custom metric\",np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "                                  (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0]))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "inside-melissa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[211. 176.]\n",
      " [ 47.  38.]]\n",
      "accuracy = TP+TN/(TP+TN+FP+FN) 0.527542372881356\n",
      "custom metric 0.49370563095634196\n"
     ]
    }
   ],
   "source": [
    "my_classifier(tags,df_tags,issues_embeddings,\"Prio - High\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "annoying-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 28  30]\n",
      " [198 216]]\n",
      "accuracy = TP+TN/(TP+TN+FP+FN) 0.5169491525423728\n",
      "custom metric 0.5018705639589911\n"
     ]
    }
   ],
   "source": [
    "my_dummy_classifier(tags,df_tags,issues_embeddings,\"Bug\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-slave",
   "metadata": {},
   "source": [
    "## Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "failing-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "regulated-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(issues_embeddings,target_labels):\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.3, random_state = 0)\n",
    "    \n",
    "    for train_index, test_index in sss.split(issues_embeddings,target_labels):\n",
    "        X_train,X_test = issues_embeddings[train_index], issues_embeddings[test_index]\n",
    "        Y_train,Y_test = target_labels[train_index], target_labels[test_index]\n",
    "    \n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ordinary-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(issues_embeddings,target_labels,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,2), dtype = np.float64)\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    issues_to_use = random.sample([i for i in range(np.shape(issues_embeddings)[0])],batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(issues_to_use):\n",
    "        batch[counter][:]  = issues_embeddings[value][:]\n",
    "        # label_0\n",
    "        labels[counter][0] = 1-target_labels.iloc[value]\n",
    "        # label_1\n",
    "        labels[counter][1] =   target_labels.iloc[value]\n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "sealed-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn(issues_embeddings,target_labels,hidden_layer_dim,learning_rate,\n",
    "                     batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings)[1],hidden_layer_dim],stddev = 1.0,dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0,dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],stddev = 1.0,dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    ##neural network's functions\n",
    "    hidden_layer = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer = tf.nn.tanh(hidden_layer)\n",
    "    \n",
    "    output_layer = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2 = tf.nn.softmax(output_layer)\n",
    "    \n",
    "    cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = generate_batch(issues_embeddings,target_labels,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "        \n",
    "        # validation\n",
    "        y_preds = sess.run(output_layer_2,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "        for i in range(len(y_preds)):\n",
    "            print(y_preds[i],v_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "former-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08677736 0.91322264] [1. 0.]\n",
      "[0.11932975 0.88067025] [0. 1.]\n",
      "[0.02914446 0.97085554] [0. 1.]\n",
      "[0.31196683 0.68803317] [0. 1.]\n",
      "[0.57121026 0.42878974] [0. 1.]\n",
      "[0.03413461 0.96586539] [0. 1.]\n",
      "[0.04087443 0.95912557] [0. 1.]\n",
      "[0.08751638 0.91248362] [0. 1.]\n",
      "[0.00226097 0.99773903] [0. 1.]\n",
      "[0.0332156 0.9667844] [1. 0.]\n",
      "[0.23383637 0.76616363] [0. 1.]\n",
      "[0.13802276 0.86197724] [0. 1.]\n",
      "[0.06656157 0.93343843] [0. 1.]\n",
      "[0.0866391 0.9133609] [0. 1.]\n",
      "[0.01597245 0.98402755] [0. 1.]\n",
      "[0.08404197 0.91595803] [0. 1.]\n",
      "[0.07984018 0.92015982] [0. 1.]\n",
      "[0.08702431 0.91297569] [0. 1.]\n",
      "[0.08423335 0.91576665] [0. 1.]\n",
      "[0.19600063 0.80399937] [0. 1.]\n",
      "[0.02734662 0.97265338] [0. 1.]\n",
      "[0.97258412 0.02741588] [0. 1.]\n",
      "[0.00149977 0.99850023] [1. 0.]\n",
      "[0.00581731 0.99418269] [0. 1.]\n",
      "[0.0314267 0.9685733] [0. 1.]\n",
      "[0.08684509 0.91315491] [0. 1.]\n",
      "[0.08742098 0.91257902] [0. 1.]\n",
      "[0.08870444 0.91129556] [0. 1.]\n",
      "[0.12781829 0.87218171] [0. 1.]\n",
      "[0.00166635 0.99833365] [0. 1.]\n",
      "[0.08816083 0.91183917] [0. 1.]\n",
      "[0.30077481 0.69922519] [0. 1.]\n",
      "[0.99089469 0.00910531] [0. 1.]\n",
      "[0.08582183 0.91417817] [0. 1.]\n",
      "[0.96526046 0.03473954] [0. 1.]\n",
      "[0.00221102 0.99778898] [0. 1.]\n",
      "[0.11649211 0.88350789] [1. 0.]\n",
      "[0.0832583 0.9167417] [0. 1.]\n",
      "[0.09837118 0.90162882] [0. 1.]\n",
      "[0.62749964 0.37250036] [0. 1.]\n",
      "[0.08545666 0.91454334] [0. 1.]\n",
      "[0.09372334 0.90627666] [0. 1.]\n",
      "[0.09246779 0.90753221] [0. 1.]\n",
      "[0.12127576 0.87872424] [0. 1.]\n",
      "[0.09783511 0.90216489] [0. 1.]\n",
      "[0.04487191 0.95512809] [0. 1.]\n",
      "[0.03520049 0.96479951] [0. 1.]\n",
      "[0.00253135 0.99746865] [0. 1.]\n",
      "[0.69627549 0.30372451] [0. 1.]\n",
      "[0.12666477 0.87333523] [0. 1.]\n",
      "[0.03769655 0.96230345] [0. 1.]\n",
      "[0.08692626 0.91307374] [0. 1.]\n",
      "[0.08477246 0.91522754] [0. 1.]\n",
      "[0.09655597 0.90344403] [0. 1.]\n",
      "[0.03941979 0.96058021] [0. 1.]\n",
      "[0.08601015 0.91398985] [0. 1.]\n",
      "[0.08277112 0.91722888] [1. 0.]\n",
      "[0.06944201 0.93055799] [0. 1.]\n",
      "[0.08512254 0.91487746] [0. 1.]\n",
      "[0.12276648 0.87723352] [0. 1.]\n",
      "[0.09536015 0.90463985] [0. 1.]\n",
      "[0.12201526 0.87798474] [0. 1.]\n",
      "[0.01839287 0.98160713] [0. 1.]\n",
      "[0.11103332 0.88896668] [0. 1.]\n",
      "[0.02379639 0.97620361] [0. 1.]\n",
      "[0.11383249 0.88616751] [0. 1.]\n",
      "[0.07587984 0.92412016] [0. 1.]\n",
      "[0.08666854 0.91333146] [0. 1.]\n",
      "[0.93324417 0.06675583] [0. 1.]\n",
      "[0.13164749 0.86835251] [0. 1.]\n",
      "[0.62528927 0.37471073] [1. 0.]\n",
      "[0.11137212 0.88862788] [0. 1.]\n",
      "[0.06850453 0.93149547] [0. 1.]\n",
      "[0.99306428 0.00693572] [0. 1.]\n",
      "[0.08031945 0.91968055] [0. 1.]\n",
      "[0.09829703 0.90170297] [0. 1.]\n",
      "[0.08521229 0.91478771] [0. 1.]\n",
      "[0.91827516 0.08172484] [0. 1.]\n",
      "[0.00711775 0.99288225] [0. 1.]\n",
      "[0.08538747 0.91461253] [0. 1.]\n",
      "[0.08556349 0.91443651] [1. 0.]\n",
      "[0.07805492 0.92194508] [0. 1.]\n",
      "[0.32604673 0.67395327] [0. 1.]\n",
      "[0.00164737 0.99835263] [0. 1.]\n",
      "[0.09132973 0.90867027] [0. 1.]\n",
      "[0.08550038 0.91449962] [0. 1.]\n",
      "[0.09847961 0.90152039] [0. 1.]\n",
      "[0.07749126 0.92250874] [0. 1.]\n",
      "[0.08871696 0.91128304] [1. 0.]\n",
      "[0.29831993 0.70168007] [0. 1.]\n",
      "[0.04017894 0.95982106] [0. 1.]\n",
      "[0.07044031 0.92955969] [0. 1.]\n",
      "[0.21383097 0.78616903] [1. 0.]\n",
      "[0.19674595 0.80325405] [0. 1.]\n",
      "[0.10183896 0.89816104] [0. 1.]\n",
      "[0.17001612 0.82998388] [1. 0.]\n",
      "[0.04430913 0.95569087] [0. 1.]\n",
      "[0.09069298 0.90930702] [0. 1.]\n",
      "[0.23373211 0.76626789] [0. 1.]\n",
      "[8.60015215e-04 9.99139985e-01] [0. 1.]\n",
      "[0.08660068 0.91339932] [0. 1.]\n",
      "[0.05259414 0.94740586] [0. 1.]\n",
      "[0.08357491 0.91642509] [0. 1.]\n",
      "[0.08597181 0.91402819] [0. 1.]\n",
      "[0.08508105 0.91491895] [0. 1.]\n",
      "[0.08527502 0.91472498] [0. 1.]\n",
      "[0.05472554 0.94527446] [0. 1.]\n",
      "[0.0845149 0.9154851] [0. 1.]\n",
      "[0.08751799 0.91248201] [0. 1.]\n",
      "[0.25299041 0.74700959] [1. 0.]\n",
      "[0.09594442 0.90405558] [0. 1.]\n",
      "[9.83957099e-04 9.99016043e-01] [0. 1.]\n",
      "[0.08687935 0.91312065] [0. 1.]\n",
      "[0.11709336 0.88290664] [0. 1.]\n",
      "[0.09607731 0.90392269] [0. 1.]\n",
      "[0.6856165 0.3143835] [0. 1.]\n",
      "[0.08591708 0.91408292] [0. 1.]\n",
      "[0.01612103 0.98387897] [0. 1.]\n",
      "[0.01614618 0.98385382] [0. 1.]\n",
      "[0.13699466 0.86300534] [0. 1.]\n",
      "[0.0872459 0.9127541] [0. 1.]\n",
      "[0.08188015 0.91811985] [0. 1.]\n",
      "[0.00803403 0.99196597] [1. 0.]\n",
      "[0.08971284 0.91028716] [0. 1.]\n",
      "[0.08230171 0.91769829] [1. 0.]\n",
      "[0.09247999 0.90752001] [0. 1.]\n",
      "[0.08751051 0.91248949] [0. 1.]\n",
      "[0.11179473 0.88820527] [0. 1.]\n",
      "[0.09834378 0.90165622] [0. 1.]\n",
      "[0.00503884 0.99496116] [0. 1.]\n",
      "[0.01369725 0.98630275] [0. 1.]\n",
      "[0.11311602 0.88688398] [1. 0.]\n",
      "[0.0949237 0.9050763] [0. 1.]\n",
      "[3.99571299e-04 9.99600429e-01] [0. 1.]\n",
      "[0.09693347 0.90306653] [1. 0.]\n",
      "[0.95872288 0.04127712] [1. 0.]\n",
      "[0.10096986 0.89903014] [0. 1.]\n",
      "[0.17244723 0.82755277] [0. 1.]\n",
      "[0.09872749 0.90127251] [0. 1.]\n",
      "[0.22860804 0.77139196] [1. 0.]\n",
      "[0.10939844 0.89060156] [0. 1.]\n",
      "[0.07502444 0.92497556] [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_issues,train_labels,test_issues,test_labels = split_dataset(issues_embeddings,target_labels)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])\n",
    "my_classifier_nn(train_issues,train_labels,8,0.1,64,500,v_batch,v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-amazon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "whole-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_v2(issues_embeddings,target_labels,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.float64)\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    issues_to_use = random.sample([i for i in range(np.shape(issues_embeddings)[0])],batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(issues_to_use):\n",
    "        batch[counter][:]  = issues_embeddings[value][:]\n",
    "        labels[counter,0] = target_labels.iloc[value]\n",
    "        \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "searching-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn_v2(issues_embeddings,target_labels,hidden_layer_dim,learning_rate,\n",
    "                        batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,1])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings)[1],hidden_layer_dim],stddev = 1.0,dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0,dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,1],stddev = 1.0,dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([1],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer = tf.nn.tanh(hidden_layer)\n",
    "    \n",
    "    output_layer  = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer2 = tf.nn.sigmoid(output_layer) \n",
    "    \n",
    "    cost_func = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # generate batch.\n",
    "            batch_x,batch_y = generate_batch_v2(issues_embeddings,target_labels,batch_size)\n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "            \n",
    "        \n",
    "        y_preds = sess.run(output_layer,feed_dict = {X_train:v_batch,Y_train:v_labels})\n",
    "        for i in range(len(y_preds)):\n",
    "            print(y_preds[i],v_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = df_tags[\"Bug\"]\n",
    "train_issues,train_labels,test_issues,test_labels = split_dataset(issues_embeddings,target_labels)\n",
    "v_batch  = np.reshape(test_issues,(-1,np.shape(issues_embeddings)[1]))\n",
    "v_labels = test_labels.to_numpy()\n",
    "v_labels = np.reshape(v_labels,(-1,1))\n",
    "preds = my_classifier_nn_v2(train_issues,train_labels,8,0.1,64,800,v_batch,v_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
