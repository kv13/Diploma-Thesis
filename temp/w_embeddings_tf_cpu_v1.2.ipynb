{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "criminal-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labriares\n",
    "import os \n",
    "import json \n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-vietnam",
   "metadata": {},
   "source": [
    "### Load & Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "periodic-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(descriptions,dir_path):\n",
    "    \n",
    "    counter = 0\n",
    "    counter_issues = 0\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            ##############################\n",
    "            counter += 1\n",
    "            #print(counter,\") reading file\",fname)\n",
    "            ##############################\n",
    "            \n",
    "            #load data in json format\n",
    "            data = json.load(json_file)\n",
    "            for p in data:\n",
    "                \n",
    "                ##############################\n",
    "                issue_name     = p['name']\n",
    "                counter_issues += 1\n",
    "                #print(\"  \",counter_issues,\")\",issue_name)\n",
    "                ##############################\n",
    "                \n",
    "                issue_desc     = p['description']\n",
    "                \n",
    "                # add all non empty issues and non dublicate.\n",
    "                if issue_desc != [] and issue_desc not in descriptions:\n",
    "                    descriptions.append(issue_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baking-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(clean_descriptions,raw_descriptions):\n",
    "    \n",
    "    # define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    for desc in raw_descriptions:\n",
    "        \n",
    "        #join all lines into one sentence\n",
    "        sentence = ' '.join(desc)\n",
    "        \n",
    "        #translate punctuation\n",
    "        new_sentence = sentence.translate(translator)\n",
    "        \n",
    "        #split the sentense in words\n",
    "        words = new_sentence.split()\n",
    "        words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "        \n",
    "        if words_sw != []:\n",
    "            clean_descriptions.append(words_sw)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "square-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_data(descriptions):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        for counter in range(len(desc)):\n",
    "            if desc[counter].isalpha():\n",
    "                desc[counter] = stemmer.stem(desc[counter])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attempted-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(descriptions,valid_size,test_size,min_size):\n",
    "    \n",
    "    valid_set = []\n",
    "    test_set  = []\n",
    "    \n",
    "    # random select descriptions.\n",
    "    seed(datetime.now())\n",
    "    \n",
    "    for i in range(valid_size):\n",
    "        flag = False\n",
    "        while flag == False:\n",
    "            temp = randint(0,len(descriptions)-1)\n",
    "            if len(descriptions[temp]) >= min_size:\n",
    "                valid_set.append(descriptions.pop(temp))\n",
    "                flag = True\n",
    "    \n",
    "    for i in range(test_size):\n",
    "        flag = False\n",
    "        while flag == False:\n",
    "            temp = randint(0,len(descriptions)-1)\n",
    "            if len(descriptions[temp]) >= min_size:\n",
    "                test_set.append(descriptions.pop(temp))\n",
    "                flag = True\n",
    "    \n",
    "    return valid_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "linear-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first time the below command should run to download stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# define necessary parameters\n",
    "dir_path         = '/home/kostas/Documents/thesis/data_1'\n",
    "raw_descriptions = []\n",
    "min_size         = 10\n",
    "\n",
    "# load all issues descriptions\n",
    "load_data(raw_descriptions,dir_path)\n",
    "\n",
    "# split and clean descriptions\n",
    "clean_descriptions = []\n",
    "clean_data(clean_descriptions,raw_descriptions)\n",
    "\n",
    "# list raw_descriptions now is useless\n",
    "del raw_descriptions\n",
    "\n",
    "# stemming, it's not necessary step.\n",
    "stemming_data(clean_descriptions)\n",
    "\n",
    "# split data set to train,validation and test set\n",
    "# validation and test set would have 20% of total data.\n",
    "total_desc = len(clean_descriptions)\n",
    "valid_size = int(0.2 * total_desc)\n",
    "test_size  = int(0.2 * total_desc)\n",
    "\n",
    "valid_set,test_set = split_dataset(clean_descriptions,valid_size,test_size,min_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cordless-princess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique descriptions 5973\n",
      "size of train set 3585\n",
      "size of validation set 1194\n",
      "size of test set 1194\n"
     ]
    }
   ],
   "source": [
    "# print messages #\n",
    "print(\"total unique descriptions\",total_desc)\n",
    "print(\"size of train set\",len(clean_descriptions))\n",
    "print(\"size of validation set\",valid_size)\n",
    "print(\"size of test set\",test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-heating",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "theoretical-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some important variables\n",
    "valid_words   = 100\n",
    "test_words    = 100\n",
    "true_neigh    = 8\n",
    "false_neigh   = 30\n",
    "\n",
    "min_occurance = 5\n",
    "unk_word      = \"UNK\"\n",
    "skip_window   = 4\n",
    "batch_size    = 2048\n",
    "embedding_dim = 64\n",
    "num_sampled   = 32\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "marked-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(word_dict):\n",
    "    \n",
    "    with open(\"vocabulary_test.txt\",\"w\") as file:\n",
    "        for key in word_dict:\n",
    "            file.write(\"%s, %s \\n\"%(key,str(word_dict[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "seventh-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(train_set,skip_window):\n",
    "    \n",
    "    #find total words in descriptions\n",
    "    total_words = 0\n",
    "    for desc in train_set:\n",
    "        total_words += len(desc)\n",
    "    \n",
    "    #initialize the corpus which will keep all word pairs\n",
    "    max_size = total_words*2*skip_window\n",
    "    corpus = -1*np.ones((max_size,2), dtype=np.int32)\n",
    "    \n",
    "    # initialize pointers for the iterations\n",
    "    desc_pointer  = 0\n",
    "    word_pointer  = 0\n",
    "    counter       = 0\n",
    "    \n",
    "    #initialize temporary buffer\n",
    "    span   = 2*skip_window+1 \n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    while counter < max_size:\n",
    "        \n",
    "        # avoid tags with -2\n",
    "        while train_set[desc_pointer][word_pointer] < 0:\n",
    "            word_pointer += 1\n",
    "            if word_pointer > len(train_set[desc_pointer])-1:\n",
    "                word_pointer  = 0\n",
    "                desc_pointer +=1\n",
    "                if desc_pointer > len(train_set) -1:\n",
    "                    break\n",
    "                    \n",
    "        #check if all descriptions have been analyzed\n",
    "        if desc_pointer > len(train_set)-1:\n",
    "            break\n",
    "        \n",
    "        find_context_words(train_set[desc_pointer],word_pointer,skip_window,span,buffer)\n",
    "        \n",
    "        for i in range(1,len(buffer)):\n",
    "            corpus[counter][0] = buffer[0]\n",
    "            corpus[counter][1] = buffer[i]\n",
    "            counter += 1\n",
    "        \n",
    "        buffer.clear()\n",
    "        \n",
    "        if word_pointer == len(train_set[desc_pointer]) -1:\n",
    "            word_pointer  = 0\n",
    "            desc_pointer +=1\n",
    "            if desc_pointer > len(train_set) -1:\n",
    "                break\n",
    "        else:\n",
    "            word_pointer += 1\n",
    "    \n",
    "    return corpus[0:counter].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alternate-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_words(description,word_index,skip_window,span,grams_list):\n",
    "    \n",
    "    # the target word in the first place\n",
    "    grams_list.append(description[word_index])\n",
    "    \n",
    "    # initialize two pointers\n",
    "    counter = 1\n",
    "    data_index = word_index-1\n",
    "    \n",
    "    while counter < span:\n",
    "        # look left from target word\n",
    "        if counter<=skip_window:\n",
    "            # if data_index<0 => out of bound no more words to take into account\n",
    "            if data_index < 0:\n",
    "                data_index = word_index + 1\n",
    "                counter = skip_window + 1\n",
    "            # if the word is not in the dict skip it\n",
    "            elif description[data_index] == -2:\n",
    "                data_index -= 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index -= 1\n",
    "                if counter > skip_window:\n",
    "                    data_index = word_index + 1\n",
    "        # look right from target word\n",
    "        else:\n",
    "            if data_index >= len(description):\n",
    "                counter = span + 1\n",
    "            elif description[data_index] == -2:\n",
    "                data_index += 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "matched-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary based on the frequency of each word.\n",
    "# remove rare words, which occurs less time than min_occurance from voc\n",
    "# word2id:  dictionary which contains the vocabulary and it's int id\n",
    "\n",
    "temp_sentences = [word for desc in clean_descriptions for word in desc]\n",
    "\n",
    "count = []\n",
    "count.extend(collections.Counter(temp_sentences).most_common())\n",
    "\n",
    "# list temp_sentences now is useless\n",
    "del temp_sentences\n",
    "\n",
    "count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "\n",
    "# compute the vocabulary size\n",
    "vocabulary_size = len(count)\n",
    "\n",
    "# assign an id to each word\n",
    "# this dictionary will have voc_size+1 length.\n",
    "word2id = dict()\n",
    "word2id[unk_word] = -2\n",
    "\n",
    "for i,(word,_) in enumerate(count):\n",
    "    word2id[word] = i\n",
    "\n",
    "# list count now is useless\n",
    "del count\n",
    "\n",
    "#express train, valid and test set using id\n",
    "train_set_id = [[word2id.get(word,-2) for word in desc] for desc in clean_descriptions]\n",
    "del clean_descriptions\n",
    "valid_set_id = [[word2id.get(word,-2) for word in desc] for desc in valid_set]\n",
    "del valid_set\n",
    "test_set_id  = [[word2id.get(word,-2) for word in desc] for desc in test_set]\n",
    "del test_set\n",
    "\n",
    "# save vocabulary\n",
    "save_vocabulary(word2id)\n",
    "\n",
    "# create corpus with word pairs\n",
    "corpus         = create_corpus(train_set_id,skip_window)\n",
    "corpus_indexes = [w for w in range(len(corpus))] \n",
    "\n",
    "# save them \n",
    "np.savetxt('corpus_words_test.txt',corpus,fmt=\"%d\")\n",
    "\n",
    "# train_set_id now is useless\n",
    "del train_set_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-teach",
   "metadata": {},
   "source": [
    "### Validation and Test Pairs Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "passing-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_pairs(test_dict):\n",
    "    \n",
    "    with open('testing_pairs_test.pkl','wb') as file:\n",
    "        pickle.dump(test_dict,file,pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "prompt-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(arr,low,high):\n",
    "    \n",
    "    i = (low - 1)\n",
    "    pivot = arr[high][1]\n",
    "    \n",
    "    for j in range(low,high):\n",
    "        \n",
    "        if arr[j][1] >= pivot:\n",
    "            \n",
    "            i += 1\n",
    "            arr[i], arr[j] = arr[j], arr[i]\n",
    "    \n",
    "    arr[i+1],arr[high] = arr[high], arr[i+1]\n",
    "    return (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adopted-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickSort(arr,low,high):\n",
    "    if len(arr) ==1:\n",
    "        return arr\n",
    "    if low<high:\n",
    "        pi = partition(arr,low,high)\n",
    "        \n",
    "        quickSort(arr,low,pi-1)\n",
    "        quickSort(arr,pi+1,high)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "personal-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_pairs(test_set,min_occurance,num_words,skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # initialize temporary buffer\n",
    "    span   = skip_window*2+1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    # numerate all words in the dataset.\n",
    "    temp_sentences = [word for desc in test_set for word in desc]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(temp_sentences).most_common())\n",
    "    \n",
    "    # list temp_sentences now is useless\n",
    "    del temp_sentences\n",
    "    \n",
    "    # remove rare words\n",
    "    count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "    \n",
    "    # compute weights and select num_testing words\n",
    "    weights      = [e[1] for e in count if e[0] != -2 ]\n",
    "    total_weight = np.sum(weights)\n",
    "    \n",
    "    \n",
    "    # generate random samples    \n",
    "    weights[:] = [x/total_weight for x in weights]\n",
    "    indexes    = [i for i in range(len(count)) if count[i][0]!=-2]\n",
    "    samples    = np.random.choice(indexes,num_words,replace = False, p = weights)\n",
    "    \n",
    "    target_w   = [count[i][0] for i in samples]\n",
    "    w_dict     = dict([(key, [[],[]]) for key in target_w])\n",
    "    \n",
    "    \n",
    "    for desc in test_set:\n",
    "        for w in target_w:\n",
    "            temp_idx = [i for i,e in enumerate(desc) if e == w]\n",
    "            for idx in temp_idx:\n",
    "                find_context_words(desc,idx,skip_window,span,buffer)\n",
    "                for i in range(1,len(buffer)):\n",
    "                    if w_dict[w][0] == []:\n",
    "                        w_dict[w][0].append([buffer[i],1])\n",
    "                    else:\n",
    "                        flag = False\n",
    "                        for neigh in w_dict[w][0]:\n",
    "                            if neigh[0] == buffer[i]:\n",
    "                                neigh[1] += 1\n",
    "                                flag = True\n",
    "                        if flag == False:\n",
    "                            w_dict[w][0].append([buffer[i],1])\n",
    "                buffer.clear()\n",
    "                \n",
    "    # sort lists based on the frequency of neighbor's appearences. Keep only the 8 most frequent.\n",
    "    # create false_neigh false neighbors for each target word.\n",
    "    for key in w_dict:\n",
    "        quickSort(w_dict[key][0],0,len(w_dict[key][0])-1)\n",
    "        \n",
    "        flag = True\n",
    "        neig_counter = 0\n",
    "        while flag == True:\n",
    "            random_idx = np.random.choice(indexes,2*false_neigh,replace = False)\n",
    "            for idx in random_idx:\n",
    "                \n",
    "                if count[idx][0] == key:\n",
    "                    continue\n",
    "                elif count[idx][0] in w_dict[key][1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    is_flag = True\n",
    "                    #search neighbors\n",
    "                    for neigh in w_dict[key][0]:\n",
    "                        if count[idx][0] == neigh[0]:\n",
    "                            is_flag = False\n",
    "                            break\n",
    "                    if is_flag == True:\n",
    "                        w_dict[key][1].append(count[idx][0])\n",
    "                        neig_counter += 1\n",
    "                        if neig_counter >= false_neigh:\n",
    "                            flag = False\n",
    "                            break\n",
    "        # if len(w_dict[key][0])>8:\n",
    "        #    w_dict[key][0]= [w_dict[key][0][i] for i in range(8)]\n",
    "    #for key in w_dict:\n",
    "    #    print(key,w_dict[key][0])\n",
    "    print(0,w_dict[0][0])\n",
    "    return w_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "coral-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[5, 171], [0, 140], [1, 121], [14, 114], [32, 104], [9, 103], [4, 81], [31, 80], [20, 79], [52, 66], [24, 65], [15, 65], [117, 55], [2, 45], [61, 45], [16, 43], [3, 41], [65, 38], [67, 38], [45, 33], [28, 31], [71, 31], [35, 31], [69, 31], [84, 30], [30, 29], [40, 28], [7, 27], [86, 26], [68, 26], [34, 26], [124, 26], [10, 26], [23, 25], [55, 25], [6, 25], [97, 24], [143, 24], [92, 24], [192, 24], [172, 23], [49, 23], [458, 22], [72, 22], [70, 22], [121, 22], [12, 22], [88, 22], [62, 21], [37, 21], [51, 21], [329, 21], [58, 20], [152, 20], [64, 20], [139, 20], [107, 19], [13, 19], [74, 19], [150, 17], [18, 17], [48, 17], [216, 17], [116, 17], [75, 16], [491, 16], [38, 16], [33, 16], [29, 16], [81, 16], [619, 16], [288, 16], [125, 15], [83, 15], [50, 15], [134, 15], [56, 15], [63, 15], [80, 14], [25, 14], [98, 14], [268, 13], [19, 13], [101, 13], [195, 13], [193, 13], [156, 13], [251, 13], [99, 13], [512, 13], [76, 13], [265, 13], [11, 12], [343, 12], [167, 12], [283, 12], [170, 12], [22, 12], [133, 12], [113, 12], [106, 12], [17, 12], [145, 12], [243, 12], [363, 11], [149, 11], [138, 11], [182, 11], [53, 11], [813, 11], [262, 11], [153, 11], [157, 11], [741, 11], [21, 11], [93, 11], [509, 11], [26, 11], [112, 11], [442, 11], [175, 11], [78, 10], [43, 10], [433, 10], [137, 10], [118, 10], [82, 10], [1999, 10], [77, 10], [250, 10], [284, 10], [57, 10], [227, 9], [308, 9], [269, 9], [214, 9], [126, 9], [630, 9], [237, 9], [90, 9], [166, 9], [417, 9], [290, 8], [179, 8], [41, 8], [132, 8], [154, 8], [46, 8], [412, 8], [692, 8], [260, 8], [206, 8], [164, 8], [130, 8], [330, 8], [8, 8], [105, 8], [315, 8], [202, 8], [36, 8], [155, 7], [114, 7], [355, 7], [165, 7], [762, 7], [342, 7], [131, 7], [110, 7], [141, 7], [457, 7], [136, 7], [144, 7], [389, 7], [42, 7], [422, 7], [756, 7], [89, 7], [393, 7], [108, 6], [369, 6], [159, 6], [226, 6], [354, 6], [162, 6], [801, 6], [122, 6], [148, 6], [73, 6], [385, 6], [147, 6], [278, 6], [543, 6], [331, 6], [488, 6], [531, 6], [201, 6], [327, 6], [958, 6], [332, 6], [317, 6], [811, 6], [229, 6], [100, 6], [583, 6], [351, 6], [115, 6], [559, 6], [878, 6], [219, 6], [478, 6], [746, 6], [1089, 6], [273, 5], [335, 5], [258, 5], [129, 5], [325, 5], [469, 5], [400, 5], [300, 5], [303, 5], [434, 5], [91, 5], [365, 5], [194, 5], [427, 5], [225, 5], [424, 5], [459, 5], [571, 5], [174, 5], [289, 5], [645, 5], [339, 5], [673, 5], [849, 5], [361, 5], [142, 5], [1610, 5], [349, 5], [858, 5], [189, 5], [913, 5], [375, 5], [211, 5], [348, 5], [749, 5], [212, 5], [181, 5], [810, 5], [127, 5], [27, 5], [94, 5], [270, 5], [782, 5], [718, 5], [625, 4], [184, 4], [724, 4], [517, 4], [79, 4], [405, 4], [104, 4], [285, 4], [391, 4], [254, 4], [467, 4], [448, 4], [794, 4], [337, 4], [298, 4], [866, 4], [820, 4], [239, 4], [656, 4], [530, 4], [238, 4], [312, 4], [231, 4], [1263, 4], [453, 4], [987, 4], [197, 4], [455, 4], [190, 4], [439, 4], [319, 4], [487, 4], [624, 4], [111, 4], [158, 4], [816, 4], [918, 4], [247, 4], [191, 4], [173, 4], [789, 4], [1311, 4], [87, 4], [217, 4], [140, 4], [596, 4], [259, 4], [390, 4], [176, 4], [311, 4], [700, 4], [356, 4], [171, 4], [204, 4], [220, 4], [572, 4], [475, 4], [1254, 3], [398, 3], [1455, 3], [286, 3], [44, 3], [151, 3], [550, 3], [373, 3], [701, 3], [2052, 3], [863, 3], [539, 3], [591, 3], [1419, 3], [403, 3], [252, 3], [537, 3], [992, 3], [425, 3], [552, 3], [345, 3], [682, 3], [2055, 3], [617, 3], [96, 3], [499, 3], [561, 3], [976, 3], [228, 3], [222, 3], [817, 3], [421, 3], [299, 3], [210, 3], [660, 3], [1055, 3], [310, 3], [705, 3], [296, 3], [965, 3], [1057, 3], [800, 3], [207, 3], [649, 3], [503, 3], [1475, 3], [102, 3], [1528, 3], [413, 3], [213, 3], [1192, 3], [1517, 3], [187, 3], [456, 3], [66, 3], [1091, 3], [527, 3], [276, 3], [622, 3], [481, 3], [885, 3], [324, 3], [196, 3], [295, 3], [614, 3], [410, 3], [240, 3], [322, 3], [103, 3], [326, 3], [267, 3], [418, 3], [1703, 3], [549, 3], [879, 3], [651, 3], [553, 3], [452, 3], [508, 3], [350, 3], [727, 3], [54, 3], [471, 3], [534, 3], [2070, 3], [160, 3], [1686, 3], [271, 3], [587, 3], [306, 3], [119, 2], [292, 2], [109, 2], [1126, 2], [790, 2], [275, 2], [635, 2], [586, 2], [1354, 2], [555, 2], [1347, 2], [967, 2], [1147, 2], [450, 2], [646, 2], [304, 2], [562, 2], [470, 2], [1470, 2], [224, 2], [600, 2], [333, 2], [788, 2], [266, 2], [601, 2], [95, 2], [281, 2], [565, 2], [1175, 2], [525, 2], [665, 2], [1024, 2], [738, 2], [775, 2], [177, 2], [307, 2], [894, 2], [1739, 2], [546, 2], [578, 2], [575, 2], [564, 2], [438, 2], [309, 2], [431, 2], [608, 2], [297, 2], [521, 2], [490, 2], [548, 2], [1720, 2], [1843, 2], [544, 2], [556, 2], [747, 2], [768, 2], [739, 2], [969, 2], [178, 2], [982, 2], [671, 2], [394, 2], [639, 2], [1976, 2], [188, 2], [895, 2], [352, 2], [59, 2], [652, 2], [1486, 2], [592, 2], [676, 2], [576, 2], [711, 2], [396, 2], [463, 2], [279, 2], [944, 2], [826, 2], [371, 2], [859, 2], [1133, 2], [1137, 2], [1294, 2], [205, 2], [245, 2], [932, 2], [683, 2], [1098, 2], [636, 2], [856, 2], [85, 2], [1849, 2], [494, 2], [1121, 2], [340, 2], [937, 2], [875, 2], [908, 2], [246, 2], [449, 2], [380, 2], [631, 2], [728, 2], [223, 2], [446, 2], [368, 2], [581, 2], [606, 2], [513, 2], [1086, 2], [47, 2], [338, 2], [710, 2], [1140, 2], [603, 2], [554, 2], [1226, 2], [1910, 2], [437, 2], [274, 2], [483, 2], [844, 2], [722, 2], [1484, 2], [634, 2], [1062, 2], [480, 2], [486, 2], [1242, 2], [198, 2], [1104, 2], [1095, 2], [605, 2], [1309, 2], [293, 2], [209, 2], [612, 2], [1710, 2], [698, 2], [1549, 2], [659, 2], [1523, 2], [1222, 2], [522, 2], [384, 2], [721, 2], [235, 2], [1474, 2], [993, 2], [663, 2], [323, 2], [974, 2], [496, 2], [501, 2], [1820, 2], [723, 2], [532, 2], [1313, 2], [802, 2], [1080, 2], [1329, 2], [454, 2], [1572, 2], [567, 2], [524, 2], [1360, 2], [1576, 2], [1282, 2], [2133, 2], [408, 2], [1264, 2], [1118, 1], [1466, 1], [1357, 1], [828, 1], [1744, 1], [430, 1], [1412, 1], [902, 1], [1679, 1], [518, 1], [280, 1], [1050, 1], [1274, 1], [1432, 1], [257, 1], [750, 1], [737, 1], [161, 1], [1596, 1], [1083, 1], [1176, 1], [842, 1], [827, 1], [1374, 1], [234, 1], [1305, 1], [573, 1], [1248, 1], [1076, 1], [953, 1], [658, 1], [341, 1], [714, 1], [485, 1], [1306, 1], [1143, 1], [1573, 1], [874, 1], [1392, 1], [1637, 1], [751, 1], [846, 1], [1125, 1], [807, 1], [1365, 1], [451, 1], [1039, 1], [1332, 1], [435, 1], [604, 1], [694, 1], [1807, 1], [526, 1], [1142, 1], [497, 1], [1370, 1], [706, 1], [1837, 1], [1350, 1], [948, 1], [637, 1], [301, 1], [922, 1], [1558, 1], [392, 1], [610, 1], [514, 1], [1593, 1], [411, 1], [302, 1], [1865, 1], [804, 1], [1209, 1], [1653, 1], [316, 1], [1366, 1], [597, 1], [1477, 1], [729, 1], [819, 1], [1664, 1], [917, 1], [709, 1], [1088, 1], [1446, 1], [256, 1], [474, 1], [492, 1], [662, 1], [473, 1], [358, 1], [2119, 1], [462, 1], [1965, 1], [616, 1], [841, 1], [1152, 1], [580, 1], [771, 1], [641, 1], [146, 1], [346, 1], [1634, 1], [2066, 1], [1004, 1], [1870, 1], [1847, 1], [925, 1], [642, 1], [831, 1], [1396, 1], [796, 1], [305, 1], [1180, 1], [1561, 1], [760, 1], [1600, 1], [1271, 1], [2003, 1], [344, 1], [666, 1], [939, 1], [850, 1], [1886, 1], [783, 1], [757, 1], [1951, 1], [1109, 1], [872, 1], [669, 1], [1457, 1], [1970, 1], [708, 1], [1496, 1], [952, 1], [927, 1], [203, 1], [376, 1], [1330, 1], [1738, 1], [1539, 1], [1540, 1], [1210, 1], [401, 1], [943, 1], [704, 1], [183, 1], [541, 1], [798, 1], [429, 1], [684, 1], [415, 1], [1205, 1], [1031, 1], [726, 1], [423, 1], [1648, 1], [644, 1], [1534, 1], [926, 1], [1737, 1], [901, 1], [998, 1], [1218, 1], [1553, 1], [1624, 1], [688, 1], [1053, 1], [1857, 1], [1265, 1], [468, 1], [1672, 1], [1105, 1], [347, 1], [1972, 1], [877, 1], [924, 1], [799, 1], [39, 1], [774, 1], [505, 1], [476, 1], [2010, 1], [781, 1], [120, 1], [1202, 1], [1324, 1], [946, 1], [1444, 1], [2121, 1], [1096, 1], [244, 1], [717, 1], [803, 1], [837, 1], [242, 1], [402, 1], [1227, 1], [1168, 1], [791, 1], [1034, 1], [1510, 1], [495, 1], [1198, 1], [135, 1], [1135, 1], [313, 1], [735, 1], [1488, 1], [482, 1], [767, 1], [236, 1], [2072, 1], [829, 1], [2020, 1], [1958, 1], [664, 1], [2001, 1], [215, 1], [1777, 1], [511, 1], [1103, 1], [416, 1], [535, 1], [249, 1], [272, 1], [1038, 1], [2103, 1], [443, 1], [1006, 1], [1166, 1], [186, 1], [1082, 1], [1587, 1], [1001, 1], [233, 1], [785, 1], [2054, 1], [919, 1], [868, 1], [681, 1], [830, 1], [613, 1], [168, 1], [627, 1], [1304, 1], [742, 1], [2089, 1], [1308, 1], [808, 1], [699, 1], [1239, 1], [1171, 1], [957, 1], [1064, 1], [1819, 1], [963, 1], [1111, 1], [661, 1], [461, 1], [321, 1], [1754, 1], [441, 1], [1597, 1], [374, 1], [1232, 1], [489, 1], [618, 1], [758, 1], [991, 1], [536, 1], [994, 1], [696, 1], [778, 1], [1622, 1], [589, 1], [1066, 1], [1656, 1], [1507, 1], [1530, 1], [980, 1], [1138, 1], [702, 1], [551, 1], [716, 1], [1804, 1], [1519, 1], [1969, 1], [1296, 1], [1391, 1], [579, 1], [1170, 1], [1978, 1], [169, 1], [643, 1], [960, 1], [1666, 1], [921, 1], [2043, 1], [1611, 1], [163, 1], [533, 1], [1445, 1], [1604, 1], [542, 1], [1805, 1], [506, 1], [655, 1], [996, 1], [1012, 1], [2079, 1], [821, 1], [690, 1], [677, 1], [263, 1], [200, 1], [947, 1], [929, 1], [882, 1], [1262, 1], [1564, 1], [582, 1], [419, 1], [334, 1], [1409, 1], [232, 1], [674, 1], [538, 1], [1113, 1], [2073, 1], [1411, 1], [436, 1], [730, 1], [1208, 1], [607, 1], [1278, 1], [880, 1], [1099, 1], [1358, 1], [1797, 1]]\n"
     ]
    }
   ],
   "source": [
    "#test_dict  = create_testing_pairs(test_set_id,min_occurance,test_words,2,true_neigh,false_neigh)\n",
    "#save_test_pairs(test_dict)\n",
    "#del test_dict\n",
    "\n",
    "\n",
    "valid_dict = create_testing_pairs(valid_set_id,min_occurance,valid_words,2,true_neigh,false_neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-conspiracy",
   "metadata": {},
   "source": [
    "## Word's Embeddings Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "altered-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "running-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(corpus_data,corpus_indexes,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size),   dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.int32)\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    words_to_use = random.sample(corpus_indexes,batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(words_to_use):\n",
    "        batch[counter]    = corpus_data[value][0]\n",
    "        labels[counter,0] = corpus_data[value][1] \n",
    "    \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "affecting-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def_cpu(corpus_data,corpus_indexes,batch_size,embedding_dim,\n",
    "                  num_sampled,learning_rate,vocabulary_size):\n",
    "    \n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.int32, shape=[None])\n",
    "    # Input label\n",
    "    Y_train = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    \n",
    "    # ensure that the following ops & var are assigned to CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        # create the embedding variable wich contains the weights\n",
    "        embedding = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "        \n",
    "        # create the lookup table for each sample in X_train=>avoiding to use one_hot encoder\n",
    "        X_embed   = tf.nn.embedding_lookup(embedding,X_train)\n",
    "        \n",
    "        # create variables for the loss function\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_dim],stddev=1.0))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "    loss_func = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,biases =nce_biases,labels = Y_train,\n",
    "                                              inputs = X_embed,num_sampled = num_sampled,\n",
    "                                              num_classes = vocabulary_size ))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    train_opt = optimizer.minimize(loss_func)\n",
    "    \n",
    "    #Define initializer for tensorflow variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #actual initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # patience method's variables \n",
    "        min_loss           = float('inf')\n",
    "        min_emb_matrix     = np.zeros((vocabulary_size,embedding_dim))\n",
    "        patience_remaining = 100\n",
    "        \n",
    "        start_time = time.time()\n",
    "        #train the model using 100 epoch patience\n",
    "        for epoch in range(50000):\n",
    "            \n",
    "            # take a batch of data.\n",
    "            batch_x,batch_y = generate_batch(corpus_data,corpus_indexes,batch_size)\n",
    "            \n",
    "            _,loss = sess.run([train_opt,loss_func],feed_dict={X_train:batch_x, Y_train:batch_y})\n",
    "            \n",
    "            patience_remaining -= 1\n",
    "            \n",
    "            if loss < min_loss:\n",
    "                min_loss           = loss\n",
    "                patience_remaining = 200\n",
    "                min_emb_matrix     = embedding.eval()\n",
    "                \n",
    "            if patience_remaining == 0:\n",
    "                break\n",
    "        \n",
    "        #normalize embeddings before using them\n",
    "        #restore min embeddings\n",
    "        embedding = tf.convert_to_tensor(min_emb_matrix)\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keepdims = True))\n",
    "        normalized_embedding = embedding/norm\n",
    "        normalized_embedding_matrix = sess.run(normalized_embedding)\n",
    "        \n",
    "        #measure total time\n",
    "        total_time = time.time() - start_time\n",
    "        print(\"training time in seconds %s \"%(str(total_time)))\n",
    "        print(\"total epochs was\",epoch)\n",
    "    return normalized_embedding_matrix\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "collaborative-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_neigh(prediction_buffer,buffer_len,cosine_sim,word):\n",
    "    \n",
    "    if len(prediction_buffer)< buffer_len:\n",
    "        prediction_buffer.append((word,cosine_sim))\n",
    "        quickSort(prediction_buffer,0,len(prediction_buffer)-1)\n",
    "    else:\n",
    "        if cosine_sim > prediction_buffer[buffer_len-1][1]:\n",
    "            prediction_buffer.pop()\n",
    "            prediction_buffer.append((word,cosine_sim))\n",
    "            quickSort(prediction_buffer,0,buffer_len-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "hearing-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first version just computes fpr and tpr. Model's classes\n",
    "# are class_A = neighbor and class_B = no neighbor. The model based on cosine similarity\n",
    "# will try to predict the right label for each word pair given.\n",
    "def model_validation_v1(embedding_matrix,words_dict,true_neigh):\n",
    "    \n",
    "    prediction_buffer = collections.deque(maxlen = true_neigh)\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    for key in words_dict:\n",
    "        target_emb = embedding_matrix[key]\n",
    "        for neigh in words_dict[key][0]:\n",
    "            neigh_emb = embedding_matrix[neigh[0]]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            check_neigh(prediction_buffer,true_neigh,result,neigh[0])\n",
    "        for neigh in words_dict[key][1]:\n",
    "            neigh_emb = embedding_matrix[neigh]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            check_neigh(prediction_buffer,true_neigh,result,neigh)\n",
    "        \n",
    "        fp_counter = 0\n",
    "        for neigh in prediction_buffer:\n",
    "            if neigh[0] in words_dict[key][1]:\n",
    "                fp_counter += 1\n",
    "        fp = fp + fp_counter\n",
    "        tn = tn + (30-fp_counter)\n",
    "        tp = tp + (8-fp_counter)\n",
    "        fn = fn + fp_counter\n",
    "        \n",
    "    tpr = tp/(tp+fn)\n",
    "    fpr = fp/(tn+fp)\n",
    "    return tpr,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "smart-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time in seconds 3.5796594619750977 \n",
      "total epochs was 677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9725, 0.007333333333333333)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_embedding_matrix = model_def_cpu(corpus,corpus_indexes,batch_size,embedding_dim,num_sampled,\n",
    "                                      learning_rate,vocabulary_size)\n",
    "\n",
    "np.savetxt('word_embeddings_test.txt',norm_embedding_matrix,fmt='%.8f')\n",
    "model_validation_v1(norm_embedding_matrix,valid_dict,true_neigh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
