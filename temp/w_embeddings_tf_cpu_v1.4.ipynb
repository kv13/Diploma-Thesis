{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541e794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labriares\n",
    "import os \n",
    "import json \n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962903a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db73c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(descriptions,dir_path):\n",
    "    \n",
    "    counter        = 0\n",
    "    counter_issues = 0\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            ##############################\n",
    "            counter += 1\n",
    "            #print(counter,\") reading file\",fname)\n",
    "            ##############################\n",
    "            \n",
    "            #load data in json format\n",
    "            data = json.load(json_file)\n",
    "            for p in data:\n",
    "                \n",
    "                ##############################\n",
    "                issue_name     = p['name']\n",
    "                counter_issues += 1\n",
    "                #print(\"  \",counter_issues,\")\",issue_name)\n",
    "                ##############################\n",
    "                \n",
    "                issue_desc     = p['description']\n",
    "                \n",
    "                # add all non empty issues and non dublicate.\n",
    "                if issue_desc != [] and issue_desc not in descriptions:\n",
    "                    descriptions.append(issue_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90def780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(clean_descriptions,raw_descriptions):\n",
    "    \n",
    "    # define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    for desc in raw_descriptions:\n",
    "        \n",
    "        #join all lines into one sentence\n",
    "        sentence = ' '.join(desc)\n",
    "        \n",
    "        #translate punctuation\n",
    "        new_sentence = sentence.translate(translator)\n",
    "        \n",
    "        #split the sentense in words\n",
    "        words = new_sentence.split()\n",
    "        words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "        \n",
    "        if words_sw != []:\n",
    "            clean_descriptions.append(words_sw)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cf0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_data(descriptions):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        for counter in range(len(desc)):\n",
    "            if desc[counter].isalpha():\n",
    "                desc[counter] = stemmer.stem(desc[counter])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135a0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(descriptions,valid_size,test_size,min_size):\n",
    "    \n",
    "    valid_set = []\n",
    "    test_set  = []\n",
    "    \n",
    "    # random select descriptions.\n",
    "    seed(datetime.now())\n",
    "    \n",
    "    for i in range(valid_size):\n",
    "        flag = False\n",
    "        while flag == False:\n",
    "            temp = randint(0,len(descriptions)-1)\n",
    "            if len(descriptions[temp]) >= min_size:\n",
    "                valid_set.append(descriptions.pop(temp))\n",
    "                flag = True\n",
    "    \n",
    "    for i in range(test_size):\n",
    "        flag = False\n",
    "        while flag == False:\n",
    "            temp = randint(0,len(descriptions)-1)\n",
    "            if len(descriptions[temp]) >= min_size:\n",
    "                test_set.append(descriptions.pop(temp))\n",
    "                flag = True\n",
    "    \n",
    "    return valid_set,test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bea0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first time the below command should run to download stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# define necessary parameters\n",
    "dir_path         = '../data'\n",
    "raw_descriptions = []\n",
    "min_size         = 10\n",
    "\n",
    "# load all issues descriptions\n",
    "load_data(raw_descriptions,dir_path)\n",
    "\n",
    "# split and clean descriptions\n",
    "clean_descriptions = []\n",
    "clean_data(clean_descriptions,raw_descriptions)\n",
    "\n",
    "# list raw_descriptions now is useless\n",
    "del raw_descriptions\n",
    "\n",
    "# stemming, it's not necessary step.\n",
    "stemming_data(clean_descriptions)\n",
    "\n",
    "# split data set to train,validation and test set\n",
    "# validation and test set would have 20% of total data.\n",
    "total_desc = len(clean_descriptions)\n",
    "valid_size = int(0.3  * total_desc)\n",
    "test_size  = int(0.1  * total_desc)\n",
    "\n",
    "valid_set,test_set = split_dataset(clean_descriptions,valid_size,test_size,min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99cec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total unique descriptions 6244\n",
      "size of train set 3747\n",
      "size of validation set 1873\n",
      "size of test set 624\n"
     ]
    }
   ],
   "source": [
    "# print messages #\n",
    "print(\"total unique descriptions\",total_desc)\n",
    "print(\"size of train set\",len(clean_descriptions))\n",
    "print(\"size of validation set\",valid_size)\n",
    "print(\"size of test set\",test_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947c954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(word_dict):\n",
    "    directory = \"../outputs\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    with open(os.path.join(directory,\"words_vocabulary.txt\"),\"w\") as file:\n",
    "        for key in word_dict:\n",
    "            file.write(\"%s, %s \\n\"%(key,str(word_dict[key])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50645610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_pairs(test_dict):\n",
    "    with open('../outputs/testing_pairs_test.pkl','wb') as file:\n",
    "        pickle.dump(test_dict,file,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c725a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(train_set,skip_window):\n",
    "    \n",
    "    # find total words in descriptions\n",
    "    total_words = 0\n",
    "    for desc in train_set:\n",
    "        total_words += len(desc)\n",
    "    \n",
    "    # initialize the corpus which will keep all word pairs\n",
    "    max_size = total_words*2*skip_window\n",
    "    corpus = -1*np.ones((max_size,2), dtype=np.int32)\n",
    "    \n",
    "    # initialize pointers for the iterations\n",
    "    desc_pointer  = 0\n",
    "    word_pointer  = 0\n",
    "    counter       = 0\n",
    "    \n",
    "    # initialize temporary buffer\n",
    "    span   = 2*skip_window+1 \n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    while counter < max_size:\n",
    "        \n",
    "        # avoid tags with -2\n",
    "        while train_set[desc_pointer][word_pointer] < 0:\n",
    "            word_pointer += 1\n",
    "            if word_pointer > len(train_set[desc_pointer])-1:\n",
    "                word_pointer  = 0\n",
    "                desc_pointer +=1\n",
    "                if desc_pointer > len(train_set) -1:\n",
    "                    break\n",
    "                    \n",
    "        #check if all descriptions have been analyzed\n",
    "        if desc_pointer > len(train_set)-1:\n",
    "            break\n",
    "        \n",
    "        find_context_words(train_set[desc_pointer],word_pointer,skip_window,span,buffer)\n",
    "        \n",
    "        for i in range(1,len(buffer)):\n",
    "            corpus[counter][0] = buffer[0]\n",
    "            corpus[counter][1] = buffer[i]\n",
    "            counter += 1\n",
    "        \n",
    "        buffer.clear()\n",
    "        \n",
    "        if word_pointer == len(train_set[desc_pointer]) -1:\n",
    "            word_pointer  = 0\n",
    "            desc_pointer +=1\n",
    "            if desc_pointer > len(train_set) -1:\n",
    "                break\n",
    "        else:\n",
    "            word_pointer += 1\n",
    "    \n",
    "    return corpus[0:counter].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceecd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_words(description,word_index,skip_window,span,grams_list):\n",
    "    \n",
    "    # the target word in the first place\n",
    "    grams_list.append(description[word_index])\n",
    "    \n",
    "    # initialize two pointers\n",
    "    counter = 1\n",
    "    data_index = word_index-1\n",
    "    \n",
    "    while counter < span:\n",
    "        # look left from target word\n",
    "        if counter<=skip_window:\n",
    "            # if data_index<0 => out of bound no more words to take into account\n",
    "            if data_index < 0:\n",
    "                data_index = word_index + 1\n",
    "                counter = skip_window + 1\n",
    "            # if the word is not in the dict skip it\n",
    "            elif description[data_index] == -2:\n",
    "                data_index -= 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index -= 1\n",
    "                if counter > skip_window:\n",
    "                    data_index = word_index + 1\n",
    "        # look right from target word\n",
    "        else:\n",
    "            if data_index >= len(description):\n",
    "                counter = span + 1\n",
    "            elif description[data_index] == -2:\n",
    "                data_index += 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c46ceccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(clean_descriptions, min_occurance, unk_word, skip_window, valid_set, test_set):\n",
    "    \n",
    "    # create vocabulary based on the frequency of each word.\n",
    "    # remove rare words, which occurs less time than min_occurance from voc\n",
    "    # word2id:  dictionary which contains the vocabulary and it's int id\n",
    "    \n",
    "    temp_sentences = [word for desc in clean_descriptions for word in desc]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(temp_sentences).most_common())\n",
    "    \n",
    "    # list temp_sentences now is useless\n",
    "    del temp_sentences\n",
    "    count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "    \n",
    "    # compute the vocabulary size\n",
    "    vocabulary_size = len(count)\n",
    "    \n",
    "    # assign an id to each word\n",
    "    # this dictionary will have voc_size+1 length.\n",
    "    word2id           = dict()\n",
    "    word2id[unk_word] = -2\n",
    "    \n",
    "    for i,(word,_) in enumerate(count):\n",
    "        word2id[word] = i\n",
    "        \n",
    "    # list count now is useless\n",
    "    del count\n",
    "    \n",
    "    # express train, valid and test set using id\n",
    "    train_set_id = [[word2id.get(word,-2) for word in desc] for desc in clean_descriptions]\n",
    "    #del clean_descriptions\n",
    "    \n",
    "    valid_set_id = [[word2id.get(word,-2) for word in desc] for desc in valid_set]\n",
    "    #del valid_set\n",
    "    \n",
    "    test_set_id  = [[word2id.get(word,-2) for word in desc] for desc in test_set]\n",
    "    #del test_set\n",
    "    \n",
    "    # save vocabulary\n",
    "    save_vocabulary(word2id)\n",
    "    \n",
    "    # create corpus with word pairs\n",
    "    corpus         = create_corpus(train_set_id,skip_window)\n",
    "    corpus_indexes = [w for w in range(len(corpus))] \n",
    "    \n",
    "    # save them \n",
    "    np.savetxt('../outputs/corpus_words_test.txt',corpus,fmt=\"%d\")\n",
    "    \n",
    "    # train_set_id now is useless\n",
    "    #del train_set_id\n",
    "    \n",
    "    return word2id,vocabulary_size,corpus,corpus_indexes,train_set_id,valid_set_id,test_set_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1efd09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_dict(test_set,min_occurance,num_words,num_words2,skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # numerate all words in the dataset.\n",
    "    temp_sentences = [word for desc in test_set for word in desc]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(temp_sentences).most_common())\n",
    "    \n",
    "    # list temp_sentences now is useless\n",
    "    del temp_sentences\n",
    "    \n",
    "    # remove rare words\n",
    "    count[:] = [e for e in count if e[1]>=min_occurance]\n",
    "    indexes  = [i for i in range(len(count)) if count[i][0] != -2]\n",
    "    \n",
    "    # split validation set into two sets one small used for cross entropy computation\n",
    "    # and the other at the end to meassure results.\n",
    "    if num_words2>0:\n",
    "        \n",
    "        samples2  = np.random.choice(indexes,num_words2,replace = False)\n",
    "        target_w2 = [count[i][0] for i in samples2]\n",
    "        w_dict2   = create_testing_pairs(test_set,count,target_w2,indexes,skip_window,true_neigh,false_neigh)\n",
    "        \n",
    "        # test on the \"num_words\" most frequent words\n",
    "        tmp_indexes = [i for i in indexes if i not in samples2]\n",
    "        target_w    = [count[tmp_indexes[i]][0] for i in range(num_words)]\n",
    "        w_dict      = create_testing_pairs(test_set,count,target_w,indexes,skip_window,true_neigh,false_neigh)\n",
    "        del tmp_indexes\n",
    "        return w_dict2,w_dict\n",
    "    \n",
    "    else:\n",
    "        # test on the \"num_words\" most frequent words\n",
    "        target_w = [count[indexes[i]][0] for i in range(num_words)]\n",
    "        w_dict   = create_testing_pairs(test_set,count,target_w,indexes,skip_window,true_neigh,false_neigh)\n",
    "        return None,w_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a144a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_pairs(test_set,count,target_w,indexes,skip_window,true_neigh,false_neigh):\n",
    "    \n",
    "    # initialize temporary buffer\n",
    "    span   = skip_window*2+1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    # initialize dictionary\n",
    "    w_dict   = dict([(key, [[],[]]) for key in target_w])\n",
    "    \n",
    "    # find true neighbors for target words\n",
    "    for desc in test_set:\n",
    "        for w in target_w:\n",
    "            temp_idx = [i for i,e in enumerate(desc) if w == e]\n",
    "            for idx in temp_idx:\n",
    "                find_context_words(desc,idx,skip_window,span,buffer)\n",
    "                for i in range(1,len(buffer)):\n",
    "                    if w_dict[w][0] == []:\n",
    "                        w_dict[w][0].append(buffer[i])\n",
    "                    elif buffer[i] not in w_dict[w][0]:\n",
    "                        w_dict[w][0].append(buffer[i])\n",
    "    \n",
    "    # find false neigbors for target words\n",
    "    for key in w_dict:\n",
    "        neig_counter = 0\n",
    "        flag         = True\n",
    "        while flag  == True:\n",
    "            random_idx   = np.random.choice(indexes,2*false_neigh,replace = False)\n",
    "            for idx in random_idx:\n",
    "                if count[idx][0] == key:\n",
    "                    continue\n",
    "                elif count[idx][0] in w_dict[key][0]:\n",
    "                    continue\n",
    "                elif count[idx][0] not in w_dict[key][1]:\n",
    "                    w_dict[key][1].append(count[idx][0])\n",
    "                    neig_counter += 1\n",
    "                    if neig_counter >= false_neigh:\n",
    "                        flag = False\n",
    "                        break\n",
    "    \n",
    "    # choose randomly only true_neigh neighbors.\n",
    "    removed_keys = []\n",
    "    for key in w_dict:\n",
    "        if len(w_dict[key][0])>=true_neigh:\n",
    "            idx_neigh =  np.random.choice([i for i in range(len(w_dict[key][0]))],true_neigh,replace = False)\n",
    "            w_dict[key][0] = [w_dict[key][0][i] for i in idx_neigh]\n",
    "        else:\n",
    "            removed_keys.append(key)\n",
    "            \n",
    "    if removed_keys != []:\n",
    "        for key in removed_keys:\n",
    "            w_dict.pop(key)\n",
    "    return w_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "152f9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(corpus_data,corpus_indexes,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size),   dtype = np.int32)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.int32)\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    words_to_use = random.sample(corpus_indexes,batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(words_to_use):\n",
    "        batch[counter]    = corpus_data[value][0]\n",
    "        labels[counter,0] = corpus_data[value][1] \n",
    "    \n",
    "    return batch,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fb61ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def_cpu(corpus_data,corpus_indexes,batch_size,embedding_dim,\n",
    "                  num_sampled,learning_rate,vocabulary_size,v_batch,v_labels):\n",
    "    \n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.int32, shape=[None])\n",
    "    # Input label\n",
    "    Y_train = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    \n",
    "    # ensure that the following ops & var are assigned to CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "    \n",
    "        # create the embedding variable wich contains the weights\n",
    "        embedding = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "        \n",
    "        # create the lookup table for each sample in X_train=>avoiding to use one_hot encoder\n",
    "        X_embed   = tf.nn.embedding_lookup(embedding,X_train)\n",
    "        \n",
    "        # create variables for the loss function\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_dim],stddev=1.0))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    loss_func = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,biases =nce_biases,labels = Y_train,\n",
    "                                              inputs = X_embed,num_sampled = num_sampled,\n",
    "                                              num_classes = vocabulary_size ))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_opt = optimizer.minimize(loss_func)\n",
    "    \n",
    "    #Define initializer for tensorflow variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #actual initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # patience method's variables \n",
    "        min_loss           = float('inf')\n",
    "        min_emb_matrix     = np.zeros((vocabulary_size,embedding_dim))\n",
    "        patience_remaining = 500\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # train the model using 500 epoch patience\n",
    "        for epoch in range(50000):\n",
    "            \n",
    "            # take a batch of data.\n",
    "            batch_x,batch_y = generate_batch(corpus_data,corpus_indexes,batch_size)\n",
    "            \n",
    "            _,train_loss = sess.run([train_opt,loss_func],feed_dict={X_train:batch_x, Y_train:batch_y})\n",
    "            valid_loss   = sess.run(loss_func,feed_dict={X_train:v_batch, Y_train:v_labels})\n",
    "            \n",
    "            patience_remaining -= 1\n",
    "            if valid_loss < min_loss:\n",
    "                min_loss           = valid_loss\n",
    "                patience_remaining = 500\n",
    "                min_emb_matrix     = embedding.eval()\n",
    "            if patience_remaining == 0:\n",
    "                break\n",
    "        \n",
    "        #restore min embeddings\n",
    "        embedding = tf.convert_to_tensor(min_emb_matrix)\n",
    "        \n",
    "        #normalize embeddings before using them\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keepdims = True))\n",
    "        normalized_embedding = embedding/norm\n",
    "        normalized_embedding_matrix = sess.run(normalized_embedding)\n",
    "        \n",
    "        #measure total time\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "    \n",
    "    return normalized_embedding_matrix,epoch+1,total_time\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48d2c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model computes tpr, fpr and auc. The classes are class_A = real neighbor\n",
    "# and class_B = false neighbor. The model based on cosine similarity\n",
    "# will try to predict the right label for each word pair given.\n",
    "\n",
    "def model_validation_v2(embedding_matrix,words_dict):\n",
    "    \n",
    "    ylabels = list()\n",
    "    ypreds  = list()\n",
    "    \n",
    "    for key in words_dict:\n",
    "        target_emb = embedding_matrix[key]\n",
    "        for true_neigh in words_dict[key][0]:\n",
    "            neigh_emb = embedding_matrix[true_neigh]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            ylabels.append(1)\n",
    "            ypreds.append(result)\n",
    "            \n",
    "        for false_neigh in words_dict[key][1]:\n",
    "            neigh_emb = embedding_matrix[false_neigh]\n",
    "            result    = np.dot(target_emb,neigh_emb)/(np.sqrt(np.dot(target_emb,target_emb))*np.sqrt(np.dot(neigh_emb,neigh_emb)))\n",
    "            ylabels.append(0)\n",
    "            ypreds.append(result)\n",
    "    \n",
    "    y = np.array(ylabels)\n",
    "    score = np.array(ypreds)\n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y,score)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb9c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logs(epoch,p_time,auc,t_auc,min_occurance,skip_window,embedding_dim,num_sampled,learning_rate):\n",
    "    with open(\"../outputs/logs.txt\",\"a\") as file:\n",
    "        file.write(\"parameter's value: min occurance %s, skip window %s, embedding dim %s, num sampled %s, learning rate %s \\n\"%(str(min_occurance),str(skip_window),str(embedding_dim),str(num_sampled),str(learning_rate)))\n",
    "        file.write(\"total time in sec %s and total epochs %s \\n\"%(str(p_time),str(epoch)))\n",
    "        file.write(\"Validation AUC: %s \\n\"%(str(auc)))\n",
    "        file.write(\"Testing AUC %s \\n\"%(str(t_auc)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d2dd95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with pm  10 1 8 4 0.01\n",
      "finished with pm  10 1 8 4 0.1\n",
      "finished with pm  10 1 8 4 1\n",
      "finished with pm  10 1 8 4 2\n",
      "finished with pm  10 1 8 8 0.01\n",
      "finished with pm  10 1 8 8 0.1\n",
      "finished with pm  10 1 8 8 1\n",
      "finished with pm  10 1 8 8 2\n",
      "finished with pm  10 1 8 16 0.01\n",
      "finished with pm  10 1 8 16 0.1\n",
      "finished with pm  10 1 8 16 1\n",
      "finished with pm  10 1 8 16 2\n",
      "finished with pm  10 1 8 32 0.01\n",
      "finished with pm  10 1 8 32 0.1\n",
      "finished with pm  10 1 8 32 1\n",
      "finished with pm  10 1 8 32 2\n",
      "finished with pm  10 1 8 64 0.01\n",
      "finished with pm  10 1 8 64 0.1\n",
      "finished with pm  10 1 8 64 1\n",
      "finished with pm  10 1 8 64 2\n",
      "finished with pm  10 1 16 4 0.01\n",
      "finished with pm  10 1 16 4 0.1\n",
      "finished with pm  10 1 16 4 1\n",
      "finished with pm  10 1 16 4 2\n",
      "finished with pm  10 1 16 8 0.01\n",
      "finished with pm  10 1 16 8 0.1\n",
      "finished with pm  10 1 16 8 1\n",
      "finished with pm  10 1 16 8 2\n",
      "finished with pm  10 1 16 16 0.01\n",
      "finished with pm  10 1 16 16 0.1\n",
      "finished with pm  10 1 16 16 1\n",
      "finished with pm  10 1 16 16 2\n",
      "finished with pm  10 1 16 32 0.01\n",
      "finished with pm  10 1 16 32 0.1\n",
      "finished with pm  10 1 16 32 1\n",
      "finished with pm  10 1 16 32 2\n",
      "finished with pm  10 1 16 64 0.01\n",
      "finished with pm  10 1 16 64 0.1\n",
      "finished with pm  10 1 16 64 1\n",
      "finished with pm  10 1 16 64 2\n",
      "finished with pm  10 1 32 4 0.01\n",
      "finished with pm  10 1 32 4 0.1\n",
      "finished with pm  10 1 32 4 1\n",
      "finished with pm  10 1 32 4 2\n",
      "finished with pm  10 1 32 8 0.01\n",
      "finished with pm  10 1 32 8 0.1\n",
      "finished with pm  10 1 32 8 1\n",
      "finished with pm  10 1 32 8 2\n",
      "finished with pm  10 1 32 16 0.01\n",
      "finished with pm  10 1 32 16 0.1\n",
      "finished with pm  10 1 32 16 1\n",
      "finished with pm  10 1 32 16 2\n",
      "finished with pm  10 1 32 32 0.01\n",
      "finished with pm  10 1 32 32 0.1\n",
      "finished with pm  10 1 32 32 1\n",
      "finished with pm  10 1 32 32 2\n",
      "finished with pm  10 1 32 64 0.01\n",
      "finished with pm  10 1 32 64 0.1\n",
      "finished with pm  10 1 32 64 1\n",
      "finished with pm  10 1 32 64 2\n",
      "finished with pm  10 1 64 4 0.01\n",
      "finished with pm  10 1 64 4 0.1\n",
      "finished with pm  10 1 64 4 1\n",
      "finished with pm  10 1 64 4 2\n",
      "finished with pm  10 1 64 8 0.01\n",
      "finished with pm  10 1 64 8 0.1\n",
      "finished with pm  10 1 64 8 1\n",
      "finished with pm  10 1 64 8 2\n",
      "finished with pm  10 1 64 16 0.01\n",
      "finished with pm  10 1 64 16 0.1\n",
      "finished with pm  10 1 64 16 1\n",
      "finished with pm  10 1 64 16 2\n",
      "finished with pm  10 1 64 32 0.01\n",
      "finished with pm  10 1 64 32 0.1\n",
      "finished with pm  10 1 64 32 1\n",
      "finished with pm  10 1 64 32 2\n",
      "finished with pm  10 1 64 64 0.01\n",
      "finished with pm  10 1 64 64 0.1\n",
      "finished with pm  10 1 64 64 1\n",
      "finished with pm  10 1 64 64 2\n",
      "finished with pm  10 1 128 4 0.01\n",
      "finished with pm  10 1 128 4 0.1\n",
      "finished with pm  10 1 128 4 1\n",
      "finished with pm  10 1 128 4 2\n",
      "finished with pm  10 1 128 8 0.01\n",
      "finished with pm  10 1 128 8 0.1\n",
      "finished with pm  10 1 128 8 1\n",
      "finished with pm  10 1 128 8 2\n",
      "finished with pm  10 1 128 16 0.01\n",
      "finished with pm  10 1 128 16 0.1\n",
      "finished with pm  10 1 128 16 1\n",
      "finished with pm  10 1 128 16 2\n",
      "finished with pm  10 1 128 32 0.01\n",
      "finished with pm  10 1 128 32 0.1\n",
      "finished with pm  10 1 128 32 1\n",
      "finished with pm  10 1 128 32 2\n",
      "finished with pm  10 1 128 64 0.01\n",
      "finished with pm  10 1 128 64 0.1\n",
      "finished with pm  10 1 128 64 1\n",
      "finished with pm  10 1 128 64 2\n",
      "finished with pm  10 2 8 4 0.01\n",
      "finished with pm  10 2 8 4 0.1\n",
      "finished with pm  10 2 8 4 1\n",
      "finished with pm  10 2 8 4 2\n",
      "finished with pm  10 2 8 8 0.01\n",
      "finished with pm  10 2 8 8 0.1\n",
      "finished with pm  10 2 8 8 1\n",
      "finished with pm  10 2 8 8 2\n",
      "finished with pm  10 2 8 16 0.01\n",
      "finished with pm  10 2 8 16 0.1\n",
      "finished with pm  10 2 8 16 1\n",
      "finished with pm  10 2 8 16 2\n",
      "finished with pm  10 2 8 32 0.01\n",
      "finished with pm  10 2 8 32 0.1\n",
      "finished with pm  10 2 8 32 1\n",
      "finished with pm  10 2 8 32 2\n",
      "finished with pm  10 2 8 64 0.01\n",
      "finished with pm  10 2 8 64 0.1\n",
      "finished with pm  10 2 8 64 1\n",
      "finished with pm  10 2 8 64 2\n",
      "finished with pm  10 2 16 4 0.01\n",
      "finished with pm  10 2 16 4 0.1\n",
      "finished with pm  10 2 16 4 1\n",
      "finished with pm  10 2 16 4 2\n",
      "finished with pm  10 2 16 8 0.01\n",
      "finished with pm  10 2 16 8 0.1\n",
      "finished with pm  10 2 16 8 1\n",
      "finished with pm  10 2 16 8 2\n",
      "finished with pm  10 2 16 16 0.01\n",
      "finished with pm  10 2 16 16 0.1\n",
      "finished with pm  10 2 16 16 1\n",
      "finished with pm  10 2 16 16 2\n",
      "finished with pm  10 2 16 32 0.01\n",
      "finished with pm  10 2 16 32 0.1\n",
      "finished with pm  10 2 16 32 1\n",
      "finished with pm  10 2 16 32 2\n",
      "finished with pm  10 2 16 64 0.01\n",
      "finished with pm  10 2 16 64 0.1\n",
      "finished with pm  10 2 16 64 1\n",
      "finished with pm  10 2 16 64 2\n",
      "finished with pm  10 2 32 4 0.01\n",
      "finished with pm  10 2 32 4 0.1\n",
      "finished with pm  10 2 32 4 1\n",
      "finished with pm  10 2 32 4 2\n",
      "finished with pm  10 2 32 8 0.01\n",
      "finished with pm  10 2 32 8 0.1\n",
      "finished with pm  10 2 32 8 1\n",
      "finished with pm  10 2 32 8 2\n",
      "finished with pm  10 2 32 16 0.01\n",
      "finished with pm  10 2 32 16 0.1\n",
      "finished with pm  10 2 32 16 1\n",
      "finished with pm  10 2 32 16 2\n",
      "finished with pm  10 2 32 32 0.01\n",
      "finished with pm  10 2 32 32 0.1\n",
      "finished with pm  10 2 32 32 1\n",
      "finished with pm  10 2 32 32 2\n",
      "finished with pm  10 2 32 64 0.01\n",
      "finished with pm  10 2 32 64 0.1\n",
      "finished with pm  10 2 32 64 1\n",
      "finished with pm  10 2 32 64 2\n",
      "finished with pm  10 2 64 4 0.01\n",
      "finished with pm  10 2 64 4 0.1\n",
      "finished with pm  10 2 64 4 1\n",
      "finished with pm  10 2 64 4 2\n",
      "finished with pm  10 2 64 8 0.01\n",
      "finished with pm  10 2 64 8 0.1\n",
      "finished with pm  10 2 64 8 1\n",
      "finished with pm  10 2 64 8 2\n",
      "finished with pm  10 2 64 16 0.01\n",
      "finished with pm  10 2 64 16 0.1\n",
      "finished with pm  10 2 64 16 1\n",
      "finished with pm  10 2 64 16 2\n",
      "finished with pm  10 2 64 32 0.01\n",
      "finished with pm  10 2 64 32 0.1\n",
      "finished with pm  10 2 64 32 1\n",
      "finished with pm  10 2 64 32 2\n",
      "finished with pm  10 2 64 64 0.01\n",
      "finished with pm  10 2 64 64 0.1\n",
      "finished with pm  10 2 64 64 1\n",
      "finished with pm  10 2 64 64 2\n",
      "finished with pm  10 2 128 4 0.01\n",
      "finished with pm  10 2 128 4 0.1\n",
      "finished with pm  10 2 128 4 1\n",
      "finished with pm  10 2 128 4 2\n",
      "finished with pm  10 2 128 8 0.01\n",
      "finished with pm  10 2 128 8 0.1\n",
      "finished with pm  10 2 128 8 1\n",
      "finished with pm  10 2 128 8 2\n",
      "finished with pm  10 2 128 16 0.01\n",
      "finished with pm  10 2 128 16 0.1\n",
      "finished with pm  10 2 128 16 1\n",
      "finished with pm  10 2 128 16 2\n",
      "finished with pm  10 2 128 32 0.01\n",
      "finished with pm  10 2 128 32 0.1\n",
      "finished with pm  10 2 128 32 1\n",
      "finished with pm  10 2 128 32 2\n",
      "finished with pm  10 2 128 64 0.01\n",
      "finished with pm  10 2 128 64 0.1\n",
      "finished with pm  10 2 128 64 1\n",
      "finished with pm  10 2 128 64 2\n"
     ]
    }
   ],
   "source": [
    "unk_word      = \"UNK\"\n",
    "valid_words   = 80\n",
    "valid_words2  = 70\n",
    "test_words    = 100\n",
    "true_neigh    = 8\n",
    "false_neigh   = 30\n",
    "batch_size    = 2048\n",
    "for min_occurance in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    for skip_window in [1,2,3,4]:\n",
    "\n",
    "        word2id,vocabulary_size,corpus,corpus_indexes,train_set_id,valid_set_id,test_set_id = create_dict(clean_descriptions, min_occurance, unk_word, skip_window, valid_set, test_set)\n",
    "\n",
    "        _,test_dict  = create_testing_dict(test_set_id,5,test_words,0,2,true_neigh,false_neigh)\n",
    "        save_test_pairs(test_dict)\n",
    "        del test_dict\n",
    "\n",
    "        v_dict2,v_dict = create_testing_dict(valid_set_id,5,valid_words,valid_words2,2,true_neigh,false_neigh)\n",
    "        t_batch  = []\n",
    "        t_label  = []\n",
    "\n",
    "        for key in v_dict2:\n",
    "            for value in v_dict2[key][0]:\n",
    "                t_batch.append(key)\n",
    "                t_label.append(value)\n",
    "\n",
    "        v_batch = np.reshape(t_batch,(len(t_batch),))\n",
    "        v_label = np.reshape(t_label,(len(t_label),1))\n",
    "\n",
    "        for embedding_dim in [8,16,32,64,128]:\n",
    "            for num_sampled in [4,8,16,32,64]:\n",
    "                for learning_rate in [0.01,0.1,1,2]:\n",
    "\n",
    "                    norm_embedding_matrix,epoch,p_time = model_def_cpu(corpus,corpus_indexes,batch_size,\n",
    "                                                                     embedding_dim,num_sampled,learning_rate,\n",
    "                                                                     vocabulary_size,v_batch,v_label)\n",
    "\n",
    "                    auc = model_validation_v2(norm_embedding_matrix,v_dict)\n",
    "\n",
    "                    # unpickling test dictionary\n",
    "                    with open('../outputs/testing_pairs_test.pkl','rb') as infile:\n",
    "                        testing_dict = pickle.load(infile)\n",
    "\n",
    "                    t_auc = model_validation_v2(norm_embedding_matrix,testing_dict)\n",
    "                    save_logs(epoch,p_time,auc,t_auc,min_occurance,skip_window,embedding_dim,num_sampled,learning_rate)\n",
    "                    print(\"finished with pm \",min_occurance,skip_window,embedding_dim,num_sampled,learning_rate)\n",
    "            \n",
    "            time.sleep(60)\n",
    "        time.sleep(60)\n",
    "        \n",
    "        os.remove(\"../outputs/corpus_words_test.txt\")\n",
    "        os.remove(\"../outputs/testing_pairs_test.pkl\")\n",
    "        os.remove(\"../outputs/words_vocabulary.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab374f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
