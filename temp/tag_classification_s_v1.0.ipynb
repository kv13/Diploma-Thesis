{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40600cf2",
   "metadata": {},
   "source": [
    "# Tag Classification Use Only Stack Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b085c2",
   "metadata": {},
   "source": [
    "This notebook contains logistic regression and neural network for tag classification but based only on stack traces.<br/>\n",
    "We want to calculate if stack embeddings alone have any correlation with tag classification and if they contains additional knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc27ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aced6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path_to_file):\n",
    "    temp_dict = dict()\n",
    "    with open(path_to_file) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            temp   = str(line)\n",
    "            values = temp.split(',')\n",
    "            temp_dict[values[0]] = int(values[1].replace(\"\\n\",\"\"))\n",
    "    \n",
    "    return temp_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6dfb8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_issues(dir_path,tag_labels,stack_traces):\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            \n",
    "            data = json.load(json_file)\n",
    "            for issue in data:\n",
    "                \n",
    "                tags = issue['tags']\n",
    "                for i in range(len(tags)):\n",
    "                    tags[i] = tags[i].strip()\n",
    "                \n",
    "                stack_trace = issue['stack_trace']\n",
    "                name        = issue['name']\n",
    "                \n",
    "                if tags != [] and stack_trace !=[] :\n",
    "                    tag_labels.append(tags)\n",
    "                    stack_traces.append(stack_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "624d10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def clean_stack_trace(stack_trace):\n",
    "    \n",
    "    clean_stack_trace = []\n",
    "    temp_stack        = stack_trace.split(\" at \")[1:]\n",
    "    \n",
    "    to_find = re.compile(\"[|,|<|>]|/|\\|=\")\n",
    "    \n",
    "    #find where each function ends and keep only the path\n",
    "    for f in temp_stack:\n",
    "        temp      = f.find(')')\n",
    "        temp_file = f[0:temp]\n",
    "        \n",
    "        # check the punctuations in order to avoid anything else\n",
    "        match_obj = to_find.search(temp_file)\n",
    "        if match_obj == None:\n",
    "            filename = find_filename(temp_file)\n",
    "            if filename != '':\n",
    "                clean_stack_trace.append(filename)\n",
    "                \n",
    "    return clean_stack_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3b075781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy paste from stack_trace_embedding notebook\n",
    "\n",
    "def find_filename(value):\n",
    "    filename = \"\"\n",
    "    words    = value.split(\"(\")\n",
    "    if len(words)>=2:\n",
    "        parts = words[0].split(\".\")\n",
    "        filename = \".\".join(parts[0:-1])\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e21f57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(stack_traces):\n",
    "    \n",
    "    clean_stack_traces = list()\n",
    "    \n",
    "    for i in range(len(stack_traces)):\n",
    "        \n",
    "        temp_trace  = stack_traces[i]\n",
    "        stack_trace = []\n",
    "        \n",
    "        if temp_trace != []:\n",
    "            if len(temp_trace)>1:\n",
    "                stack_trace = clean_stack_trace(' '.join(temp_trace))\n",
    "            else:\n",
    "                stack_trace = clean_stack_trace(temp_trace[0])\n",
    "                \n",
    "        clean_stack_traces.append(stack_trace)\n",
    "        \n",
    "    return clean_stack_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "16c0c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(arithmetic_stack_traces,stack_embedding_matrix):\n",
    "    \n",
    "    stack_traces_dim  = np.shape(stack_embedding_matrix)[1]\n",
    "    \n",
    "    num_issues        = len(arithmetic_stack_traces)\n",
    "    issues_embeddings = np.zeros((num_issues,stack_traces_dim))\n",
    "    \n",
    "    for counter in range(num_issues):\n",
    "        \n",
    "        temp_stack  = arithmetic_stack_traces[counter]\n",
    "        total_funcs = 0\n",
    "        \n",
    "        for func in temp_stack:\n",
    "            if func != -2:\n",
    "                total_funcs += 1\n",
    "                issues_embeddings[counter][0:stack_traces_dim] = issues_embeddings[counter][0:stack_traces_dim]+ stack_embedding_matrix[func]\n",
    "        if total_funcs != 0:\n",
    "            issues_embeddings[counter]= issues_embeddings[counter]/ total_funcs\n",
    "    \n",
    "    return issues_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6652b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stack traces embeddings \n",
    "stack_embedding_matrix = np.loadtxt('../results/stack_embeddings_g.txt', dtype=np.float64)\n",
    "\n",
    "# load vocabularies\n",
    "func2id_path = \"../outputs/stacktraces_vocabulary_g.txt\"\n",
    "func2id = load_dict(func2id_path)\n",
    "\n",
    "#load tags and descriptions\n",
    "dir_path     = '../data'\n",
    "tag_labels   = list()\n",
    "stack_traces = list()\n",
    "\n",
    "# load issues\n",
    "load_issues(dir_path,tag_labels,stack_traces)\n",
    "\n",
    "# clean stack traces\n",
    "clean_stack_traces = clean_data(stack_traces)\n",
    "\n",
    "clean_stack_traces_2 = list()\n",
    "clean_tags_2         = list()\n",
    "\n",
    "# remove empty stack traces\n",
    "for counter,value in enumerate(clean_stack_traces):\n",
    "    if value != []:\n",
    "        clean_stack_traces_2.append(value)\n",
    "        clean_tags_2.append(tag_labels[counter])\n",
    "\n",
    "del clean_stack_traces\n",
    "del stack_traces\n",
    "\n",
    "arithmetic_stack_traces = [[func2id.get(func,-2) for func in trace] for trace in clean_stack_traces_2]\n",
    "del clean_stack_traces_2\n",
    "\n",
    "issues_embeddings  = compute_embeddings(arithmetic_stack_traces,stack_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f654c85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38540108  0.20484011  0.10255159 -0.47162718  0.27095725  0.4557721\n",
      "  0.20261681  0.0100261 ]\n"
     ]
    }
   ],
   "source": [
    "print(issues_embeddings[450])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f88c9",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "06a9a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_labels = list()\n",
    "# copy by reference in order to avoid to change every where the variable name\n",
    "tag_labels = clean_tags_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8d64b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['Bug','Google Play or Beta feedback','Prio - High']\n",
    "no_tags = 3\n",
    "np_tags = np.zeros((len(arithmetic_stack_traces),no_tags))\n",
    "\n",
    "for counter in range(len(tag_labels)):\n",
    "    for counter_2,value in enumerate(tags):\n",
    "        if value in tag_labels[counter]:\n",
    "            np_tags[counter][counter_2] = 1\n",
    "            \n",
    "df_tags = pd.DataFrame(np_tags, columns = tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5879cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier(tags,df_tags,issues_embeddings,cl_label,n_splits):\n",
    "    \n",
    "    target_label    = df_tags[cl_label]\n",
    "    counter_1       = np.sum(target_label)\n",
    "    weight_0        = 1/(target_label.shape[0]-counter_1)\n",
    "    weight_1        = 1/counter_1\n",
    "    w               = {0:weight_0,1:weight_1}\n",
    "    skf             = StratifiedKFold(n_splits)\n",
    "    model           = LogisticRegression(solver='lbfgs',class_weight = w)\n",
    "    total_confusion = np.zeros((2,2))\n",
    "    \n",
    "    for train_index, test_index in skf.split(issues_embeddings,target_label):\n",
    "        \n",
    "        X_train,X_test = issues_embeddings[train_index], issues_embeddings[test_index]\n",
    "        y_train,y_test = target_label[train_index], target_label[test_index]\n",
    "        \n",
    "        #fit model \n",
    "        model.fit(X_train,y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        #print(confusion_matrix(y_test,predictions))\n",
    "        total_confusion = total_confusion+confusion_matrix(y_test,predictions)\n",
    "        \n",
    "    print(total_confusion)\n",
    "    print(\"accuracy = TP+TN/(TP+TN+FP+FN)\",(total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion))\n",
    "    print(\"GM\",np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "                                  (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0]))))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "131bbdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26.  22.]\n",
      " [172. 231.]]\n",
      "accuracy = TP+TN/(TP+TN+FP+FN) 0.5698447893569845\n",
      "GM 0.5572107958104742\n",
      "\n",
      "\n",
      "[[152. 131.]\n",
      " [ 68. 100.]]\n",
      "accuracy = TP+TN/(TP+TN+FP+FN) 0.5587583148558758\n",
      "GM 0.5654236051605539\n",
      "\n",
      "\n",
      "[[139. 230.]\n",
      " [ 33.  49.]]\n",
      "accuracy = TP+TN/(TP+TN+FP+FN) 0.41685144124168516\n",
      "GM 0.4744444065192336\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only stack_traces\n",
    "my_classifier(tags,df_tags,issues_embeddings,\"Bug\",10)\n",
    "my_classifier(tags,df_tags,issues_embeddings,\"Google Play or Beta feedback\",10)\n",
    "my_classifier(tags,df_tags,issues_embeddings,\"Prio - High\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bde499",
   "metadata": {},
   "source": [
    "## Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b411192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa1217e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset2(issues_embeddings,target_labels):\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.1, random_state = 0)\n",
    "    \n",
    "    X_train_0 = list()\n",
    "    X_train_1 = list()\n",
    "    \n",
    "    for train_index, test_index in sss.split(issues_embeddings,target_labels):\n",
    "        #X_train,X_test = issues_embeddings[train_index], issues_embeddings[test_index]\n",
    "        #Y_train,Y_test = target_labels[train_index], target_labels[test_index]\n",
    "        \n",
    "        \n",
    "        X_test = issues_embeddings[test_index]\n",
    "        Y_test = target_labels[test_index]\n",
    "        \n",
    "        for index in train_index:\n",
    "            if target_labels.iloc[index] == 0:\n",
    "                X_train_0.append(issues_embeddings[index])\n",
    "            elif target_labels.iloc[index] == 1:\n",
    "                X_train_1.append(issues_embeddings[index])\n",
    "                \n",
    "    return X_train_0,X_train_1,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8ed36a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(issues_embeddings,target_labels,batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,2), dtype = np.float64)\n",
    "    \n",
    "    issues_to_use = random.sample([i for i in range(np.shape(issues_embeddings)[0])],batch_size)\n",
    "    \n",
    "    for counter,value in enumerate(issues_to_use):\n",
    "        batch[counter][:]  = issues_embeddings[value][:]\n",
    "        # label_0\n",
    "        labels[counter][0] = 1-target_labels.iloc[value]\n",
    "        # label_1\n",
    "        labels[counter][1] =   target_labels.iloc[value]\n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3bc27f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(issues_embeddings_0, issues_embeddings_1, batch_size):\n",
    "    \n",
    "    batch  = np.ndarray(shape = (batch_size,np.shape(issues_embeddings_0)[1]), dtype = np.float64)\n",
    "    labels = np.ndarray(shape = (batch_size,2), dtype = np.float64)\n",
    "    \n",
    "    issues_to_use_0 = random.sample([i for i in range(np.shape(issues_embeddings_0)[0])],batch_size//2)\n",
    "    issues_to_use_1 = random.sample([i for i in range(np.shape(issues_embeddings_1)[0])],batch_size//2)\n",
    "    \n",
    "    # even indexes for issues belong to class 0\n",
    "    # odd  indexes for issues belong to class 1\n",
    "    counter_0 = 0\n",
    "    counter_1 = 0\n",
    "    \n",
    "    for counter in range(batch_size):\n",
    "        \n",
    "        # even indexes\n",
    "        if counter%2 == 0 :\n",
    "            batch[counter][:]  = issues_embeddings_0[issues_to_use_0[counter_0]][:]\n",
    "            labels[counter][0] = 1\n",
    "            labels[counter][1] = 0\n",
    "            counter_0 += 1\n",
    "        else:\n",
    "            batch[counter][:]  = issues_embeddings_1[issues_to_use_1[counter_1]][:]\n",
    "            labels[counter][0] = 0\n",
    "            labels[counter][1] = 1\n",
    "            counter_1 += 1\n",
    "            \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aecf53ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier_nn2(issues_embeddings_0,issues_embeddings_1,hidden_layer_dim,\n",
    "                      learning_rate,batch_size,epochs,v_batch,v_labels):\n",
    "    \n",
    "    # input data\n",
    "    X_train = tf.placeholder(tf.float64, shape=[None,np.shape(issues_embeddings_0)[1]])\n",
    "    # input label\n",
    "    Y_train = tf.placeholder(tf.float64, shape=[None,2])\n",
    "    \n",
    "    # input-hidden layer variables\n",
    "    W1 = tf.Variable(tf.truncated_normal([np.shape(issues_embeddings_0)[1],hidden_layer_dim],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([hidden_layer_dim],stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                      dtype=tf.float64),name = 'b1')\n",
    "    \n",
    "    # hidden-output layer variables\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_layer_dim,2],\n",
    "                                         stddev = 1.0/ math.sqrt(hidden_layer_dim),\n",
    "                                         dtype=tf.float64),name = 'W2')\n",
    "    b2 = tf.Variable(tf.random_normal([2],dtype=tf.float64),name = 'b2')\n",
    "    \n",
    "    # neural network's functions\n",
    "    hidden_layer   = tf.add(tf.matmul(X_train,W1),b1)\n",
    "    hidden_layer   = tf.nn.tanh(hidden_layer)\n",
    "    \n",
    "    output_layer   = tf.add(tf.matmul(hidden_layer,W2),b2)\n",
    "    output_layer_2 = tf.nn.softmax(output_layer)\n",
    "    \n",
    "    cost_func = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = Y_train,logits = output_layer))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_func)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # generate batch.\n",
    "            batch_x,batch_y = pooling(issues_embeddings_0,issues_embeddings_1,batch_size)\n",
    "            \n",
    "            # train the model\n",
    "            _,loss = sess.run([optimizer,cost_func],feed_dict={X_train:batch_x,Y_train:batch_y})\n",
    "        \n",
    "        # validation\n",
    "        y_probs     = sess.run(output_layer_2,feed_dict={X_train:v_batch,Y_train:v_labels})\n",
    "    \n",
    "    return compute_metrics(y_probs,v_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5970e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_probs,v_labels):\n",
    "    \n",
    "    y_preds_1 = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "    y_true_1  = np.ndarray(shape = (np.shape(v_labels)[0],1), dtype = np.float64)\n",
    "    \n",
    "    for i in range(np.shape(v_labels)[0]):\n",
    "        y_true_1[i]  = v_labels[i][1]\n",
    "        y_preds_1[i] = 0 if y_probs[i][0]>y_probs[i][1] else 1\n",
    "    \n",
    "    total_confusion = confusion_matrix(y_true=y_true_1,y_pred=y_preds_1)\n",
    "    return total_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "542fea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739287ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5a3e2673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# use only stack traces embeddings\n",
    "target_labels = df_tags[\"Bug\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28948854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5760869565217391\n",
      "GM 0.550565119871857\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_confusion = np.zeros((2,2))\n",
    "\n",
    "for i in range(10):\n",
    "    total_confusion += my_classifier_nn2(train_issues_0,train_issues_1,4,0.01,\n",
    "                                         2*batch_size,1000,v_batch,v_labels)\n",
    "\n",
    "    \n",
    "acc = (total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion)\n",
    "gm  = np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "              (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0])))\n",
    "\n",
    "print(\"accuracy\",acc)\n",
    "print(\"GM\",gm)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e3e68c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "# second implementation\n",
    "target_labels = df_tags[\"Prio - High\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b3ff4881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.48695652173913045\n",
      "GM 0.5349618776383189\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the neural network using and stack traces embeddings\n",
    "total_confusion = np.zeros((2,2))\n",
    "\n",
    "for i in range(10):\n",
    "    total_confusion += my_classifier_nn2(train_issues_0,train_issues_1,\n",
    "                                         4,0.01,2*batch_size,50,v_batch,v_labels)\n",
    "\n",
    "    \n",
    "acc = (total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion)\n",
    "gm  = np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "              (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0])))\n",
    "\n",
    "print(\"accuracy\",acc)\n",
    "print(\"GM\",gm)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "19b3caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "# second implementation\n",
    "target_labels = df_tags[\"Google Play or Beta feedback\"]\n",
    "train_issues_0,train_issues_1,test_issues,test_labels = split_dataset2(issues_embeddings,\n",
    "                                                                      target_labels)\n",
    "\n",
    "batch_size = np.shape(train_issues_0)[0] if np.shape(train_issues_0)[0]<np.shape(train_issues_1)[0] else np.shape(train_issues_1)[0]  \n",
    "print(batch_size)\n",
    "v_batch,v_labels = generate_batch(test_issues,test_labels,np.shape(test_issues)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "74a222b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5978260869565217\n",
      "GM 0.6109215597192633\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the neural network using and stack traces embeddings\n",
    "total_confusion = np.zeros((2,2))\n",
    "\n",
    "for i in range(10):\n",
    "    total_confusion += my_classifier_nn2(train_issues_0,train_issues_1,\n",
    "                                         4,0.01,2*batch_size,50,v_batch,v_labels)\n",
    "\n",
    "    \n",
    "acc = (total_confusion[0][0]+total_confusion[1][1])/np.sum(total_confusion)\n",
    "gm  = np.sqrt((total_confusion[0][0]/(total_confusion[0][0]+total_confusion[0][1]))*\n",
    "              (total_confusion[1][1]/(total_confusion[1][1]+total_confusion[1][0])))\n",
    "\n",
    "print(\"accuracy\",acc)\n",
    "print(\"GM\",gm)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
